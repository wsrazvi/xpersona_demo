{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xpersonaaa.py",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02pZgFtszA0q"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVVL6YBMbbSQ",
        "outputId": "42bb42a3-56a7-4761-d995-67fc209bed27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " %cd drive/MyDrive/Xpersona-master/crosslingual"
      ],
      "metadata": {
        "id": "dT9mze134fo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939fcba9-a91d-41a2-822a-71bccaea2b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Xpersona-master/crosslingual\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "lang_list = [\"En\", \"Zh\", \"Fr\", \"Id\", \"It\", \"Jp\", \"Ko\"]\n",
        "\n",
        "for lang in lang_list:\n",
        "    for split in [\"train\", \"valid\", \"test\"]:\n",
        "        if lang == \"En\":\n",
        "            file_name = \"%s_persona_%s.json\" % (lang, split)\n",
        "        elif split == \"train\":\n",
        "            file_name = \"%s_persona_%s_corrected.json\" % (lang, split)\n",
        "        else:\n",
        "            file_name = \"%s_persona_split_%s_human_annotated.json\" % (lang, split)\n",
        "        print(file_name)\n",
        "        with open(\"/content/drive/MyDrive/Xpersona-master/dataset/\" + file_name, 'rb') as json_file:\n",
        "            data = json.load(json_file)\n",
        "            file_out_x = open(\"/content/drive/MyDrive/Xpersona-master/%s.x.%s\" % (split, lang.lower()), \"w\")  # dialog history\n",
        "            file_out_y = open(\"/content/drive/MyDrive/Xpersona-master/%s.y.%s\" % (split, lang.lower()), \"w\")  # response\n",
        "            for each_dialog in tqdm(data):\n",
        "                # preprocess each dialogue\n",
        "                persona_list = each_dialog[\"persona\"]\n",
        "                persona_str = \"\"\n",
        "                # persona\n",
        "                for persona in persona_list:\n",
        "                    persona_str = persona_str + persona + \" \"\n",
        "\n",
        "                # dialogue\n",
        "                dialogue_tuples = each_dialog[\"dialogue\"]\n",
        "                turns = []\n",
        "                for tuple_ in dialogue_tuples:\n",
        "                    for turn in tuple_:\n",
        "                        turns.append(turn)\n",
        "\n",
        "                for idx in range(len(turns)):\n",
        "                    if idx % 2 == 0:\n",
        "                        continue\n",
        "                    if idx == 1:\n",
        "                        user_turn = turns[idx-1]\n",
        "                        system_turn = turns[idx]\n",
        "                        file_out_x.write(persona_str + user_turn + \"\\n\")\n",
        "                        file_out_y.write(system_turn + \"\\n\")\n",
        "                    else:\n",
        "                        user_turn1 = turns[idx-3]\n",
        "                        system_turn1 = turns[idx-2]\n",
        "                        user_turn2 = turns[idx-1]\n",
        "                        system_turn2 = turns[idx]\n",
        "                        file_out_x.write(persona_str + user_turn1 + \" \" + system_turn1 + \" \" + user_turn2 + \"\\n\")\n",
        "                        file_out_y.write(system_turn2 + \"\\n\")\n",
        "\n",
        "            file_out_x.close()\n",
        "            file_out_y.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0MYFPcEbd6m",
        "outputId": "653804da-ab0d-4544-f4ad-ad12387c3f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En_persona_train.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16878/16878 [00:00<00:00, 31283.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En_persona_valid.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 41896.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En_persona_test.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 28314.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zh_persona_train_corrected.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16878/16878 [00:00<00:00, 27581.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zh_persona_split_valid_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 222/222 [00:00<00:00, 24538.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zh_persona_split_test_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 222/222 [00:00<00:00, 18538.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fr_persona_train_corrected.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16878/16878 [00:00<00:00, 24501.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fr_persona_split_valid_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 248/248 [00:00<00:00, 27001.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fr_persona_split_test_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 249/249 [00:00<00:00, 17515.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id_persona_train_corrected.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16878/16878 [00:00<00:00, 29786.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id_persona_split_valid_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 484/484 [00:00<00:00, 26655.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Id_persona_split_test_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 484/484 [00:00<00:00, 24175.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It_persona_train_corrected.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16878/16878 [00:01<00:00, 15146.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It_persona_split_valid_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:00<00:00, 16843.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It_persona_split_test_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:00<00:00, 12641.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jp_persona_train_corrected.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16878/16878 [00:00<00:00, 21601.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jp_persona_split_valid_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 13024.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jp_persona_split_test_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 275/275 [00:00<00:00, 17683.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ko_persona_train_corrected.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16878/16878 [00:00<00:00, 19991.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ko_persona_split_valid_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 299/299 [00:00<00:00, 18256.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ko_persona_split_test_human_annotated.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [00:00<00:00, 16211.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "class LogFormatter():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def format(self, record):\n",
        "        elapsed_seconds = round(record.created - self.start_time)\n",
        "\n",
        "        prefix = \"%s - %s - %s\" % (\n",
        "            record.levelname,\n",
        "            time.strftime('%x %X'),\n",
        "            timedelta(seconds=elapsed_seconds)\n",
        "        )\n",
        "        message = record.getMessage()\n",
        "        message = message.replace('\\n', '\\n' + ' ' * (len(prefix) + 3))\n",
        "        return \"%s - %s\" % (prefix, message) if message else ''\n",
        "\n",
        "\n",
        "def create_logger(filepath, rank):\n",
        "    \"\"\"\n",
        "    Create a logger.\n",
        "    Use a different log file for each process.\n",
        "    \"\"\"\n",
        "    # create log formatter\n",
        "    log_formatter = LogFormatter()\n",
        "\n",
        "    # create file handler and set level to debug\n",
        "    if filepath is not None:\n",
        "        if rank > 0:\n",
        "            filepath = '%s-%i' % (filepath, rank)\n",
        "        file_handler = logging.FileHandler(filepath, \"a\")\n",
        "        file_handler.setLevel(logging.DEBUG)\n",
        "        file_handler.setFormatter(log_formatter)\n",
        "\n",
        "    # create console handler and set level to info\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "    console_handler.setFormatter(log_formatter)\n",
        "\n",
        "    # create logger and set level to debug\n",
        "    logger = logging.getLogger()\n",
        "    logger.handlers = []\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    logger.propagate = False\n",
        "    if filepath is not None:\n",
        "        logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    # reset logger elapsed time\n",
        "    def reset_time():\n",
        "        log_formatter.start_time = time.time()\n",
        "    logger.reset_time = reset_time\n",
        "\n",
        "    return logger"
      ],
      "metadata": {
        "id": "S2ISbF-my09K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "import getpass\n",
        "import argparse\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "FALSY_STRINGS = {'off', 'false', '0'}\n",
        "TRUTHY_STRINGS = {'on', 'true', '1'}\n",
        "\n",
        "DUMP_PATH = '/checkpoint/%s/dumped' % getpass.getuser()\n",
        "DYNAMIC_COEFF = ['lambda_clm', 'lambda_mlm', 'lambda_pc', 'lambda_ae', 'lambda_mt', 'lambda_bt', \"lambda_s2slm\"]\n",
        "\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "\n",
        "def bool_flag(s):\n",
        "    \"\"\"\n",
        "    Parse boolean arguments from the command line.\n",
        "    \"\"\"\n",
        "    if s.lower() in FALSY_STRINGS:\n",
        "        return False\n",
        "    elif s.lower() in TRUTHY_STRINGS:\n",
        "        return True\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError(\"Invalid value for a boolean flag!\")\n",
        "\n",
        "\n",
        "def initialize_exp(params):\n",
        "    \"\"\"\n",
        "    Initialize the experience:\n",
        "    - dump parameters\n",
        "    - create a logger\n",
        "    \"\"\"\n",
        "    # dump parameters\n",
        "    get_dump_path(params)\n",
        "    pickle.dump(params, open(os.path.join(params.dump_path, 'params.pkl'), 'wb'))\n",
        "\n",
        "    # get running command\n",
        "    command = [\"python\", sys.argv[0]]\n",
        "    for x in sys.argv[1:]:\n",
        "        if x.startswith('--'):\n",
        "            assert '\"' not in x and \"'\" not in x\n",
        "            command.append(x)\n",
        "        else:\n",
        "            assert \"'\" not in x\n",
        "            if re.match('^[a-zA-Z0-9_]+$', x):\n",
        "                command.append(\"%s\" % x)\n",
        "            else:\n",
        "                command.append(\"'%s'\" % x)\n",
        "    command = ' '.join(command)\n",
        "    params.command = command + ' --exp_id \"%s\"' % params.exp_id\n",
        "\n",
        "    # check experiment name\n",
        "    assert len(params.exp_name.strip()) > 0\n",
        "\n",
        "    # create a logger\n",
        "    logger = create_logger(os.path.join(params.dump_path, 'train.log'), rank=getattr(params, 'global_rank', 0))\n",
        "    logger.info(\"============ Initialized logger ============\")\n",
        "    logger.info(\"\\n\".join(\"%s: %s\" % (k, str(v))\n",
        "                          for k, v in sorted(dict(vars(params)).items())))\n",
        "    logger.info(\"The experiment will be stored in %s\\n\" % params.dump_path)\n",
        "    logger.info(\"Running command: %s\" % command)\n",
        "    logger.info(\"\")\n",
        "    return logger\n",
        "\n",
        "\n",
        "def get_dump_path(params):\n",
        "    \"\"\"\n",
        "    Create a directory to store the experiment.\n",
        "    \"\"\"\n",
        "    dump_path = DUMP_PATH if params.dump_path == '' else params.dump_path\n",
        "    assert len(params.exp_name) > 0\n",
        "\n",
        "    # create the sweep path if it does not exist\n",
        "    sweep_path = os.path.join(dump_path, params.exp_name)\n",
        "    if not os.path.exists(sweep_path):\n",
        "        subprocess.Popen(\"mkdir -p %s\" % sweep_path, shell=True).wait()\n",
        "\n",
        "    # create an ID for the job if it is not given in the parameters.\n",
        "    # if we run on the cluster, the job ID is the one of Chronos.\n",
        "    # otherwise, it is randomly generated\n",
        "    if params.exp_id == '':\n",
        "        chronos_job_id = os.environ.get('CHRONOS_JOB_ID')\n",
        "        slurm_job_id = os.environ.get('SLURM_JOB_ID')\n",
        "        assert chronos_job_id is None or slurm_job_id is None\n",
        "        exp_id = chronos_job_id if chronos_job_id is not None else slurm_job_id\n",
        "        if exp_id is None:\n",
        "            chars = 'abcdefghijklmnopqrstuvwxyz0123456789'\n",
        "            while True:\n",
        "                exp_id = ''.join(random.choice(chars) for _ in range(10))\n",
        "                if not os.path.isdir(os.path.join(sweep_path, exp_id)):\n",
        "                    break\n",
        "        else:\n",
        "            assert exp_id.isdigit()\n",
        "        params.exp_id = exp_id\n",
        "\n",
        "    # create the dump folder / update parameters\n",
        "    params.dump_path = os.path.join(sweep_path, params.exp_id)\n",
        "    if not os.path.isdir(params.dump_path):\n",
        "        subprocess.Popen(\"mkdir -p %s\" % params.dump_path, shell=True).wait()\n",
        "\n",
        "\n",
        "def to_cuda(*args):\n",
        "    \"\"\"\n",
        "    Move tensors to CUDA.\n",
        "    \"\"\"\n",
        "    return [None if x is None else x.cuda() for x in args]\n",
        "\n",
        "\n",
        "def restore_segmentation(path):\n",
        "    \"\"\"\n",
        "    Take a file segmented with BPE and restore it to its original segmentation.\n",
        "    \"\"\"\n",
        "    assert os.path.isfile(path)\n",
        "    restore_cmd = \"sed -i -r 's/(@@ )|(@@ ?$)//g' %s\"\n",
        "    subprocess.Popen(restore_cmd % path, shell=True).wait()\n",
        "\n",
        "\n",
        "def parse_lambda_config(params):\n",
        "    \"\"\"\n",
        "    Parse the configuration of lambda coefficient (for scheduling).\n",
        "    x = \"3\"                  # lambda will be a constant equal to x\n",
        "    x = \"0:1,1000:0\"         # lambda will start from 1 and linearly decrease to 0 during the first 1000 iterations\n",
        "    x = \"0:0,1000:0,2000:1\"  # lambda will be equal to 0 for the first 1000 iterations, then will linearly increase to 1 until iteration 2000\n",
        "    \"\"\"\n",
        "    for name in DYNAMIC_COEFF:\n",
        "        x = getattr(params, name)\n",
        "        split = x.split(',')\n",
        "        if len(split) == 1:\n",
        "            setattr(params, name, float(x))\n",
        "            setattr(params, name + '_config', None)\n",
        "        else:\n",
        "            split = [s.split(':') for s in split]\n",
        "            assert all(len(s) == 2 for s in split)\n",
        "            assert all(k.isdigit() for k, _ in split)\n",
        "            assert all(int(split[i][0]) < int(split[i + 1][0]) for i in range(len(split) - 1))\n",
        "            setattr(params, name, float(split[0][1]))\n",
        "            setattr(params, name + '_config', [(int(k), float(v)) for k, v in split])\n",
        "\n",
        "\n",
        "def get_lambda_value(config, n_iter):\n",
        "    \"\"\"\n",
        "    Compute a lambda value according to its schedule configuration.\n",
        "    \"\"\"\n",
        "    ranges = [i for i in range(len(config) - 1) if config[i][0] <= n_iter < config[i + 1][0]]\n",
        "    if len(ranges) == 0:\n",
        "        assert n_iter >= config[-1][0]\n",
        "        return config[-1][1]\n",
        "    assert len(ranges) == 1\n",
        "    i = ranges[0]\n",
        "    x_a, y_a = config[i]\n",
        "    x_b, y_b = config[i + 1]\n",
        "    return y_a + (n_iter - x_a) * float(y_b - y_a) / float(x_b - x_a)\n",
        "\n",
        "\n",
        "def update_lambdas(params, n_iter):\n",
        "    \"\"\"\n",
        "    Update all lambda coefficients.\n",
        "    \"\"\"\n",
        "    for name in DYNAMIC_COEFF:\n",
        "        config = getattr(params, name + '_config')\n",
        "        if config is not None:\n",
        "            setattr(params, name, get_lambda_value(config, n_iter))\n",
        "\n",
        "\n",
        "def set_sampling_probs(data, params):\n",
        "    \"\"\"\n",
        "    Set the probability of sampling specific languages / language pairs during training.\n",
        "    \"\"\"\n",
        "    coeff = params.lg_sampling_factor\n",
        "    if coeff == -1:\n",
        "        return\n",
        "    assert coeff > 0\n",
        "\n",
        "    # monolingual data\n",
        "    params.mono_list = [k for k, v in data['mono_stream'].items() if 'train' in v]\n",
        "    if len(params.mono_list) > 0:\n",
        "        probs = np.array([1.0 * len(data['mono_stream'][lang]['train']) for lang in params.mono_list])\n",
        "        probs /= probs.sum()\n",
        "        probs = np.array([p ** coeff for p in probs])\n",
        "        probs /= probs.sum()\n",
        "        params.mono_probs = probs\n",
        "\n",
        "    # parallel data\n",
        "    params.para_list = [k for k, v in data['para'].items() if 'train' in v]\n",
        "    if len(params.para_list) > 0:\n",
        "        probs = np.array([1.0 * len(data['para'][(l1, l2)]['train']) for (l1, l2) in params.para_list])\n",
        "        probs /= probs.sum()\n",
        "        probs = np.array([p ** coeff for p in probs])\n",
        "        probs /= probs.sum()\n",
        "        params.para_probs = probs\n",
        "\n",
        "\n",
        "def concat_batches(x1, len1, lang1_id, x2, len2, lang2_id, pad_idx, eos_idx, reset_positions):\n",
        "    \"\"\"\n",
        "    Concat batches with different languages.\n",
        "    \"\"\"\n",
        "    assert reset_positions is False or lang1_id != lang2_id\n",
        "    lengths = len1 + len2\n",
        "    if not reset_positions:\n",
        "        lengths -= 1\n",
        "    slen, bs = lengths.max().item(), lengths.size(0)\n",
        "\n",
        "    x = x1.new(slen, bs).fill_(pad_idx)\n",
        "    x[:len1.max().item()].copy_(x1)\n",
        "    positions = torch.arange(slen)[:, None].repeat(1, bs).to(x1.device)\n",
        "    langs = x1.new(slen, bs).fill_(lang1_id)\n",
        "\n",
        "    for i in range(bs):\n",
        "        l1 = len1[i] if reset_positions else len1[i] - 1\n",
        "        x[l1:l1 + len2[i], i].copy_(x2[:len2[i], i])\n",
        "        if reset_positions:\n",
        "            positions[l1:, i] -= len1[i]\n",
        "        langs[l1:, i] = lang2_id\n",
        "\n",
        "    assert (x == eos_idx).long().sum().item() == (4 if reset_positions else 3) * bs\n",
        "\n",
        "    return x, lengths, positions, langs\n",
        "\n",
        "\n",
        "def truncate(x, lengths, max_len, eos_index):\n",
        "    \"\"\"\n",
        "    Truncate long sentences.\n",
        "    \"\"\"\n",
        "    if lengths.max().item() > max_len:\n",
        "        x = x[:max_len].clone()\n",
        "        lengths = lengths.clone()\n",
        "        for i in range(len(lengths)):\n",
        "            if lengths[i] > max_len:\n",
        "                lengths[i] = max_len\n",
        "                x[max_len - 1, i] = eos_index\n",
        "    return x, lengths\n",
        "\n",
        "\n",
        "def shuf_order(langs, params=None, n=5):\n",
        "    \"\"\"\n",
        "    Randomize training order.\n",
        "    \"\"\"\n",
        "    if len(langs) == 0:\n",
        "        return []\n",
        "\n",
        "    if params is None:\n",
        "        return [langs[i] for i in np.random.permutation(len(langs))]\n",
        "\n",
        "    # sample monolingual and parallel languages separately\n",
        "    mono = [l1 for l1, l2 in langs if l2 is None]\n",
        "    para = [(l1, l2) for l1, l2 in langs if l2 is not None]\n",
        "\n",
        "    # uniform / weighted sampling\n",
        "    if params.lg_sampling_factor == -1:\n",
        "        p_mono = None\n",
        "        p_para = None\n",
        "    else:\n",
        "        p_mono = np.array([params.mono_probs[params.mono_list.index(k)] for k in mono])\n",
        "        p_para = np.array([params.para_probs[params.para_list.index(tuple(sorted(k)))] for k in para])\n",
        "        p_mono = p_mono / p_mono.sum()\n",
        "        p_para = p_para / p_para.sum()\n",
        "\n",
        "    s_mono = [mono[i] for i in np.random.choice(len(mono), size=min(n, len(mono)), p=p_mono, replace=True)] if len(mono) > 0 else []\n",
        "    s_para = [para[i] for i in np.random.choice(len(para), size=min(n, len(para)), p=p_para, replace=True)] if len(para) > 0 else []\n",
        "\n",
        "    assert len(s_mono) + len(s_para) > 0\n",
        "    return [(lang, None) for lang in s_mono] + s_para\n",
        "\n",
        "\n",
        "def find_modules(module, module_name, module_instance, found):\n",
        "    \"\"\"\n",
        "    Recursively find all instances of a specific module inside a module.\n",
        "    \"\"\"\n",
        "    if isinstance(module, module_instance):\n",
        "        found.append((module_name, module))\n",
        "    else:\n",
        "        for name, child in module.named_children():\n",
        "            name = ('%s[%s]' if name.isdigit() else '%s.%s') % (module_name, name)\n",
        "            find_modules(child, name, module_instance, found)\n",
        "\n",
        "\n",
        "def mask_out_v2(params, x, lens, enc_lens=None):\n",
        "    \"\"\"\n",
        "    Decide of random words to mask out, and what target they get assigned.\n",
        "    Args:\n",
        "        enc_lens: lengths of encoder parts, if set, only target seq part will be masked.\n",
        "    \"\"\"\n",
        "    slen, bs = x.size()\n",
        "\n",
        "    if params.sample_alpha == 0:\n",
        "        pred_mask = np.random.rand(slen, bs) <= params.word_pred\n",
        "        # pred_mask = torch.from_numpy(pred_mask.astype(np.uint8))\n",
        "        pred_mask = torch.from_numpy(pred_mask.astype(np.bool_))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    # do not predict padding\n",
        "    pred_mask[x == params.pad_index] = 0\n",
        "    pred_mask[0] = 0\n",
        "    # if enc_lens isn't None, do not predict src seq\n",
        "    if enc_lens is not None:\n",
        "        arng = torch.arange(slen)\n",
        "        pred_mask[enc_lens[None, :] > arng[:, None]] = 0\n",
        "    \n",
        "    # TODO what if all words are not masked?\n",
        "    _x_real = x[pred_mask]\n",
        "    _x_mask = _x_real.clone().fill_(params.mask_index)\n",
        "    x = x.masked_scatter(pred_mask, _x_mask)\n",
        "\n",
        "    return x, _x_real, pred_mask"
      ],
      "metadata": {
        "id": "q-b5BuOKxqbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "import inspect\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "class Adam(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Same as https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py,\n",
        "    without amsgrad, with step in a tensor, and states initialization in __init__.\n",
        "    It was important to add `.item()` in `state['step'].item()`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                state['step'] = 0  # torch.zeros(1)\n",
        "                state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super().__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"\n",
        "        Step.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # if group['weight_decay'] != 0:\n",
        "                #     grad.add_(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                # denom = exp_avg_sq.sqrt().clamp_(min=group['eps'])\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state['step']  # .item()\n",
        "                bias_correction2 = 1 - beta2 ** state['step']  # .item()\n",
        "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    p.data.add_(-group['weight_decay'] * group['lr'], p.data)\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class AdamInverseSqrtWithWarmup(Adam):\n",
        "    \"\"\"\n",
        "    Decay the LR based on the inverse square root of the update number.\n",
        "    We also support a warmup phase where we linearly increase the learning rate\n",
        "    from some initial learning rate (`warmup-init-lr`) until the configured\n",
        "    learning rate (`lr`). Thereafter we decay proportional to the number of\n",
        "    updates, with a decay factor set to align with the configured learning rate.\n",
        "    During warmup:\n",
        "        lrs = torch.linspace(warmup_init_lr, lr, warmup_updates)\n",
        "        lr = lrs[update_num]\n",
        "    After warmup:\n",
        "        lr = decay_factor / sqrt(update_num)\n",
        "    where\n",
        "        decay_factor = lr * sqrt(warmup_updates)\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, warmup_updates=4000, warmup_init_lr=1e-7,\n",
        "                 exp_factor=0.5):\n",
        "        super().__init__(\n",
        "            params,\n",
        "            lr=warmup_init_lr,\n",
        "            betas=betas,\n",
        "            eps=eps,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "\n",
        "        # linearly warmup for the first warmup_updates\n",
        "        self.warmup_updates = warmup_updates\n",
        "        self.warmup_init_lr = warmup_init_lr\n",
        "        warmup_end_lr = lr\n",
        "        self.lr_step = (warmup_end_lr - warmup_init_lr) / warmup_updates\n",
        "\n",
        "        # then, decay prop. to the inverse square root of the update number\n",
        "        self.exp_factor = exp_factor\n",
        "        self.decay_factor = warmup_end_lr * warmup_updates ** self.exp_factor\n",
        "\n",
        "        # total number of updates\n",
        "        for param_group in self.param_groups:\n",
        "            param_group['num_updates'] = 0\n",
        "\n",
        "    def get_lr_for_step(self, num_updates):\n",
        "        if num_updates < self.warmup_updates:\n",
        "            return self.warmup_init_lr + num_updates * self.lr_step\n",
        "        else:\n",
        "            return self.decay_factor * (num_updates ** -self.exp_factor)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        super().step(closure)\n",
        "        for param_group in self.param_groups:\n",
        "            param_group['num_updates'] += 1\n",
        "            param_group['lr'] = self.get_lr_for_step(param_group['num_updates'])\n",
        "\n",
        "\n",
        "class AdamCosineWithWarmup(Adam):\n",
        "    \"\"\"\n",
        "    Assign LR based on a cyclical schedule that follows the cosine function.\n",
        "    See https://arxiv.org/pdf/1608.03983.pdf for details.\n",
        "    We also support a warmup phase where we linearly increase the learning rate\n",
        "    from some initial learning rate (``--warmup-init-lr``) until the configured\n",
        "    learning rate (``--lr``).\n",
        "    During warmup::\n",
        "      lrs = torch.linspace(args.warmup_init_lr, args.lr, args.warmup_updates)\n",
        "      lr = lrs[update_num]\n",
        "    After warmup::\n",
        "      lr = lr_min + 0.5*(lr_max - lr_min)*(1 + cos(t_curr / t_i))\n",
        "    where ``t_curr`` is current percentage of updates within the current period\n",
        "    range and ``t_i`` is the current period range, which is scaled by ``t_mul``\n",
        "    after every iteration.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, warmup_updates=4000, warmup_init_lr=1e-7,\n",
        "                 min_lr=1e-9, init_period=1000000, period_mult=1, lr_shrink=0.75):\n",
        "        super().__init__(\n",
        "            params,\n",
        "            lr=warmup_init_lr,\n",
        "            betas=betas,\n",
        "            eps=eps,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "\n",
        "        # linearly warmup for the first warmup_updates\n",
        "        self.warmup_updates = warmup_updates\n",
        "        self.warmup_init_lr = warmup_init_lr\n",
        "        warmup_end_lr = lr\n",
        "        self.lr_step = (warmup_end_lr - warmup_init_lr) / warmup_updates\n",
        "\n",
        "        # then, apply cosine scheduler\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = lr\n",
        "        self.period = init_period\n",
        "        self.period_mult = period_mult\n",
        "        self.lr_shrink = lr_shrink\n",
        "\n",
        "        # total number of updates\n",
        "        for param_group in self.param_groups:\n",
        "            param_group['num_updates'] = 0\n",
        "\n",
        "    def get_lr_for_step(self, num_updates):\n",
        "        if num_updates < self.warmup_updates:\n",
        "            return self.warmup_init_lr + num_updates * self.lr_step\n",
        "        else:\n",
        "            t = num_updates - self.warmup_updates\n",
        "            if self.period_mult == 1:\n",
        "                pid = math.floor(t / self.period)\n",
        "                t_i = self.period\n",
        "                t_curr = t - (self.period * pid)\n",
        "            else:\n",
        "                pid = math.floor(math.log(1 - t / self.period * (1 - self.period_mult), self.period_mult))\n",
        "                t_i = self.period * (self.period_mult ** pid)\n",
        "                t_curr = t - (1 - self.period_mult ** pid) / (1 - self.period_mult) * self.period\n",
        "            lr_shrink = self.lr_shrink ** pid\n",
        "            min_lr = self.min_lr * lr_shrink\n",
        "            max_lr = self.max_lr * lr_shrink\n",
        "            return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * t_curr / t_i))\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        super().step(closure)\n",
        "        for param_group in self.param_groups:\n",
        "            param_group['num_updates'] += 1\n",
        "            param_group['lr'] = self.get_lr_for_step(param_group['num_updates'])\n",
        "\n",
        "\n",
        "def get_optimizer(parameters, s):\n",
        "    \"\"\"\n",
        "    Parse optimizer parameters.\n",
        "    Input should be of the form:\n",
        "        - \"sgd,lr=0.01\"\n",
        "        - \"adagrad,lr=0.1,lr_decay=0.05\"\n",
        "    \"\"\"\n",
        "    if \",\" in s:\n",
        "        method = s[:s.find(',')]\n",
        "        optim_params = {}\n",
        "        for x in s[s.find(',') + 1:].split(','):\n",
        "            split = x.split('=')\n",
        "            assert len(split) == 2\n",
        "            assert re.match(\"^[+-]?(\\d+(\\.\\d*)?|\\.\\d+)$\", split[1]) is not None\n",
        "            optim_params[split[0]] = float(split[1])\n",
        "    else:\n",
        "        method = s\n",
        "        optim_params = {}\n",
        "\n",
        "    if method == 'adadelta':\n",
        "        optim_fn = optim.Adadelta\n",
        "    elif method == 'adagrad':\n",
        "        optim_fn = optim.Adagrad\n",
        "    elif method == 'adam':\n",
        "        optim_fn = Adam\n",
        "        optim_params['betas'] = (optim_params.get('beta1', 0.9), optim_params.get('beta2', 0.999))\n",
        "        optim_params.pop('beta1', None)\n",
        "        optim_params.pop('beta2', None)\n",
        "    elif method == 'adam_inverse_sqrt':\n",
        "        optim_fn = AdamInverseSqrtWithWarmup\n",
        "        optim_params['betas'] = (optim_params.get('beta1', 0.9), optim_params.get('beta2', 0.999))\n",
        "        optim_params.pop('beta1', None)\n",
        "        optim_params.pop('beta2', None)\n",
        "    elif method == 'adam_cosine':\n",
        "        optim_fn = AdamCosineWithWarmup\n",
        "        optim_params['betas'] = (optim_params.get('beta1', 0.9), optim_params.get('beta2', 0.999))\n",
        "        optim_params.pop('beta1', None)\n",
        "        optim_params.pop('beta2', None)\n",
        "    elif method == 'adamax':\n",
        "        optim_fn = optim.Adamax\n",
        "    elif method == 'asgd':\n",
        "        optim_fn = optim.ASGD\n",
        "    elif method == 'rmsprop':\n",
        "        optim_fn = optim.RMSprop\n",
        "    elif method == 'rprop':\n",
        "        optim_fn = optim.Rprop\n",
        "    elif method == 'sgd':\n",
        "        optim_fn = optim.SGD\n",
        "        assert 'lr' in optim_params\n",
        "    else:\n",
        "        raise Exception('Unknown optimization method: \"%s\"' % method)\n",
        "\n",
        "    # check that we give good parameters to the optimizer\n",
        "    expected_args = inspect.getargspec(optim_fn.__init__)[0]\n",
        "    assert expected_args[:2] == ['self', 'params']\n",
        "    if not all(k in expected_args[2:] for k in optim_params.keys()):\n",
        "        raise Exception('Unexpected parameters: expected \"%s\", got \"%s\"' % (\n",
        "            str(expected_args[2:]), str(optim_params.keys())))\n",
        "\n",
        "    return optim_fn(parameters, **optim_params)"
      ],
      "metadata": {
        "id": "5nykQ6CIy_YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import getLogger\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class StreamDataset(object):\n",
        "\n",
        "    def __init__(self, sent, pos, bs, params):\n",
        "        \"\"\"\n",
        "        Prepare batches for data iterator.\n",
        "        \"\"\"\n",
        "        bptt = params.bptt\n",
        "        self.eos = params.eos_index\n",
        "\n",
        "        # checks\n",
        "        assert len(pos) == (sent == self.eos).sum()\n",
        "        assert len(pos) == (sent[pos[:, 1]] == self.eos).sum()\n",
        "\n",
        "        n_tokens = len(sent)\n",
        "        n_batches = math.ceil(n_tokens / (bs * bptt))\n",
        "        t_size = n_batches * bptt * bs\n",
        "\n",
        "        buffer = np.zeros(t_size, dtype=sent.dtype) + self.eos\n",
        "        buffer[t_size - n_tokens:] = sent\n",
        "        buffer = buffer.reshape((bs, n_batches * bptt)).T\n",
        "        self.data = np.zeros((n_batches * bptt + 1, bs), dtype=sent.dtype) + self.eos\n",
        "        self.data[1:] = buffer\n",
        "\n",
        "        self.bptt = bptt\n",
        "        self.n_tokens = n_tokens\n",
        "        self.n_batches = n_batches\n",
        "        self.n_sentences = len(pos)\n",
        "        self.lengths = torch.LongTensor(bs).fill_(bptt)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        return self.n_sentences\n",
        "\n",
        "    def select_data(self, a, b):\n",
        "        \"\"\"\n",
        "        Only select a subset of the dataset.\n",
        "        \"\"\"\n",
        "        if not (0 <= a < b <= self.n_batches):\n",
        "            logger.warning(\"Invalid split values: %i %i - %i\" % (a, b, self.n_batches))\n",
        "            return\n",
        "        assert 0 <= a < b <= self.n_batches\n",
        "        logger.info(\"Selecting batches from %i to %i ...\" % (a, b))\n",
        "\n",
        "        # sub-select\n",
        "        self.data = self.data[a * self.bptt:b * self.bptt]\n",
        "        self.n_batches = b - a\n",
        "        self.n_sentences = (self.data == self.eos).sum().item()\n",
        "\n",
        "    def get_iterator(self, shuffle, subsample=1):\n",
        "        \"\"\"\n",
        "        Return a sentences iterator.\n",
        "        \"\"\"\n",
        "        indexes = (np.random.permutation if shuffle else range)(self.n_batches // subsample)\n",
        "        for i in indexes:\n",
        "            a = self.bptt * i\n",
        "            b = self.bptt * (i + 1)\n",
        "            yield torch.from_numpy(self.data[a:b].astype(np.int64)), self.lengths\n",
        "\n",
        "\n",
        "class Dataset(object):\n",
        "\n",
        "    def __init__(self, sent, pos, params):\n",
        "\n",
        "        self.eos_index = params.eos_index\n",
        "        self.pad_index = params.pad_index\n",
        "        self.batch_size = params.batch_size\n",
        "        self.tokens_per_batch = params.tokens_per_batch\n",
        "        self.max_batch_size = params.max_batch_size\n",
        "\n",
        "        self.sent = sent\n",
        "        self.pos = pos\n",
        "        self.lengths = self.pos[:, 1] - self.pos[:, 0]\n",
        "\n",
        "        # check number of sentences\n",
        "        assert len(self.pos) == (self.sent == self.eos_index).sum()\n",
        "\n",
        "        # # remove empty sentences\n",
        "        # self.remove_empty_sentences()\n",
        "\n",
        "        # sanity checks\n",
        "        self.check()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.pos)\n",
        "\n",
        "    def check(self):\n",
        "        \"\"\"\n",
        "        Sanity checks.\n",
        "        \"\"\"\n",
        "        eos = self.eos_index\n",
        "        assert len(self.pos) == (self.sent[self.pos[:, 1]] == eos).sum()  # check sentences indices\n",
        "        # assert self.lengths.min() > 0                                     # check empty sentences\n",
        "\n",
        "    def batch_sentences(self, sentences):\n",
        "        \"\"\"\n",
        "        Take as input a list of n sentences (torch.LongTensor vectors) and return\n",
        "        a tensor of size (slen, n) where slen is the length of the longest\n",
        "        sentence, and a vector lengths containing the length of each sentence.\n",
        "        \"\"\"\n",
        "        # sentences = sorted(sentences, key=lambda x: len(x), reverse=True)\n",
        "        lengths = torch.LongTensor([len(s) + 2 for s in sentences])\n",
        "        sent = torch.LongTensor(lengths.max().item(), lengths.size(0)).fill_(self.pad_index)\n",
        "\n",
        "        sent[0] = self.eos_index\n",
        "        for i, s in enumerate(sentences):\n",
        "            if lengths[i] > 2:  # if sentence not empty\n",
        "                sent[1:lengths[i] - 1, i].copy_(torch.from_numpy(s.astype(np.int64)))\n",
        "            sent[lengths[i] - 1, i] = self.eos_index\n",
        "\n",
        "        return sent, lengths\n",
        "\n",
        "    def remove_empty_sentences(self):\n",
        "        \"\"\"\n",
        "        Remove empty sentences.\n",
        "        \"\"\"\n",
        "        init_size = len(self.pos)\n",
        "        indices = np.arange(len(self.pos))\n",
        "        indices = indices[self.lengths[indices] > 0]\n",
        "        self.pos = self.pos[indices]\n",
        "        self.lengths = self.pos[:, 1] - self.pos[:, 0]\n",
        "        logger.info(\"Removed %i empty sentences.\" % (init_size - len(indices)))\n",
        "        self.check()\n",
        "\n",
        "    def remove_long_sentences(self, max_len):\n",
        "        \"\"\"\n",
        "        Remove sentences exceeding a certain length.\n",
        "        \"\"\"\n",
        "        assert max_len >= 0\n",
        "        if max_len == 0:\n",
        "            return\n",
        "        init_size = len(self.pos)\n",
        "        indices = np.arange(len(self.pos))\n",
        "        indices = indices[self.lengths[indices] <= max_len]\n",
        "        self.pos = self.pos[indices]\n",
        "        self.lengths = self.pos[:, 1] - self.pos[:, 0]\n",
        "        logger.info(\"Removed %i too long sentences.\" % (init_size - len(indices)))\n",
        "        self.check()\n",
        "\n",
        "    def select_data(self, a, b):\n",
        "        \"\"\"\n",
        "        Only select a subset of the dataset.\n",
        "        \"\"\"\n",
        "        assert 0 <= a < b <= len(self.pos)\n",
        "        logger.info(\"Selecting sentences from %i to %i ...\" % (a, b))\n",
        "\n",
        "        # sub-select\n",
        "        self.pos = self.pos[a:b]\n",
        "        self.lengths = self.pos[:, 1] - self.pos[:, 0]\n",
        "\n",
        "        # re-index\n",
        "        min_pos = self.pos.min()\n",
        "        max_pos = self.pos.max()\n",
        "        self.pos -= min_pos\n",
        "        self.sent = self.sent[min_pos:max_pos + 1]\n",
        "\n",
        "        # sanity checks\n",
        "        self.check()\n",
        "\n",
        "    def get_batches_iterator(self, batches, return_indices):\n",
        "        \"\"\"\n",
        "        Return a sentences iterator, given the associated sentence batches.\n",
        "        \"\"\"\n",
        "        assert type(return_indices) is bool\n",
        "\n",
        "        for sentence_ids in batches:\n",
        "            if 0 < self.max_batch_size < len(sentence_ids):\n",
        "                np.random.shuffle(sentence_ids)\n",
        "                sentence_ids = sentence_ids[:self.max_batch_size]\n",
        "            pos = self.pos[sentence_ids]\n",
        "            sent = [self.sent[a:b] for a, b in pos]\n",
        "            sent = self.batch_sentences(sent)\n",
        "            yield (sent, sentence_ids) if return_indices else sent\n",
        "\n",
        "    def get_iterator(self, shuffle, group_by_size=False, n_sentences=-1, seed=None, return_indices=False):\n",
        "        \"\"\"\n",
        "        Return a sentences iterator.\n",
        "        \"\"\"\n",
        "        assert seed is None or shuffle is True and type(seed) is int\n",
        "        rng = np.random.RandomState(seed)\n",
        "        n_sentences = len(self.pos) if n_sentences == -1 else n_sentences\n",
        "        assert 0 < n_sentences <= len(self.pos)\n",
        "        assert type(shuffle) is bool and type(group_by_size) is bool\n",
        "        assert group_by_size is False or shuffle is True\n",
        "\n",
        "        # sentence lengths\n",
        "        lengths = self.lengths + 2\n",
        "\n",
        "        # select sentences to iterate over\n",
        "        if shuffle:\n",
        "            indices = rng.permutation(len(self.pos))[:n_sentences]\n",
        "        else:\n",
        "            indices = np.arange(n_sentences)\n",
        "\n",
        "        # group sentences by lengths\n",
        "        if group_by_size:\n",
        "            indices = indices[np.argsort(lengths[indices], kind='mergesort')]\n",
        "\n",
        "        # create batches - either have a fixed number of sentences, or a similar number of tokens\n",
        "        if self.tokens_per_batch == -1:\n",
        "            batches = np.array_split(indices, math.ceil(len(indices) * 1. / self.batch_size))\n",
        "        else:\n",
        "            batch_ids = np.cumsum(lengths[indices]) // self.tokens_per_batch\n",
        "            _, bounds = np.unique(batch_ids, return_index=True)\n",
        "            batches = [indices[bounds[i]:bounds[i + 1]] for i in range(len(bounds) - 1)]\n",
        "            if bounds[-1] < len(indices):\n",
        "                batches.append(indices[bounds[-1]:])\n",
        "\n",
        "        # optionally shuffle batches\n",
        "        if shuffle:\n",
        "            rng.shuffle(batches)\n",
        "\n",
        "        # sanity checks\n",
        "        assert n_sentences == sum([len(x) for x in batches])\n",
        "        assert lengths[indices].sum() == sum([lengths[x].sum() for x in batches])\n",
        "        # assert set.union(*[set(x.tolist()) for x in batches]) == set(range(n_sentences))  # slow\n",
        "\n",
        "        # return the iterator\n",
        "        return self.get_batches_iterator(batches, return_indices)\n",
        "\n",
        "\n",
        "class ParallelDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sent1, pos1, sent2, pos2, params):\n",
        "\n",
        "        self.eos_index = params.eos_index\n",
        "        self.pad_index = params.pad_index\n",
        "        self.batch_size = params.batch_size\n",
        "        self.tokens_per_batch = params.tokens_per_batch\n",
        "        self.max_batch_size = params.max_batch_size\n",
        "\n",
        "        self.sent1 = sent1\n",
        "        self.sent2 = sent2\n",
        "        self.pos1 = pos1\n",
        "        self.pos2 = pos2\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "\n",
        "        # check number of sentences\n",
        "        assert len(self.pos1) == (self.sent1 == self.eos_index).sum()\n",
        "        assert len(self.pos2) == (self.sent2 == self.eos_index).sum()\n",
        "\n",
        "        # remove empty sentences\n",
        "        self.remove_empty_sentences()\n",
        "\n",
        "        # sanity checks\n",
        "        self.check()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.pos1)\n",
        "\n",
        "    def check(self):\n",
        "        \"\"\"\n",
        "        Sanity checks.\n",
        "        \"\"\"\n",
        "        eos = self.eos_index\n",
        "        assert len(self.pos1) == len(self.pos2) > 0                          # check number of sentences\n",
        "        assert len(self.pos1) == (self.sent1[self.pos1[:, 1]] == eos).sum()  # check sentences indices\n",
        "        assert len(self.pos2) == (self.sent2[self.pos2[:, 1]] == eos).sum()  # check sentences indices\n",
        "        assert eos <= self.sent1.min() < self.sent1.max()                    # check dictionary indices\n",
        "        assert eos <= self.sent2.min() < self.sent2.max()                    # check dictionary indices\n",
        "        assert self.lengths1.min() > 0                                       # check empty sentences\n",
        "        assert self.lengths2.min() > 0                                       # check empty sentences\n",
        "\n",
        "    def remove_empty_sentences(self):\n",
        "        \"\"\"\n",
        "        Remove empty sentences.\n",
        "        \"\"\"\n",
        "        init_size = len(self.pos1)\n",
        "        indices = np.arange(len(self.pos1))\n",
        "        indices = indices[self.lengths1[indices] > 0]\n",
        "        indices = indices[self.lengths2[indices] > 0]\n",
        "        self.pos1 = self.pos1[indices]\n",
        "        self.pos2 = self.pos2[indices]\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "        logger.info(\"Removed %i empty sentences.\" % (init_size - len(indices)))\n",
        "        self.check()\n",
        "\n",
        "    def remove_long_sentences(self, max_len):\n",
        "        \"\"\"\n",
        "        Remove sentences exceeding a certain length.\n",
        "        \"\"\"\n",
        "        assert max_len >= 0\n",
        "        if max_len == 0:\n",
        "            return\n",
        "        init_size = len(self.pos1)\n",
        "        indices = np.arange(len(self.pos1))\n",
        "        indices = indices[self.lengths1[indices] <= max_len]\n",
        "        indices = indices[self.lengths2[indices] <= max_len]\n",
        "        self.pos1 = self.pos1[indices]\n",
        "        self.pos2 = self.pos2[indices]\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "        logger.info(\"Removed %i too long sentences.\" % (init_size - len(indices)))\n",
        "        self.check()\n",
        "    \n",
        "    def cut_long_sentences(self, max_len1, max_len2):\n",
        "        assert max_len1 > 0 and max_len2 > 0\n",
        "\n",
        "        def _cut(length, pos, max_len):\n",
        "            # indices to cut\n",
        "            indices = np.arange(len(pos))\n",
        "            indices = indices[length[indices] > max_len]\n",
        "            pos[indices, 1] = pos[indices, 0] + max_len\n",
        "            length[indices] = max_len\n",
        "\n",
        "        _cut(self.lengths1, self.pos1, max_len1)\n",
        "        _cut(self.lengths2, self.pos2, max_len2)\n",
        "\n",
        "    def select_data(self, a, b):\n",
        "        \"\"\"\n",
        "        Only select a subset of the dataset.\n",
        "        \"\"\"\n",
        "        assert 0 <= a < b <= len(self.pos1)\n",
        "        logger.info(\"Selecting sentences from %i to %i ...\" % (a, b))\n",
        "\n",
        "        # sub-select\n",
        "        self.pos1 = self.pos1[a:b]\n",
        "        self.pos2 = self.pos2[a:b]\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "\n",
        "        # re-index\n",
        "        min_pos1 = self.pos1.min()\n",
        "        max_pos1 = self.pos1.max()\n",
        "        min_pos2 = self.pos2.min()\n",
        "        max_pos2 = self.pos2.max()\n",
        "        self.pos1 -= min_pos1\n",
        "        self.pos2 -= min_pos2\n",
        "        self.sent1 = self.sent1[min_pos1:max_pos1 + 1]\n",
        "        self.sent2 = self.sent2[min_pos2:max_pos2 + 1]\n",
        "\n",
        "        # sanity checks\n",
        "        # self.check()\n",
        "\n",
        "    def get_batches_iterator(self, batches, return_indices):\n",
        "        \"\"\"\n",
        "        Return a sentences iterator, given the associated sentence batches.\n",
        "        \"\"\"\n",
        "        assert type(return_indices) is bool\n",
        "\n",
        "        for sentence_ids in batches:\n",
        "            if 0 < self.max_batch_size < len(sentence_ids):\n",
        "                np.random.shuffle(sentence_ids)\n",
        "                sentence_ids = sentence_ids[:self.max_batch_size]\n",
        "            pos1 = self.pos1[sentence_ids]\n",
        "            pos2 = self.pos2[sentence_ids]\n",
        "            sent1 = self.batch_sentences([self.sent1[a:b] for a, b in pos1])\n",
        "            sent2 = self.batch_sentences([self.sent2[a:b] for a, b in pos2])\n",
        "            yield (sent1, sent2, sentence_ids) if return_indices else (sent1, sent2)\n",
        "\n",
        "    def get_iterator(self, shuffle, group_by_size=False, n_sentences=-1, return_indices=False):\n",
        "        \"\"\"\n",
        "        Return a sentences iterator.\n",
        "        \"\"\"\n",
        "        n_sentences = len(self.pos1) if n_sentences == -1 else n_sentences\n",
        "        assert 0 < n_sentences <= len(self.pos1)\n",
        "        assert type(shuffle) is bool and type(group_by_size) is bool\n",
        "\n",
        "        # sentence lengths\n",
        "        lengths = self.lengths1 + self.lengths2 + 4\n",
        "\n",
        "        # select sentences to iterate over\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(len(self.pos1))[:n_sentences]\n",
        "        else:\n",
        "            indices = np.arange(n_sentences)\n",
        "\n",
        "        # group sentences by lengths\n",
        "        if group_by_size:\n",
        "            indices = indices[np.argsort(lengths[indices], kind='mergesort')]\n",
        "\n",
        "        # create batches - either have a fixed number of sentences, or a similar number of tokens\n",
        "        if self.tokens_per_batch == -1:\n",
        "            batches = np.array_split(indices, math.ceil(len(indices) * 1. / self.batch_size))\n",
        "        else:\n",
        "            batch_ids = np.cumsum(lengths[indices]) // self.tokens_per_batch\n",
        "            _, bounds = np.unique(batch_ids, return_index=True)\n",
        "            batches = [indices[bounds[i]:bounds[i + 1]] for i in range(len(bounds) - 1)]\n",
        "            if bounds[-1] < len(indices):\n",
        "                batches.append(indices[bounds[-1]:])\n",
        "\n",
        "        # optionally shuffle batches\n",
        "        if shuffle:\n",
        "            np.random.shuffle(batches)\n",
        "\n",
        "        # sanity checks\n",
        "        assert n_sentences == sum([len(x) for x in batches])\n",
        "        assert lengths[indices].sum() == sum([lengths[x].sum() for x in batches])\n",
        "        # assert set.union(*[set(x.tolist()) for x in batches]) == set(range(n_sentences))  # slow\n",
        "\n",
        "        # return the iterator\n",
        "        return self.get_batches_iterator(batches, return_indices)\n",
        "\n",
        "\n",
        "class TripleDataset(Dataset):\n",
        "\n",
        "    def __init__(self, sent1, pos1, sent2, pos2, sent3, pos3, params):\n",
        "\n",
        "        self.eos_index = params.eos_index\n",
        "        self.pad_index = params.pad_index\n",
        "        self.batch_size = params.batch_size\n",
        "        self.tokens_per_batch = params.tokens_per_batch\n",
        "        self.max_batch_size = params.max_batch_size\n",
        "\n",
        "        self.sent1 = sent1\n",
        "        self.sent2 = sent2\n",
        "        self.sent3 = sent3\n",
        "        self.pos1 = pos1\n",
        "        self.pos2 = pos2\n",
        "        self.pos3 = pos3\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "        self.lengths3 = self.pos3[:, 1] - self.pos3[:, 0]\n",
        "\n",
        "        # check number of sentences\n",
        "        assert len(self.pos1) == (self.sent1 == self.eos_index).sum()\n",
        "        assert len(self.pos2) == (self.sent2 == self.eos_index).sum()\n",
        "        assert len(self.pos3) == (self.sent3 == self.eos_index).sum()\n",
        "\n",
        "        # remove empty sentences\n",
        "        self.remove_empty_sentences()\n",
        "\n",
        "        # sanity checks\n",
        "        self.check()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.pos1)\n",
        "\n",
        "    def check(self):\n",
        "        \"\"\"\n",
        "        Sanity checks.\n",
        "        \"\"\"\n",
        "        eos = self.eos_index\n",
        "        assert len(self.pos1) == len(self.pos2) == len(self.pos3) > 0                          # check number of sentences\n",
        "        assert len(self.pos1) == (self.sent1[self.pos1[:, 1]] == eos).sum()  # check sentences indices\n",
        "        assert len(self.pos2) == (self.sent2[self.pos2[:, 1]] == eos).sum()  # check sentences indices\n",
        "        assert len(self.pos3) == (self.sent3[self.pos3[:, 1]] == eos).sum()  # check sentences indices\n",
        "        assert eos <= self.sent1.min() < self.sent1.max()                    # check dictionary indices\n",
        "        assert eos <= self.sent2.min() < self.sent2.max()                    # check dictionary indices\n",
        "        assert eos <= self.sent3.min() < self.sent3.max()                    # check dictionary indices\n",
        "        assert self.lengths1.min() > 0                                       # check empty sentences\n",
        "        assert self.lengths2.min() > 0                                       # check empty sentences\n",
        "        assert self.lengths3.min() > 0                                       # check empty sentences\n",
        "\n",
        "\n",
        "    def remove_empty_sentences(self):\n",
        "        \"\"\"\n",
        "        Remove empty sentences.\n",
        "        \"\"\"\n",
        "        init_size = len(self.pos1)\n",
        "        indices = np.arange(len(self.pos1))\n",
        "        indices = indices[self.lengths1[indices] > 0]\n",
        "        indices = indices[self.lengths2[indices] > 0]\n",
        "        indices = indices[self.lengths3[indices] > 0]\n",
        "        self.pos1 = self.pos1[indices]\n",
        "        self.pos2 = self.pos2[indices]\n",
        "        self.pos3 = self.pos3[indices]\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "        self.lengths3 = self.pos3[:, 1] - self.pos3[:, 0]\n",
        "        logger.info(\"Removed %i empty sentences.\" % (init_size - len(indices)))\n",
        "        self.check()\n",
        "\n",
        "    def remove_long_sentences(self, max_len):\n",
        "        \"\"\"\n",
        "        Remove sentences exceeding a certain length.\n",
        "        \"\"\"\n",
        "        assert max_len >= 0\n",
        "        if max_len == 0:\n",
        "            return\n",
        "        init_size = len(self.pos1)\n",
        "        indices = np.arange(len(self.pos1))\n",
        "        indices = indices[self.lengths1[indices] <= max_len]\n",
        "        indices = indices[self.lengths2[indices] <= max_len]\n",
        "        indices = indices[self.lengths3[indices] <= max_len]\n",
        "        self.pos1 = self.pos1[indices]\n",
        "        self.pos2 = self.pos2[indices]\n",
        "        self.pos3 = self.pos3[indices]\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "        self.lengths3 = self.pos3[:, 1] - self.pos3[:, 0]\n",
        "        logger.info(\"Removed %i too long sentences.\" % (init_size - len(indices)))\n",
        "        self.check()\n",
        "    \n",
        "    def cut_long_sentences(self, max_len1, max_len2, max_len3):\n",
        "        assert max_len1 > 0 and max_len2 > 0 and max_len3 > 0\n",
        "\n",
        "        def _cut(length, pos, max_len):\n",
        "            # indices to cut\n",
        "            indices = np.arange(len(pos))\n",
        "            indices = indices[length[indices] > max_len]\n",
        "            pos[indices, 1] = pos[indices, 0] + max_len\n",
        "            length[indices] = max_len\n",
        "\n",
        "        _cut(self.lengths1, self.pos1, max_len1)\n",
        "        _cut(self.lengths2, self.pos2, max_len2)\n",
        "        _cut(self.lengths3, self.pos3, max_len3)\n",
        "\n",
        "\n",
        "    def select_data(self, a, b):\n",
        "        \"\"\"\n",
        "        Only select a subset of the dataset.\n",
        "        \"\"\"\n",
        "        assert 0 <= a < b <= len(self.pos1)\n",
        "        logger.info(\"Selecting sentences from %i to %i ...\" % (a, b))\n",
        "\n",
        "        # sub-select\n",
        "        self.pos1 = self.pos1[a:b]\n",
        "        self.pos2 = self.pos2[a:b]\n",
        "        self.pos3 = self.pos3[a:b]\n",
        "        self.lengths1 = self.pos1[:, 1] - self.pos1[:, 0]\n",
        "        self.lengths2 = self.pos2[:, 1] - self.pos2[:, 0]\n",
        "        self.lengths3 = self.pos3[:, 1] - self.pos3[:, 0]\n",
        "\n",
        "        # re-index\n",
        "        min_pos1 = self.pos1.min()\n",
        "        max_pos1 = self.pos1.max()\n",
        "        min_pos2 = self.pos2.min()\n",
        "        max_pos2 = self.pos2.max()\n",
        "        min_pos3 = self.pos3.min()\n",
        "        max_pos3 = self.pos3.max()\n",
        "        self.pos1 -= min_pos1\n",
        "        self.pos2 -= min_pos2\n",
        "        self.pos3 -= min_pos3\n",
        "        self.sent1 = self.sent1[min_pos1:max_pos1 + 1]\n",
        "        self.sent2 = self.sent2[min_pos2:max_pos2 + 1]\n",
        "        self.sent3 = self.sent3[min_pos3:max_pos3 + 1]\n",
        "\n",
        "        # sanity checks\n",
        "        self.check()\n",
        "\n",
        "    def get_batches_iterator(self, batches, return_indices):\n",
        "        \"\"\"\n",
        "        Return a sentences iterator, given the associated sentence batches.\n",
        "        \"\"\"\n",
        "        assert type(return_indices) is bool\n",
        "\n",
        "        for sentence_ids in batches:\n",
        "            if 0 < self.max_batch_size < len(sentence_ids):\n",
        "                np.random.shuffle(sentence_ids)\n",
        "                sentence_ids = sentence_ids[:self.max_batch_size]\n",
        "            pos1 = self.pos1[sentence_ids]\n",
        "            pos2 = self.pos2[sentence_ids]\n",
        "            pos3 = self.pos3[sentence_ids]\n",
        "            sent1 = self.batch_sentences([self.sent1[a:b] for a, b in pos1])\n",
        "            sent2 = self.batch_sentences([self.sent2[a:b] for a, b in pos2])\n",
        "            sent3 = self.batch_sentences([self.sent3[a:b] for a, b in pos3])\n",
        "            yield (sent1, sent2, sent3, sentence_ids) if return_indices else (sent1, sent2, sent3)\n",
        "\n",
        "    def get_iterator(self, shuffle, group_by_size=False, n_sentences=-1, return_indices=False):\n",
        "        \"\"\"\n",
        "        Return a sentences iterator.\n",
        "        \"\"\"\n",
        "        n_sentences = len(self.pos1) if n_sentences == -1 else n_sentences\n",
        "        assert 0 < n_sentences <= len(self.pos1)\n",
        "        assert type(shuffle) is bool and type(group_by_size) is bool\n",
        "\n",
        "        # sentence lengths\n",
        "        lengths = self.lengths1 + self.lengths2 + self.lengths3 + 6\n",
        "\n",
        "        # select sentences to iterate over\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(len(self.pos1))[:n_sentences]\n",
        "        else:\n",
        "            indices = np.arange(n_sentences)\n",
        "\n",
        "        # group sentences by lengths\n",
        "        if group_by_size:\n",
        "            indices = indices[np.argsort(lengths[indices], kind='mergesort')]\n",
        "\n",
        "        # create batches - either have a fixed number of sentences, or a similar number of tokens\n",
        "        if self.tokens_per_batch == -1:\n",
        "            batches = np.array_split(indices, math.ceil(len(indices) * 1. / self.batch_size))\n",
        "        else:\n",
        "            batch_ids = np.cumsum(lengths[indices]) // self.tokens_per_batch\n",
        "            _, bounds = np.unique(batch_ids, return_index=True)\n",
        "            batches = [indices[bounds[i]:bounds[i + 1]] for i in range(len(bounds) - 1)]\n",
        "            if bounds[-1] < len(indices):\n",
        "                batches.append(indices[bounds[-1]:])\n",
        "\n",
        "        # optionally shuffle batches\n",
        "        if shuffle:\n",
        "            np.random.shuffle(batches)\n",
        "\n",
        "        # sanity checks\n",
        "        assert n_sentences == sum([len(x) for x in batches])\n",
        "        assert lengths[indices].sum() == sum([lengths[x].sum() for x in batches])\n",
        "        # assert set.union(*[set(x.tolist()) for x in batches]) == set(range(n_sentences))  # slow\n",
        "\n",
        "        # return the iterator\n",
        "        return self.get_batches_iterator(batches, return_indices)"
      ],
      "metadata": {
        "id": "prjN_Ci-zeQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from logging import getLogger\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "PAD_WORD = '<pad>'\n",
        "UNK_WORD = '<unk>'\n",
        "\n",
        "SPECIAL_WORD = '<special%i>'\n",
        "SPECIAL_WORDS = 10\n",
        "\n",
        "SEP_WORD = SPECIAL_WORD % 0\n",
        "MASK_WORD = SPECIAL_WORD % 1\n",
        "\n",
        "\n",
        "class Dictionary(object):\n",
        "\n",
        "    def __init__(self, id2word, word2id, counts):\n",
        "        assert len(id2word) == len(word2id) == len(counts)\n",
        "        self.id2word = id2word\n",
        "        self.word2id = word2id\n",
        "        self.counts = counts\n",
        "        self.bos_index = word2id[BOS_WORD]\n",
        "        self.eos_index = word2id[EOS_WORD]\n",
        "        self.pad_index = word2id[PAD_WORD]\n",
        "        self.unk_index = word2id[UNK_WORD]\n",
        "        self.check_valid()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of words in the dictionary.\n",
        "        \"\"\"\n",
        "        return len(self.id2word)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Returns the word of the specified index.\n",
        "        \"\"\"\n",
        "        return self.id2word[i]\n",
        "\n",
        "    def __contains__(self, w):\n",
        "        \"\"\"\n",
        "        Returns whether a word is in the dictionary.\n",
        "        \"\"\"\n",
        "        return w in self.word2id\n",
        "\n",
        "    def __eq__(self, y):\n",
        "        \"\"\"\n",
        "        Compare this dictionary with another one.\n",
        "        \"\"\"\n",
        "        self.check_valid()\n",
        "        y.check_valid()\n",
        "        if len(self.id2word) != len(y):\n",
        "            return False\n",
        "        return all(self.id2word[i] == y[i] for i in range(len(y)))\n",
        "\n",
        "    def check_valid(self):\n",
        "        \"\"\"\n",
        "        Check that the dictionary is valid.\n",
        "        \"\"\"\n",
        "        assert self.bos_index == 0\n",
        "        assert self.eos_index == 1\n",
        "        assert self.pad_index == 2\n",
        "        assert self.unk_index == 3\n",
        "        assert all(self.id2word[4 + i] == SPECIAL_WORD % i for i in range(SPECIAL_WORDS))\n",
        "        assert len(self.id2word) == len(self.word2id) == len(self.counts)\n",
        "        assert set(self.word2id.keys()) == set(self.counts.keys())\n",
        "        for i in range(len(self.id2word)):\n",
        "            assert self.word2id[self.id2word[i]] == i\n",
        "        last_count = 1e18\n",
        "        for i in range(4 + SPECIAL_WORDS, len(self.id2word) - 1):\n",
        "            count = self.counts[self.id2word[i]]\n",
        "            assert count <= last_count\n",
        "            last_count = count\n",
        "\n",
        "    def index(self, word, no_unk=False):\n",
        "        \"\"\"\n",
        "        Returns the index of the specified word.\n",
        "        \"\"\"\n",
        "        if no_unk:\n",
        "            return self.word2id[word]\n",
        "        else:\n",
        "            return self.word2id.get(word, self.unk_index)\n",
        "\n",
        "    def max_vocab(self, max_vocab):\n",
        "        \"\"\"\n",
        "        Limit the vocabulary size.\n",
        "        \"\"\"\n",
        "        assert max_vocab >= 1\n",
        "        init_size = len(self)\n",
        "        self.id2word = {k: v for k, v in self.id2word.items() if k < max_vocab}\n",
        "        self.word2id = {v: k for k, v in self.id2word.items()}\n",
        "        self.counts = {k: v for k, v in self.counts.items() if k in self.word2id}\n",
        "        self.check_valid()\n",
        "        logger.info(\"Maximum vocabulary size: %i. Dictionary size: %i -> %i (removed %i words).\"\n",
        "                    % (max_vocab, init_size, len(self), init_size - len(self)))\n",
        "\n",
        "    def min_count(self, min_count):\n",
        "        \"\"\"\n",
        "        Threshold on the word frequency counts.\n",
        "        \"\"\"\n",
        "        assert min_count >= 0\n",
        "        init_size = len(self)\n",
        "        self.id2word = {k: v for k, v in self.id2word.items() if self.counts[self.id2word[k]] >= min_count or k < 4 + SPECIAL_WORDS}\n",
        "        self.word2id = {v: k for k, v in self.id2word.items()}\n",
        "        self.counts = {k: v for k, v in self.counts.items() if k in self.word2id}\n",
        "        self.check_valid()\n",
        "        logger.info(\"Minimum frequency count: %i. Dictionary size: %i -> %i (removed %i words).\"\n",
        "                    % (min_count, init_size, len(self), init_size - len(self)))\n",
        "\n",
        "    @staticmethod\n",
        "    def read_vocab(vocab_path):\n",
        "        \"\"\"\n",
        "        Create a dictionary from a vocabulary file.\n",
        "        \"\"\"\n",
        "        skipped = 0\n",
        "        assert os.path.isfile(vocab_path), vocab_path\n",
        "        word2id = {BOS_WORD: 0, EOS_WORD: 1, PAD_WORD: 2, UNK_WORD: 3}\n",
        "        for i in range(SPECIAL_WORDS):\n",
        "            word2id[SPECIAL_WORD % i] = 4 + i\n",
        "        counts = {k: 0 for k in word2id.keys()}\n",
        "        f = open(vocab_path, 'r', encoding='utf-8')\n",
        "        for i, line in enumerate(f):\n",
        "            if '\\u2028' in line:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            line = line.rstrip().split()\n",
        "            if len(line) != 2:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            assert len(line) == 2, (i, line)\n",
        "            # assert line[0] not in word2id and line[1].isdigit(), (i, line)\n",
        "            assert line[1].isdigit(), (i, line)\n",
        "            if line[0] in word2id:\n",
        "                skipped += 1\n",
        "                print('%s already in vocab' % line[0])\n",
        "                continue\n",
        "            if not line[1].isdigit():\n",
        "                skipped += 1\n",
        "                print('Empty word at line %s with count %s' % (i, line))\n",
        "                continue\n",
        "            word2id[line[0]] = 4 + SPECIAL_WORDS + i - skipped  # shift because of extra words\n",
        "            counts[line[0]] = int(line[1])\n",
        "        f.close()\n",
        "        id2word = {v: k for k, v in word2id.items()}\n",
        "        dico = Dictionary(id2word, word2id, counts)\n",
        "        logger.info(\"Read %i words from the vocabulary file.\" % len(dico))\n",
        "        if skipped > 0:\n",
        "            logger.warning(\"Skipped %i empty lines!\" % skipped)\n",
        "        return dico\n",
        "\n",
        "    @staticmethod\n",
        "    def index_data(path, bin_path, dico):\n",
        "        \"\"\"\n",
        "        Index sentences with a dictionary.\n",
        "        \"\"\"\n",
        "        if bin_path is not None and os.path.isfile(bin_path):\n",
        "            print(\"Loading data from %s ...\" % bin_path)\n",
        "            data = torch.load(bin_path)\n",
        "            assert dico == data['dico']\n",
        "            return data\n",
        "\n",
        "        positions = []\n",
        "        sentences = []\n",
        "        unk_words = {}\n",
        "\n",
        "        # index sentences\n",
        "        f = open(path, 'r', encoding='utf-8')\n",
        "        for i, line in enumerate(f):\n",
        "            if i % 1000000 == 0 and i > 0:\n",
        "                print(i)\n",
        "            s = line.rstrip().split()\n",
        "            # skip empty sentences\n",
        "            if len(s) == 0:\n",
        "                print(\"Empty sentence in line %i.\" % i)\n",
        "            # index sentence words\n",
        "            count_unk = 0\n",
        "            indexed = []\n",
        "            for w in s:\n",
        "                word_id = dico.index(w, no_unk=False)\n",
        "                # if we find a special word which is not an unknown word, skip the sentence\n",
        "                if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:\n",
        "                    logger.warning('Found unexpected special word \"%s\" (%i)!!' % (w, word_id))\n",
        "                    continue\n",
        "                assert word_id >= 0\n",
        "                indexed.append(word_id)\n",
        "                if word_id == dico.unk_index:\n",
        "                    unk_words[w] = unk_words.get(w, 0) + 1\n",
        "                    count_unk += 1\n",
        "            # add sentence\n",
        "            positions.append([len(sentences), len(sentences) + len(indexed)])\n",
        "            sentences.extend(indexed)\n",
        "            sentences.append(1)  # EOS index\n",
        "        f.close()\n",
        "\n",
        "        # tensorize data\n",
        "        positions = np.int64(positions)\n",
        "        if len(dico) < 1 << 16:\n",
        "            sentences = np.uint16(sentences)\n",
        "        elif len(dico) < 1 << 31:\n",
        "            sentences = np.int32(sentences)\n",
        "        else:\n",
        "            raise Exception(\"Dictionary is too big.\")\n",
        "        assert sentences.min() >= 0\n",
        "        data = {\n",
        "            'dico': dico,\n",
        "            'positions': positions,\n",
        "            'sentences': sentences,\n",
        "            'unk_words': unk_words,\n",
        "        }\n",
        "        if bin_path is not None:\n",
        "            print(\"Saving the data to %s ...\" % bin_path)\n",
        "            torch.save(data, bin_path, pickle_protocol=4)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "ufoeyolAzlFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import getLogger\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def process_binarized(data, params):\n",
        "    \"\"\"\n",
        "    Process a binarized dataset and log main statistics.\n",
        "    \"\"\"\n",
        "    dico = data['dico']\n",
        "    assert ((data['sentences'].dtype == np.uint16) and (len(dico) < 1 << 16) or\n",
        "            (data['sentences'].dtype == np.int32) and (1 << 16 <= len(dico) < 1 << 31))\n",
        "    logger.info(\"%i words (%i unique) in %i sentences. %i unknown words (%i unique) covering %.2f%% of the data.\" % (\n",
        "        len(data['sentences']) - len(data['positions']),\n",
        "        len(dico), len(data['positions']),\n",
        "        sum(data['unk_words'].values()), len(data['unk_words']),\n",
        "        100. * sum(data['unk_words'].values()) / (len(data['sentences']) - len(data['positions']))\n",
        "    ))\n",
        "    if params.max_vocab != -1:\n",
        "        assert params.max_vocab > 0\n",
        "        logger.info(\"Selecting %i most frequent words ...\" % params.max_vocab)\n",
        "        dico.max_vocab(params.max_vocab)\n",
        "        data['sentences'][data['sentences'] >= params.max_vocab] = dico.index(UNK_WORD)\n",
        "        unk_count = (data['sentences'] == dico.index(UNK_WORD)).sum()\n",
        "        logger.info(\"Now %i unknown words covering %.2f%% of the data.\"\n",
        "                    % (unk_count, 100. * unk_count / (len(data['sentences']) - len(data['positions']))))\n",
        "    if params.min_count > 0:\n",
        "        logger.info(\"Selecting words with >= %i occurrences ...\" % params.min_count)\n",
        "        dico.min_count(params.min_count)\n",
        "        data['sentences'][data['sentences'] >= len(dico)] = dico.index(UNK_WORD)\n",
        "        unk_count = (data['sentences'] == dico.index(UNK_WORD)).sum()\n",
        "        logger.info(\"Now %i unknown words covering %.2f%% of the data.\"\n",
        "                    % (unk_count, 100. * unk_count / (len(data['sentences']) - len(data['positions']))))\n",
        "    if (data['sentences'].dtype == np.int32) and (len(dico) < 1 << 16):\n",
        "        logger.info(\"Less than 65536 words. Moving data from int32 to uint16 ...\")\n",
        "        data['sentences'] = data['sentences'].astype(np.uint16)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_binarized(path, params):\n",
        "    \"\"\"\n",
        "    Load a binarized dataset.\n",
        "    \"\"\"\n",
        "    assert path.endswith('.pth')\n",
        "    if params.debug_train:\n",
        "        path = path.replace('train', 'valid')\n",
        "    if getattr(params, 'multi_gpu', False):\n",
        "        split_path = '%s.%i.pth' % (path[:-4], params.local_rank)\n",
        "        if os.path.isfile(split_path):\n",
        "            assert params.split_data is False\n",
        "            path = split_path\n",
        "    assert os.path.isfile(path), path\n",
        "    logger.info(\"Loading data from %s ...\" % path)\n",
        "    data = torch.load(path)\n",
        "    data = process_binarized(data, params)\n",
        "    return data\n",
        "\n",
        "\n",
        "def set_dico_parameters(params, data, dico):\n",
        "    \"\"\"\n",
        "    Update dictionary parameters.\n",
        "    \"\"\"\n",
        "    if 'dico' in data:\n",
        "        assert data['dico'] == dico\n",
        "    else:\n",
        "        data['dico'] = dico\n",
        "\n",
        "    n_words = len(dico)\n",
        "    bos_index = dico.index(BOS_WORD)\n",
        "    eos_index = dico.index(EOS_WORD)\n",
        "    pad_index = dico.index(PAD_WORD)\n",
        "    unk_index = dico.index(UNK_WORD)\n",
        "    mask_index = dico.index(MASK_WORD)\n",
        "    if hasattr(params, 'bos_index'):\n",
        "        assert params.n_words == n_words\n",
        "        assert params.bos_index == bos_index\n",
        "        assert params.eos_index == eos_index\n",
        "        assert params.pad_index == pad_index\n",
        "        assert params.unk_index == unk_index\n",
        "        assert params.mask_index == mask_index\n",
        "    else:\n",
        "        params.n_words = n_words\n",
        "        params.bos_index = bos_index\n",
        "        params.eos_index = eos_index\n",
        "        params.pad_index = pad_index\n",
        "        params.unk_index = unk_index\n",
        "        params.mask_index = mask_index\n",
        "\n",
        "\n",
        "def load_mono_data(params, data):\n",
        "    \"\"\"\n",
        "    Load monolingual data.\n",
        "    \"\"\"\n",
        "    data['mono'] = {}\n",
        "    data['mono_stream'] = {}\n",
        "\n",
        "    for lang in params.mono_dataset.keys():\n",
        "\n",
        "        logger.info('============ Monolingual data (%s)' % lang)\n",
        "\n",
        "        assert lang in params.langs and lang not in data['mono']\n",
        "        data['mono'][lang] = {}\n",
        "        data['mono_stream'][lang] = {}\n",
        "\n",
        "        for splt in ['train', 'valid', 'test']:\n",
        "\n",
        "            # no need to load training data for evaluation\n",
        "            if splt == 'train' and params.eval_only:\n",
        "                continue\n",
        "\n",
        "            # load data / update dictionary parameters / update data\n",
        "            mono_data = load_binarized(params.mono_dataset[lang][splt], params)\n",
        "            set_dico_parameters(params, data, mono_data['dico'])\n",
        "\n",
        "            # create stream dataset\n",
        "            bs = params.batch_size if splt == 'train' else 1\n",
        "            data['mono_stream'][lang][splt] = StreamDataset(mono_data['sentences'], mono_data['positions'], bs, params)\n",
        "\n",
        "            # if there are several processes on the same machine, we can split the dataset\n",
        "            if splt == 'train' and params.split_data and 1 < params.n_gpu_per_node <= data['mono_stream'][lang][splt].n_batches:\n",
        "                n_batches = data['mono_stream'][lang][splt].n_batches // params.n_gpu_per_node\n",
        "                a = n_batches * params.local_rank\n",
        "                b = n_batches * params.local_rank + n_batches\n",
        "                data['mono_stream'][lang][splt].select_data(a, b)\n",
        "\n",
        "            # for denoising auto-encoding and online back-translation, we need a non-stream (batched) dataset\n",
        "            if lang in params.ae_steps or lang in params.bt_src_langs:\n",
        "\n",
        "                # create batched dataset\n",
        "                dataset = Dataset(mono_data['sentences'], mono_data['positions'], params)\n",
        "\n",
        "                # remove empty and too long sentences\n",
        "                # if splt == 'train':\n",
        "                dataset.remove_empty_sentences()\n",
        "                dataset.remove_long_sentences(params.max_len)\n",
        "\n",
        "                # if there are several processes on the same machine, we can split the dataset\n",
        "                if splt == 'train' and params.n_gpu_per_node > 1 and params.split_data:\n",
        "                    n_sent = len(dataset) // params.n_gpu_per_node\n",
        "                    a = n_sent * params.local_rank\n",
        "                    b = n_sent * params.local_rank + n_sent\n",
        "                    dataset.select_data(a, b)\n",
        "\n",
        "                data['mono'][lang][splt] = dataset\n",
        "\n",
        "            logger.info(\"\")\n",
        "\n",
        "    logger.info(\"\")\n",
        "\n",
        "\n",
        "def load_para_data(params, data):\n",
        "    \"\"\"\n",
        "    Load parallel data.\n",
        "    \"\"\"\n",
        "    data['para'] = {}\n",
        "\n",
        "    required_para_train = set(params.clm_steps + params.mlm_steps + params.pc_steps + params.mt_steps)\n",
        "\n",
        "    for src, tgt in params.para_dataset.keys():\n",
        "\n",
        "        logger.info('============ Parallel data (%s-%s)' % (src, tgt))\n",
        "\n",
        "        assert (src, tgt) not in data['para']\n",
        "        data['para'][(src, tgt)] = {}\n",
        "\n",
        "        for splt in ['train', 'valid', 'test']:\n",
        "\n",
        "            # no need to load training data for evaluation\n",
        "            if splt == 'train' and params.eval_only:\n",
        "                continue\n",
        "\n",
        "            # for back-translation, we can't load training data\n",
        "            if splt == 'train' and (src, tgt) not in required_para_train and (tgt, src) not in required_para_train:\n",
        "                continue\n",
        "\n",
        "            # load binarized datasets\n",
        "            src_path, tgt_path = params.para_dataset[(src, tgt)][splt]\n",
        "            src_data = load_binarized(src_path, params)\n",
        "            tgt_data = load_binarized(tgt_path, params)\n",
        "\n",
        "            # update dictionary parameters\n",
        "            set_dico_parameters(params, data, src_data['dico'])\n",
        "            set_dico_parameters(params, data, tgt_data['dico'])\n",
        "\n",
        "            # create ParallelDataset\n",
        "            dataset = ParallelDataset(\n",
        "                src_data['sentences'], src_data['positions'],\n",
        "                tgt_data['sentences'], tgt_data['positions'],\n",
        "                params\n",
        "            )\n",
        "\n",
        "            # remove empty and too long sentences\n",
        "            # if splt == 'train':\n",
        "            dataset.remove_empty_sentences()\n",
        "            dataset.remove_long_sentences(params.max_len)\n",
        "\n",
        "            # for validation and test set, enumerate sentence per sentence\n",
        "            if splt != 'train':\n",
        "                dataset.tokens_per_batch = -1\n",
        "\n",
        "            # if there are several processes on the same machine, we can split the dataset\n",
        "            if splt == 'train' and params.n_gpu_per_node > 1 and params.split_data:\n",
        "                n_sent = len(dataset) // params.n_gpu_per_node\n",
        "                a = n_sent * params.local_rank\n",
        "                b = n_sent * params.local_rank + n_sent\n",
        "                dataset.select_data(a, b)\n",
        "\n",
        "            data['para'][(src, tgt)][splt] = dataset\n",
        "            logger.info(\"\")\n",
        "\n",
        "    logger.info(\"\")\n",
        "\n",
        "\n",
        "def check_data_params(params):\n",
        "    \"\"\"\n",
        "    Check datasets parameters.\n",
        "    \"\"\"\n",
        "    # data path\n",
        "    assert os.path.isdir(params.data_path), params.data_path\n",
        "\n",
        "    # check languages\n",
        "    params.langs = params.lgs.split('-') if params.lgs != 'debug' else ['en']\n",
        "    assert len(params.langs) == len(set(params.langs)) >= 1\n",
        "    # assert sorted(params.langs) == params.langs\n",
        "    params.id2lang = {k: v for k, v in enumerate(sorted(params.langs))}\n",
        "    params.lang2id = {k: v for v, k in params.id2lang.items()}\n",
        "    params.n_langs = len(params.langs)\n",
        "\n",
        "    # CLM steps\n",
        "    clm_steps = [s.split('-') for s in params.clm_steps.split(',') if len(s) > 0]\n",
        "    params.clm_steps = [(s[0], None) if len(s) == 1 else tuple(s) for s in clm_steps]\n",
        "    assert all([(l1 in params.langs) and (l2 in params.langs or l2 is None) for l1, l2 in params.clm_steps])\n",
        "    assert len(params.clm_steps) == len(set(params.clm_steps))\n",
        "\n",
        "    # MLM / TLM steps\n",
        "    mlm_steps = [s.split('-') for s in params.mlm_steps.split(',') if len(s) > 0]\n",
        "    params.mlm_steps = [(s[0], None) if len(s) == 1 else tuple(s) for s in mlm_steps]\n",
        "    assert all([(l1 in params.langs) and (l2 in params.langs or l2 is None) for l1, l2 in params.mlm_steps])\n",
        "    assert len(params.mlm_steps) == len(set(params.mlm_steps))\n",
        "\n",
        "    # S2SLM steps\n",
        "    s2slm_steps = [s.split('-') for s in params.s2slm_steps.split(',') if len(s) > 0]\n",
        "    params.s2slm_steps = [(s[0], None) if len(s) == 1 else tuple(s) for s in s2slm_steps]\n",
        "    assert all([(l1 in params.langs) and (l2 in params.langs or l2 is None) for l1, l2 in params.s2slm_steps])\n",
        "    assert len(params.s2slm_steps) == len(set(params.s2slm_steps))\n",
        "\n",
        "    # parallel classification steps\n",
        "    params.pc_steps = [tuple(s.split('-')) for s in params.pc_steps.split(',') if len(s) > 0]\n",
        "    assert all([len(x) == 2 for x in params.pc_steps])\n",
        "    assert all([l1 in params.langs and l2 in params.langs for l1, l2 in params.pc_steps])\n",
        "    assert all([l1 != l2 for l1, l2 in params.pc_steps])\n",
        "    assert len(params.pc_steps) == len(set(params.pc_steps))\n",
        "\n",
        "    # machine translation steps\n",
        "    params.mt_steps = [tuple(s.split('-')) for s in params.mt_steps.split(',') if len(s) > 0]\n",
        "    assert all([len(x) == 2 for x in params.mt_steps])\n",
        "    assert all([l1 in params.langs and l2 in params.langs for l1, l2 in params.mt_steps])\n",
        "    assert all([l1 != l2 for l1, l2 in params.mt_steps])\n",
        "    assert len(params.mt_steps) == len(set(params.mt_steps))\n",
        "    # assert len(params.mt_steps) == 0 or not params.encoder_only\n",
        "\n",
        "    # denoising auto-encoder steps\n",
        "    params.ae_steps = [s for s in params.ae_steps.split(',') if len(s) > 0]\n",
        "    assert all([lang in params.langs for lang in params.ae_steps])\n",
        "    assert len(params.ae_steps) == len(set(params.ae_steps))\n",
        "    # assert len(params.ae_steps) == 0 or not params.encoder_only\n",
        "\n",
        "    # back-translation steps\n",
        "    params.bt_steps = [tuple(s.split('-')) for s in params.bt_steps.split(',') if len(s) > 0]\n",
        "    assert all([len(x) == 3 for x in params.bt_steps])\n",
        "    assert all([l1 in params.langs and l2 in params.langs and l3 in params.langs for l1, l2, l3 in params.bt_steps])\n",
        "    assert all([l1 == l3 and l1 != l2 for l1, l2, l3 in params.bt_steps])\n",
        "    assert len(params.bt_steps) == len(set(params.bt_steps))\n",
        "    assert len(params.bt_steps) == 0 or not params.encoder_only\n",
        "    params.bt_src_langs = [l1 for l1, _, _ in params.bt_steps]\n",
        "\n",
        "    # check monolingual datasets\n",
        "    required_mono = set([l1 for l1, l2 in (params.mlm_steps + params.clm_steps + params.s2slm_steps) if l2 is None] + params.ae_steps + params.bt_src_langs)\n",
        "    # params.mono_dataset = {\n",
        "    #     lang: {\n",
        "    #         splt: os.path.join(params.data_path, '%s.%s.pth' % (splt, lang))\n",
        "    #         for splt in ['train', 'valid', 'test']\n",
        "    #     } for lang in params.langs if lang in required_mono\n",
        "    # }\n",
        "    # NOTE swap lang splt\n",
        "    params.mono_dataset = {\n",
        "        lang: {\n",
        "            splt: os.path.join(params.data_path, '%s.%s.pth' % (lang, splt))\n",
        "            for splt in ['train', 'valid', 'test']\n",
        "        } for lang in params.langs if lang in required_mono\n",
        "    }\n",
        "    assert all([all([os.path.isfile(p) for p in paths.values()]) for paths in params.mono_dataset.values()]), params.mono_dataset\n",
        "\n",
        "    # check parallel datasets\n",
        "    required_para_train = set(params.clm_steps + params.mlm_steps + params.pc_steps + params.mt_steps + params.s2slm_steps)\n",
        "    required_para = required_para_train | set([(l2, l3) for _, l2, l3 in params.bt_steps])\n",
        "    params.para_dataset = {\n",
        "        (src, tgt): {\n",
        "            splt: (os.path.join(params.data_path, '%s.%s-%s.%s.pth' % (splt, src, tgt, src)),\n",
        "                   os.path.join(params.data_path, '%s.%s-%s.%s.pth' % (splt, src, tgt, tgt)))\n",
        "            for splt in ['train', 'valid', 'test']\n",
        "            if splt != 'train' or (src, tgt) in required_para_train or (tgt, src) in required_para_train\n",
        "        } for src in params.langs for tgt in params.langs\n",
        "        if src < tgt and ((src, tgt) in required_para or (tgt, src) in required_para)\n",
        "    }\n",
        "    for paths in params.para_dataset.values():\n",
        "        for p1, p2 in paths.values():\n",
        "            if not os.path.isfile(p1):\n",
        "                logger.error(f\"{p1} not found\")\n",
        "            if not os.path.isfile(p2):\n",
        "                logger.error(f\"{p2} not found\")\n",
        "    assert all([all([os.path.isfile(p1) and os.path.isfile(p2) for p1, p2 in paths.values()]) for paths in params.para_dataset.values()])\n",
        "\n",
        "    # check that we can evaluate on BLEU\n",
        "    assert params.eval_bleu is False or len(params.mt_steps + params.bt_steps) > 0\n",
        "\n",
        "\n",
        "def load_data(params):\n",
        "    \"\"\"\n",
        "    Load monolingual data.\n",
        "    The returned dictionary contains:\n",
        "        - dico (dictionary)\n",
        "        - vocab (FloatTensor)\n",
        "        - train / valid / test (monolingual datasets)\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    # monolingual datasets\n",
        "    load_mono_data(params, data)\n",
        "\n",
        "    # parallel datasets\n",
        "    load_para_data(params, data)\n",
        "\n",
        "    # monolingual data summary\n",
        "    logger.info('============ Data summary')\n",
        "    for lang, v in data['mono_stream'].items():\n",
        "        for data_set in v.keys():\n",
        "            logger.info('{: <18} - {: >5} - {: >12}:{: >10}'.format('Monolingual data', data_set, lang, len(v[data_set])))\n",
        "\n",
        "    # parallel data summary\n",
        "    for (src, tgt), v in data['para'].items():\n",
        "        for data_set in v.keys():\n",
        "            logger.info('{: <18} - {: >5} - {: >12}:{: >10}'.format('Parallel data', data_set, '%s-%s' % (src, tgt), len(v[data_set])))\n",
        "\n",
        "    logger.info(\"\")\n",
        "    return data"
      ],
      "metadata": {
        "id": "B9Hcz2Yazp9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import itertools\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\n",
        "class HashingMemory(nn.Module):\n",
        "\n",
        "    MEM_VALUES_PARAMS = '.values.weight'\n",
        "    VALUES = None\n",
        "    EVAL_MEMORY = True\n",
        "    _ids = itertools.count(0)\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, params):\n",
        "\n",
        "        super().__init__()\n",
        "        self.id = next(self._ids)\n",
        "\n",
        "        # global parameters\n",
        "        self.input2d = params.mem_input2d\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.size = params.mem_size\n",
        "        self.modulo_size = params.mem_modulo_size\n",
        "        self.n_indices = params.n_indices\n",
        "        self.k_dim = params.mem_k_dim\n",
        "        self.v_dim = params.mem_v_dim if params.mem_v_dim > 0 else output_dim\n",
        "        self.heads = params.mem_heads\n",
        "        self.knn = params.mem_knn\n",
        "        self.shuffle_indices = params.mem_shuffle_indices\n",
        "        self.keys_normalized_init = params.mem_keys_normalized_init\n",
        "        self.product_quantization = params.mem_product_quantization\n",
        "        assert self.modulo_size == -1 and self.size == self.n_indices or self.n_indices > self.size == self.modulo_size >= 1\n",
        "\n",
        "        # keys / queries\n",
        "        self.keys_type = params.mem_keys_type\n",
        "        self.learn_keys = params.mem_keys_learn\n",
        "        self.use_different_keys = params.mem_use_different_keys\n",
        "        self.query_detach_input = params.mem_query_detach_input\n",
        "        self.query_net_learn = params.mem_query_net_learn\n",
        "        self.multi_query_net = params.mem_multi_query_net\n",
        "        self.shuffle_query = params.mem_shuffle_query\n",
        "        assert self.use_different_keys is False or self.keys_type in ['gaussian', 'uniform']\n",
        "        assert self.use_different_keys is False or self.heads >= 2 or self.product_quantization\n",
        "        assert self.multi_query_net is False or self.heads >= 2 or self.product_quantization\n",
        "        assert self.shuffle_query is False or self.heads > 1 and params.mem_query_layer_sizes == ''\n",
        "        assert self.shuffle_query is False or self.input_dim % (2 ** self.heads) == 0\n",
        "\n",
        "        # scoring / re-scoring\n",
        "        self.normalize_query = params.mem_normalize_query\n",
        "        self.temperature = params.mem_temperature\n",
        "        self.score_softmax = params.mem_score_softmax\n",
        "        self.score_subtract = params.mem_score_subtract\n",
        "        self.score_normalize = params.mem_score_normalize\n",
        "        assert self.score_subtract in ['', 'min', 'mean', 'median']\n",
        "        assert self.score_subtract == '' or self.knn >= 2\n",
        "        assert not (self.score_normalize and self.score_softmax and self.score_subtract == '')\n",
        "\n",
        "        # dropout\n",
        "        self.input_dropout = params.mem_input_dropout\n",
        "        self.query_dropout = params.mem_query_dropout\n",
        "        self.value_dropout = params.mem_value_dropout\n",
        "\n",
        "        # initialize keys\n",
        "        self.init_keys()\n",
        "\n",
        "        # self.values = nn.Embedding(self.size, self.v_dim, sparse=params.mem_sparse)\n",
        "        self.values = nn.EmbeddingBag(self.size, self.v_dim, mode='sum', sparse=params.mem_sparse)\n",
        "\n",
        "        # optionally use the same values for all memories\n",
        "        if params.mem_share_values:\n",
        "            if HashingMemory.VALUES is None:\n",
        "                HashingMemory.VALUES = self.values.weight\n",
        "            else:\n",
        "                self.values.weight = HashingMemory.VALUES\n",
        "\n",
        "        # values initialization\n",
        "        if params.mem_value_zero_init:\n",
        "            nn.init.zeros_(self.values.weight)\n",
        "        else:\n",
        "            nn.init.normal_(self.values.weight, mean=0, std=self.v_dim ** -0.5)\n",
        "\n",
        "        # no query network\n",
        "        if len(params.mem_query_layer_sizes) == 0:\n",
        "            assert self.heads == 1 or self.use_different_keys or self.shuffle_query\n",
        "            assert self.input_dim == self.k_dim\n",
        "            self.query_proj = QueryIdentity(self.input_dim, self.heads, self.shuffle_query)\n",
        "\n",
        "        # query network\n",
        "        if len(params.mem_query_layer_sizes) > 0:\n",
        "            assert not self.shuffle_query\n",
        "\n",
        "            # layer sizes / number of features\n",
        "            l_sizes = list(params.mem_query_layer_sizes)\n",
        "            assert len(l_sizes) >= 2 and l_sizes[0] == l_sizes[-1] == 0\n",
        "            l_sizes[0] = self.input_dim\n",
        "            l_sizes[-1] = (self.k_dim // 2) if self.multi_query_net else (self.heads * self.k_dim)\n",
        "\n",
        "            # convolutional or feedforward\n",
        "            if self.input2d:\n",
        "                self.query_proj = QueryConv(\n",
        "                    self.input_dim, self.heads, self.k_dim, self.product_quantization,\n",
        "                    self.multi_query_net, l_sizes, params.mem_query_kernel_sizes,\n",
        "                    bias=params.mem_query_bias, batchnorm=params.mem_query_batchnorm,\n",
        "                    grouped_conv=params.mem_grouped_conv\n",
        "                )\n",
        "            else:\n",
        "                assert params.mem_query_kernel_sizes == ''\n",
        "                assert not params.mem_query_residual\n",
        "                self.query_proj = QueryMLP(\n",
        "                    self.input_dim, self.heads, self.k_dim, self.product_quantization,\n",
        "                    self.multi_query_net, l_sizes,\n",
        "                    bias=params.mem_query_bias, batchnorm=params.mem_query_batchnorm,\n",
        "                    grouped_conv=params.mem_grouped_conv\n",
        "                )\n",
        "\n",
        "        # shuffle indices for different heads\n",
        "        if self.shuffle_indices:\n",
        "            head_permutations = [torch.randperm(self.n_indices).unsqueeze(0) for i in range(self.heads)]\n",
        "            self.register_buffer('head_permutations', torch.cat(head_permutations, 0))\n",
        "\n",
        "        # do not learn the query network\n",
        "        if self.query_net_learn is False:\n",
        "            for p in self.query_proj.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Read from the memory.\n",
        "        \"\"\"\n",
        "        # detach input\n",
        "        if self.query_detach_input:\n",
        "            input = input.detach()\n",
        "\n",
        "        # input dimensions\n",
        "        if self.input2d:\n",
        "            assert input.shape[1] == self.input_dim\n",
        "            n_images, _, height, width = input.shape\n",
        "            prefix_shape = (n_images, width, height)\n",
        "        else:\n",
        "            assert input.shape[-1] == self.input_dim\n",
        "            prefix_shape = input.shape[:-1]\n",
        "\n",
        "        # compute query / store it\n",
        "        bs = np.prod(prefix_shape)\n",
        "        input = F.dropout(input, p=self.input_dropout, training=self.training)    # input shape\n",
        "        query = self.query_proj(input)                                            # (bs * heads, k_dim)\n",
        "        query = F.dropout(query, p=self.query_dropout, training=self.training)    # (bs * heads, k_dim)\n",
        "        assert query.shape == (bs * self.heads, self.k_dim)\n",
        "\n",
        "        # get indices\n",
        "        scores, indices = self.get_indices(query, self.knn)                       # (bs * heads, knn) ** 2\n",
        "\n",
        "        # optionally shuffle indices for different heads\n",
        "        if self.shuffle_indices:\n",
        "            indices = indices.view(bs, self.heads, -1).chunk(self.heads, 1)\n",
        "            indices = [p[idx] for p, idx in zip(self.head_permutations, indices)]\n",
        "            indices = torch.cat(indices, 1).view(bs * self.heads, -1)\n",
        "\n",
        "        # take indices modulo the memory size\n",
        "        if self.modulo_size != -1:\n",
        "            indices = indices % self.modulo_size\n",
        "\n",
        "        # re-scoring\n",
        "        if self.temperature != 1:\n",
        "            scores = scores / self.temperature                                    # (bs * heads, knn)\n",
        "        if self.score_softmax:\n",
        "            scores = F.softmax(scores.float(), dim=-1).type_as(scores)            # (bs * heads, knn)\n",
        "        if self.score_subtract != '':\n",
        "            if self.score_subtract == 'min':\n",
        "                to_sub = scores.min(1, keepdim=True)[0]                           # (bs * heads, 1)\n",
        "            if self.score_subtract == 'mean':\n",
        "                to_sub = scores.mean(1, keepdim=True)                             # (bs * heads, 1)\n",
        "            if self.score_subtract == 'median':\n",
        "                to_sub = scores.median(1, keepdim=True)[0]                        # (bs * heads, 1)\n",
        "            scores = scores - to_sub                                              # (bs * heads, knn)\n",
        "        if self.score_normalize:\n",
        "            scores = scores / scores.norm(p=1, dim=1, keepdim=True)               # (bs * heads, knn)\n",
        "\n",
        "        # merge heads / knn (since we sum heads)\n",
        "        indices = indices.view(bs, self.heads * self.knn)                         # (bs, heads * knn)\n",
        "        scores = scores.view(bs, self.heads * self.knn)                           # (bs, heads * knn)\n",
        "\n",
        "        # weighted sum of values\n",
        "        # output = self.values(indices) * scores.unsqueeze(-1)                    # (bs * heads, knn, v_dim)\n",
        "        # output = output.sum(1)                                                  # (bs * heads, v_dim)\n",
        "        output = self.values(\n",
        "            indices,\n",
        "            per_sample_weights=scores.to(self.values.weight.data)\n",
        "        ).to(scores)                                                              # (bs, v_dim)\n",
        "        output = F.dropout(output, p=self.value_dropout, training=self.training)  # (bs, v_dim)\n",
        "\n",
        "        # reshape output\n",
        "        if self.input2d:\n",
        "            output = output.view(n_images, width, height, self.v_dim)             # (n_images, width, height, v_dim)\n",
        "            output = output.transpose(1, 3)                                       # (n_images, v_dim, height, width)\n",
        "        else:\n",
        "            if len(prefix_shape) >= 2:\n",
        "                output = output.view(prefix_shape + (self.v_dim,))                # (..., v_dim)\n",
        "\n",
        "        # store indices / scores (eval mode only - for usage statistics)\n",
        "        if not self.training and HashingMemory.EVAL_MEMORY:\n",
        "            self.last_indices = indices.view(bs, self.heads, self.knn).detach().cpu()\n",
        "            self.last_scores = scores.view(bs, self.heads, self.knn).detach().cpu().float()\n",
        "\n",
        "        return output\n",
        "\n",
        "    def init_keys(self):\n",
        "        raise Exception(\"Not implemented!\")\n",
        "\n",
        "    def _get_indices(self, query, knn, keys):\n",
        "        raise Exception(\"Not implemented!\")\n",
        "\n",
        "    def get_indices(self, query, knn):\n",
        "        raise Exception(\"Not implemented!\")\n",
        "\n",
        "    @staticmethod\n",
        "    def register_args(parser):\n",
        "        \"\"\"\n",
        "        Register memory parameters\n",
        "        \"\"\"\n",
        "        # memory implementation\n",
        "        parser.add_argument(\"--mem_implementation\", type=str, default=\"fast\",\n",
        "                            help=\"Memory implementation (flat, pq_default, pq_fast)\")\n",
        "\n",
        "        # optimization\n",
        "        parser.add_argument(\"--mem_grouped_conv\", type=bool_flag, default=False,\n",
        "                            help=\"Use grouped convolutions in the query network\")\n",
        "        parser.add_argument(\"--mem_values_optimizer\", type=str, default=\"\",\n",
        "                            help=\"Memory values optimizer (\"\" for the same optimizer as the rest of the model)\")\n",
        "        parser.add_argument(\"--mem_sparse\", type=bool_flag, default=False,\n",
        "                            help=\"Perform sparse updates for the values\")\n",
        "\n",
        "        # global parameters\n",
        "        parser.add_argument(\"--mem_input2d\", type=bool_flag, default=False,\n",
        "                            help=\"Convolutional query network\")\n",
        "        parser.add_argument(\"--mem_k_dim\", type=int, default=16,\n",
        "                            help=\"Memory keys dimension\")\n",
        "        parser.add_argument(\"--mem_v_dim\", type=int, default=-1,\n",
        "                            help=\"Memory values dimension (-1 for automatic output dimension)\")\n",
        "        parser.add_argument(\"--mem_heads\", type=int, default=1,\n",
        "                            help=\"Number of memory reading heads\")\n",
        "        parser.add_argument(\"--mem_knn\", type=int, default=10,\n",
        "                            help=\"Number of memory slots to read / update - k-NN to the query\")\n",
        "        parser.add_argument(\"--mem_share_values\", type=bool_flag, default=False,\n",
        "                            help=\"Share values across memories\")\n",
        "        parser.add_argument(\"--mem_shuffle_indices\", type=bool_flag, default=False,\n",
        "                            help=\"Shuffle indices for different heads\")\n",
        "        parser.add_argument(\"--mem_shuffle_query\", type=bool_flag, default=False,\n",
        "                            help=\"Shuffle query dimensions (when the query network is the identity and there are multiple heads)\")\n",
        "        parser.add_argument(\"--mem_modulo_size\", type=int, default=-1,\n",
        "                            help=\"Effective memory size: indices are taken modulo this parameter. -1 to disable.\")\n",
        "\n",
        "        # keys\n",
        "        parser.add_argument(\"--mem_keys_type\", type=str, default=\"uniform\",\n",
        "                            help=\"Memory keys type (binary,gaussian,uniform)\")\n",
        "        parser.add_argument(\"--mem_n_keys\", type=int, default=512,\n",
        "                            help=\"Number of keys\")\n",
        "        parser.add_argument(\"--mem_keys_normalized_init\", type=bool_flag, default=False,\n",
        "                            help=\"Normalize keys at initialization\")\n",
        "        parser.add_argument(\"--mem_keys_learn\", type=bool_flag, default=False,\n",
        "                            help=\"Learn keys\")\n",
        "        parser.add_argument(\"--mem_use_different_keys\", type=bool_flag, default=False,\n",
        "                            help=\"Use different keys for each head / product quantization\")\n",
        "\n",
        "        # queries\n",
        "        parser.add_argument(\"--mem_query_detach_input\", type=bool_flag, default=False,\n",
        "                            help=\"Detach input\")\n",
        "        parser.add_argument(\"--mem_query_layer_sizes\", type=str, default=\"\",\n",
        "                            help=\"Query MLP layer sizes ('', '0', '0,512,0')\")\n",
        "        parser.add_argument(\"--mem_query_kernel_sizes\", type=str, default=\"\",\n",
        "                            help=\"Query MLP kernel sizes (2D inputs only)\")\n",
        "        parser.add_argument(\"--mem_query_bias\", type=bool_flag, default=True,\n",
        "                            help=\"Query MLP bias\")\n",
        "        parser.add_argument(\"--mem_query_batchnorm\", type=bool_flag, default=True,\n",
        "                            help=\"Query MLP batch norm\")\n",
        "        parser.add_argument(\"--mem_query_net_learn\", type=bool_flag, default=True,\n",
        "                            help=\"Query MLP learn\")\n",
        "        parser.add_argument(\"--mem_query_residual\", type=bool_flag, default=False,\n",
        "                            help=\"Use a bottleneck with a residual layer in the query MLP\")\n",
        "        parser.add_argument(\"--mem_multi_query_net\", type=bool_flag, default=False,\n",
        "                            help=\"Use multiple query MLP (one for each head)\")\n",
        "\n",
        "        # values initialization\n",
        "        parser.add_argument(\"--mem_value_zero_init\", type=bool_flag, default=False,\n",
        "                            help=\"Initialize values with zeros\")\n",
        "\n",
        "        # scoring\n",
        "        parser.add_argument(\"--mem_normalize_query\", type=bool_flag, default=False,\n",
        "                            help=\"Normalize queries\")\n",
        "        parser.add_argument(\"--mem_temperature\", type=float, default=1,\n",
        "                            help=\"Divide scores by a temperature\")\n",
        "        parser.add_argument(\"--mem_score_softmax\", type=bool_flag, default=True,\n",
        "                            help=\"Apply softmax on scores\")\n",
        "        parser.add_argument(\"--mem_score_subtract\", type=str, default=\"\",\n",
        "                            help=\"Subtract scores ('', min, mean, median)\")\n",
        "        parser.add_argument(\"--mem_score_normalize\", type=bool_flag, default=False,\n",
        "                            help=\"L1 normalization of the scores\")\n",
        "\n",
        "        # dropout\n",
        "        parser.add_argument(\"--mem_input_dropout\", type=float, default=0,\n",
        "                            help=\"Input dropout\")\n",
        "        parser.add_argument(\"--mem_query_dropout\", type=float, default=0,\n",
        "                            help=\"Query dropout\")\n",
        "        parser.add_argument(\"--mem_value_dropout\", type=float, default=0,\n",
        "                            help=\"Value dropout\")\n",
        "\n",
        "    @staticmethod\n",
        "    def build(input_dim, output_dim, params):\n",
        "        if params.mem_implementation == 'flat':\n",
        "            M = HashingMemoryFlat\n",
        "        elif params.mem_implementation == 'pq_default':\n",
        "            M = HashingMemoryProduct\n",
        "        elif params.mem_implementation == 'pq_fast':\n",
        "            M = HashingMemoryProductFast\n",
        "        else:\n",
        "            raise Exception(\"Unknown memory implementation!\")\n",
        "        return M(input_dim, output_dim, params)\n",
        "\n",
        "    @staticmethod\n",
        "    def check_params(params):\n",
        "        \"\"\"\n",
        "        Check and initialize memory parameters.\n",
        "        \"\"\"\n",
        "        # memory\n",
        "        assert params.mem_implementation in ['flat', 'pq_default', 'pq_fast']\n",
        "        params.mem_product_quantization = params.mem_implementation != 'flat'\n",
        "\n",
        "        # optimization\n",
        "        assert params.mem_grouped_conv is False or params.mem_multi_query_net\n",
        "        params.mem_values_optimizer = params.optimizer if params.mem_values_optimizer == '' else params.mem_values_optimizer\n",
        "        params.mem_values_optimizer = params.mem_values_optimizer.replace('adam', 'sparseadam') if params.mem_sparse else params.mem_values_optimizer\n",
        "\n",
        "        # even number of key dimensions for product quantization\n",
        "        assert params.mem_k_dim >= 2\n",
        "        assert params.mem_product_quantization is False or params.mem_k_dim % 2 == 0\n",
        "\n",
        "        # memory type\n",
        "        assert params.mem_keys_type in ['binary', 'gaussian', 'uniform']\n",
        "\n",
        "        # number of indices\n",
        "        if params.mem_keys_type == 'binary':\n",
        "            assert params.mem_keys_normalized_init is False\n",
        "            assert 1 << params.mem_k_dim == params.mem_n_keys\n",
        "        if params.mem_product_quantization:\n",
        "            params.n_indices = params.mem_n_keys ** 2\n",
        "        else:\n",
        "            params.n_indices = params.mem_n_keys\n",
        "\n",
        "        # actual memory size\n",
        "        if params.mem_modulo_size == -1:\n",
        "            params.mem_size = params.n_indices\n",
        "        else:\n",
        "            assert 1 <= params.mem_modulo_size < params.n_indices\n",
        "            params.mem_size = params.mem_modulo_size\n",
        "\n",
        "        # different keys / different query MLP / shuffle hidden dimensions when no query network\n",
        "        assert not params.mem_use_different_keys or params.mem_keys_type in ['gaussian', 'uniform']\n",
        "        assert not params.mem_use_different_keys or params.mem_heads >= 2 or params.mem_product_quantization\n",
        "        assert not params.mem_multi_query_net or params.mem_heads >= 2 or params.mem_product_quantization\n",
        "        assert not params.mem_multi_query_net or params.mem_query_layer_sizes not in ['', '0,0']\n",
        "        assert not params.mem_shuffle_query or params.mem_heads > 1 and params.mem_query_layer_sizes == ''\n",
        "\n",
        "        # query network\n",
        "        if params.mem_query_layer_sizes == '':\n",
        "            assert params.mem_heads == 1 or params.mem_use_different_keys or params.mem_shuffle_query\n",
        "        else:\n",
        "            s = [int(x) for x in filter(None, params.mem_query_layer_sizes.split(','))]\n",
        "            assert len(s) >= 2 and s[0] == s[-1] == 0\n",
        "            params.mem_query_layer_sizes = s\n",
        "            assert not params.mem_query_residual or params.mem_input2d\n",
        "\n",
        "        # convolutional query network kernel sizes\n",
        "        if params.mem_query_kernel_sizes == '':\n",
        "            assert not params.mem_input2d or params.mem_query_layer_sizes == ''\n",
        "        else:\n",
        "            assert params.mem_input2d\n",
        "            s = [int(x) for x in filter(None, params.mem_query_kernel_sizes.split(','))]\n",
        "            params.mem_query_kernel_sizes = s\n",
        "            assert all(ks % 2 == 1 for ks in s)\n",
        "            assert len(params.mem_query_kernel_sizes) == len(params.mem_query_layer_sizes) - 1 >= 1\n",
        "\n",
        "        # scoring\n",
        "        assert params.mem_score_subtract in ['', 'min', 'mean', 'median']\n",
        "        assert params.mem_score_subtract == '' or params.mem_knn >= 2\n",
        "        assert not (params.mem_score_normalize and params.mem_score_softmax and params.mem_score_subtract == '')\n",
        "\n",
        "        # dropout\n",
        "        assert 0 <= params.mem_input_dropout < 1\n",
        "        assert 0 <= params.mem_query_dropout < 1\n",
        "        assert 0 <= params.mem_value_dropout < 1\n",
        "\n",
        "\n",
        "class HashingMemoryFlat(HashingMemory):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, params):\n",
        "        super().__init__(input_dim, output_dim, params)\n",
        "        assert self.use_different_keys is False or self.heads >= 2\n",
        "        assert not self.product_quantization\n",
        "\n",
        "    def init_keys(self):\n",
        "        \"\"\"\n",
        "        Initialize keys.\n",
        "        \"\"\"\n",
        "        assert self.keys_type in ['binary', 'gaussian', 'uniform']\n",
        "\n",
        "        # binary keys\n",
        "        if self.keys_type == 'binary':\n",
        "            keys = torch.FloatTensor(2 ** self.k_dim, self.k_dim)\n",
        "            for i in range(keys.shape[0]):\n",
        "                for j in range(keys.shape[1]):\n",
        "                    keys[i, j] = int((1 << j) & i > 0)\n",
        "            keys *= 2\n",
        "            keys -= 1\n",
        "            keys /= math.sqrt(self.k_dim)\n",
        "\n",
        "        # random keys from Gaussian or uniform distributions\n",
        "        if self.keys_type in ['gaussian', 'uniform']:\n",
        "            init = get_gaussian_keys if self.keys_type == 'gaussian' else get_uniform_keys\n",
        "            if self.use_different_keys:\n",
        "                keys = torch.from_numpy(np.array([\n",
        "                    init(self.n_indices, self.k_dim, self.keys_normalized_init, seed=i)\n",
        "                    for i in range(self.heads)\n",
        "                ])).view(self.heads, self.n_indices, self.k_dim)\n",
        "            else:\n",
        "                keys = torch.from_numpy(init(self.n_indices, self.k_dim, self.keys_normalized_init, seed=0))\n",
        "\n",
        "        # learned or fixed keys\n",
        "        if self.learn_keys:\n",
        "            self.keys = nn.Parameter(keys)\n",
        "        else:\n",
        "            self.register_buffer('keys', keys)\n",
        "\n",
        "    # def _get_indices(self, query, knn, keys):\n",
        "    #     \"\"\"\n",
        "    #     Generate scores and indices given keys and unnormalized queries.\n",
        "    #     \"\"\"\n",
        "    #     QUERY_SIZE = 4096\n",
        "    #     assert query.dim() == 2 and query.size(1) == self.k_dim\n",
        "\n",
        "    #     # optionally normalize queries\n",
        "    #     if self.normalize_query:\n",
        "    #         query = query / query.norm(2, 1, keepdim=True).expand_as(query)  # (bs, kdim)\n",
        "\n",
        "    #     # compute memory indices, and split the query if it is too large\n",
        "    #     with torch.no_grad():\n",
        "    #         if len(query) <= QUERY_SIZE:\n",
        "    #             indices = get_knn_faiss(keys.float(), query.float(), knn, distance='dot_product')[1]\n",
        "    #         else:\n",
        "    #             indices = torch.cat([\n",
        "    #                 get_knn_faiss(keys.float(), query[i:i + QUERY_SIZE].float(), knn, distance='dot_product')[1]\n",
        "    #                 for i in range(0, len(query), QUERY_SIZE)\n",
        "    #             ], 0)\n",
        "    #             # indices0 = get_knn_faiss(keys.float(), query.float(), knn, distance='dot_product')[1]\n",
        "    #             # assert (indices0 - indices).abs().sum().item() == 0\n",
        "    #         assert len(indices) == len(query)\n",
        "\n",
        "    #     # compute value scores\n",
        "    #     scores = (keys[indices] * query.unsqueeze(1)).sum(2)\n",
        "\n",
        "    #     # return scores with indices\n",
        "    #     assert scores.shape == indices.shape == (query.shape[0], knn)\n",
        "    #     return scores, indices\n",
        "\n",
        "    def _get_indices(self, query, knn, keys):\n",
        "        \"\"\"\n",
        "        Generate scores and indices given keys and unnormalized queries.\n",
        "        \"\"\"\n",
        "        assert query.dim() == 2 and query.size(1) == self.k_dim\n",
        "\n",
        "        # optionally normalize queries\n",
        "        if self.normalize_query:\n",
        "            query = query / query.norm(2, 1, keepdim=True).expand_as(query)   # (bs, kdim)\n",
        "\n",
        "        # compute scores with indices\n",
        "        scores = F.linear(query, keys, bias=None)                             # (bs, n_keys)\n",
        "        scores, indices = scores.topk(knn, dim=1, largest=True, sorted=True)  # (bs, knn) ** 2\n",
        "        # scores, indices = get_knn_faiss(keys.float(), query.float().contiguous(), knn, distance='dot_product')   # (bs, knn) ** 2\n",
        "\n",
        "        # return scores with indices\n",
        "        assert scores.shape == indices.shape == (query.shape[0], knn)\n",
        "        return scores, indices\n",
        "\n",
        "    def get_indices(self, query, knn):\n",
        "        \"\"\"\n",
        "        Generate scores and indices given unnormalized queries.\n",
        "        \"\"\"\n",
        "        assert query.dim() == 2 and query.size(1) == self.k_dim\n",
        "        if self.use_different_keys is False:\n",
        "            return self._get_indices(query, knn, self.keys)\n",
        "        else:\n",
        "            bs = len(query)\n",
        "            query = query.view(-1, self.heads, self.k_dim)\n",
        "            outputs = [\n",
        "                self._get_indices(query[:, i], knn, self.keys[i])\n",
        "                for i in range(self.heads)\n",
        "            ]\n",
        "            scores = torch.cat([s.unsqueeze(1) for s, _ in outputs], 1).view(bs, knn)\n",
        "            indices = torch.cat([idx.unsqueeze(1) for _, idx in outputs], 1).view(bs, knn)\n",
        "            return scores, indices\n",
        "\n",
        "\n",
        "class HashingMemoryProduct(HashingMemory):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, params):\n",
        "        super().__init__(input_dim, output_dim, params)\n",
        "        assert self.k_dim % 2 == 0\n",
        "        assert self.product_quantization\n",
        "\n",
        "    def create_keys(self):\n",
        "        \"\"\"\n",
        "        This function creates keys and returns them.\n",
        "        I guess you could see that from the name of the function and the fact that is has a return statement.\n",
        "        \"\"\"\n",
        "        assert self.keys_type in ['binary', 'gaussian', 'uniform']\n",
        "        half = self.k_dim // 2\n",
        "        n_keys = int(self.n_indices ** 0.5)\n",
        "\n",
        "        # binary keys\n",
        "        if self.keys_type == 'binary':\n",
        "            keys = torch.FloatTensor(2 ** half, half)\n",
        "            for i in range(keys.shape[0]):\n",
        "                for j in range(keys.shape[1]):\n",
        "                    keys[i, j] = int((1 << j) & i > 0)\n",
        "            keys *= 2\n",
        "            keys -= 1\n",
        "            keys /= math.sqrt(self.k_dim)\n",
        "\n",
        "        # random keys from Gaussian or uniform distributions\n",
        "        if self.keys_type in ['gaussian', 'uniform']:\n",
        "            init = get_gaussian_keys if self.keys_type == 'gaussian' else get_uniform_keys\n",
        "            if self.use_different_keys:\n",
        "                keys = torch.from_numpy(np.array([\n",
        "                    init(n_keys, half, self.keys_normalized_init, seed=(2 * i + j))\n",
        "                    for i in range(self.heads)\n",
        "                    for j in range(2)\n",
        "                ])).view(self.heads, 2, n_keys, half)\n",
        "            else:\n",
        "                keys = torch.from_numpy(init(n_keys, half, self.keys_normalized_init, seed=0))\n",
        "\n",
        "        return keys\n",
        "\n",
        "    def init_keys(self):\n",
        "        \"\"\"\n",
        "        Initialize keys.\n",
        "        \"\"\"\n",
        "        keys = self.create_keys()\n",
        "\n",
        "        # learned or fixed keys\n",
        "        if self.learn_keys:\n",
        "            self.keys = nn.Parameter(keys)\n",
        "        else:\n",
        "            self.register_buffer('keys', keys)\n",
        "\n",
        "    def _get_indices(self, query, knn, keys1, keys2):\n",
        "        \"\"\"\n",
        "        Generate scores and indices given keys and unnormalized queries.\n",
        "        \"\"\"\n",
        "        assert query.dim() == 2 and query.size(1) == self.k_dim\n",
        "        assert len(keys1) == len(keys2)\n",
        "        half = self.k_dim // 2\n",
        "        n_keys = len(keys1)\n",
        "\n",
        "        # split query for product quantization\n",
        "        q1 = query[:, :half]                                                                            # (bs, half)\n",
        "        q2 = query[:, half:]                                                                            # (bs, half)\n",
        "\n",
        "        # optionally normalize queries\n",
        "        if self.normalize_query:\n",
        "            q1 = q1 / q1.norm(2, 1, keepdim=True).expand_as(q1)                                         # (bs, half)\n",
        "            q2 = q2 / q2.norm(2, 1, keepdim=True).expand_as(q2)                                         # (bs, half)\n",
        "\n",
        "        # compute memory value indices\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # compute indices with associated scores\n",
        "            scores1, indices1 = get_knn_faiss(keys1.float(), q1.float(), knn, distance='dot_product')  # (bs, knn) ** 2\n",
        "            scores2, indices2 = get_knn_faiss(keys2.float(), q2.float(), knn, distance='dot_product')  # (bs, knn) ** 2\n",
        "\n",
        "            # cartesian product on best candidate keys\n",
        "            concat_scores = cartesian_product(scores1, scores2)                                         # (bs, knn ** 2, 2)\n",
        "            concat_indices = cartesian_product(indices1, indices2)                                      # (bs, knn ** 2, 2)\n",
        "\n",
        "            all_scores = concat_scores.sum(2)                                                           # (bs, knn ** 2)\n",
        "            all_indices = concat_indices[:, :, 0] * n_keys + concat_indices[:, :, 1]                    # (bs, knn ** 2)\n",
        "\n",
        "            _scores, best_indices = torch.topk(all_scores, k=knn, dim=1, largest=True, sorted=True)     # (bs, knn)\n",
        "            indices = all_indices.gather(1, best_indices)                                               # (bs, knn)\n",
        "\n",
        "        # compute value scores - for some reason, this part is extremely slow when the keys are learned\n",
        "        indices1 = indices / n_keys\n",
        "        indices2 = indices % n_keys\n",
        "        scores1 = (keys1[indices1] * q1.unsqueeze(1)).sum(2)\n",
        "        scores2 = (keys2[indices2] * q2.unsqueeze(1)).sum(2)\n",
        "        scores = scores1 + scores2\n",
        "\n",
        "        # return scores with indices\n",
        "        assert scores.shape == indices.shape == (query.shape[0], knn)\n",
        "        return scores, indices\n",
        "\n",
        "    def get_indices(self, query, knn):\n",
        "        \"\"\"\n",
        "        Generate scores and indices given unnormalized queries.\n",
        "        \"\"\"\n",
        "        assert query.dim() == 2 and query.size(1) == self.k_dim\n",
        "        if self.use_different_keys is False:\n",
        "            return self._get_indices(query, knn, self.keys, self.keys)\n",
        "        else:\n",
        "            bs = len(query)\n",
        "            query = query.view(-1, self.heads, self.k_dim)\n",
        "            outputs = [\n",
        "                self._get_indices(query[:, i], knn, self.keys[i][0], self.keys[i][1])\n",
        "                for i in range(self.heads)\n",
        "            ]\n",
        "            scores = torch.cat([s.unsqueeze(1) for s, _ in outputs], 1).view(bs, knn)\n",
        "            indices = torch.cat([idx.unsqueeze(1) for _, idx in outputs], 1).view(bs, knn)\n",
        "            return scores, indices\n",
        "\n",
        "\n",
        "class HashingMemoryProductFast(HashingMemoryProduct):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, params):\n",
        "        super().__init__(input_dim, output_dim, params)\n",
        "\n",
        "    def _get_indices(self, query, knn, keys1, keys2):\n",
        "        \"\"\"\n",
        "        Generate scores and indices given keys and unnormalized queries.\n",
        "        \"\"\"\n",
        "        assert query.dim() == 2 and query.size(1) == self.k_dim\n",
        "        assert len(keys1) == len(keys2)\n",
        "        bs = query.size(0)\n",
        "        half = self.k_dim // 2\n",
        "        n_keys = len(keys1)\n",
        "\n",
        "        # split query for product quantization\n",
        "        q1 = query[:, :half]                                                                                          # (bs, half)\n",
        "        q2 = query[:, half:]                                                                                          # (bs, half)\n",
        "\n",
        "        # optionally normalize queries\n",
        "        if self.normalize_query:\n",
        "            q1 = q1 / q1.norm(2, 1, keepdim=True).expand_as(q1)                                                       # (bs, half)\n",
        "            q2 = q2 / q2.norm(2, 1, keepdim=True).expand_as(q2)                                                       # (bs, half)\n",
        "\n",
        "        # compute indices with associated scores\n",
        "        scores1 = F.linear(q1, keys1, bias=None)                                                                      # (bs, n_keys ** 0.5)\n",
        "        scores2 = F.linear(q2, keys2, bias=None)                                                                      # (bs, n_keys ** 0.5)\n",
        "        scores1, indices1 = scores1.topk(knn, dim=1, largest=True, sorted=True)                                       # (bs, knn) ** 2\n",
        "        scores2, indices2 = scores2.topk(knn, dim=1, largest=True, sorted=True)                                       # (bs, knn) ** 2\n",
        "        # scores1, indices1 = get_knn_faiss(keys1, q1.contiguous(), knn, distance='dot_product')                        # (bs, knn) ** 2\n",
        "        # scores2, indices2 = get_knn_faiss(keys2, q2.contiguous(), knn, distance='dot_product')                        # (bs, knn) ** 2\n",
        "\n",
        "        # cartesian product on best candidate keys\n",
        "        all_scores = (\n",
        "            scores1.view(bs, knn, 1).expand(bs, knn, knn) +\n",
        "            scores2.view(bs, 1, knn).expand(bs, knn, knn)\n",
        "        ).view(bs, -1)                                                                                                # (bs, knn ** 2)\n",
        "        all_indices = (\n",
        "            indices1.view(bs, knn, 1).expand(bs, knn, knn) * n_keys +\n",
        "            indices2.view(bs, 1, knn).expand(bs, knn, knn)\n",
        "        ).view(bs, -1)                                                                                                # (bs, knn ** 2)\n",
        "\n",
        "        # select overall best scores and indices\n",
        "        scores, best_indices = torch.topk(all_scores, k=knn, dim=1, largest=True, sorted=True)                        # (bs, knn)\n",
        "        indices = all_indices.gather(1, best_indices)                                                                 # (bs, knn)\n",
        "\n",
        "        # code below: debug instant retrieval speed\n",
        "        # scores = torch.zeros(bs, knn, dtype=query.dtype, device=query.device)\n",
        "        # indices = torch.arange(knn, dtype=torch.int64, device=query.device).view(1, knn).expand(bs, knn)\n",
        "\n",
        "        # return scores with indices\n",
        "        assert scores.shape == indices.shape == (bs, knn)\n",
        "        return scores, indices"
      ],
      "metadata": {
        "id": "UNLopb5b8UNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "def mlp(sizes, bias=True, batchnorm=True, groups=1):\n",
        "    \"\"\"\n",
        "    Generate a feedforward neural network.\n",
        "    \"\"\"\n",
        "    assert len(sizes) >= 2\n",
        "    pairs = [(sizes[i], sizes[i + 1]) for i in range(len(sizes) - 1)]\n",
        "    layers = []\n",
        "\n",
        "    for i, (dim_in, dim_out) in enumerate(pairs):\n",
        "        if groups == 1 or i == 0:\n",
        "            layers.append(nn.Linear(dim_in, groups * dim_out, bias=bias))\n",
        "        else:\n",
        "            layers.append(GroupedLinear(groups * dim_in, groups * dim_out, bias=bias, groups=groups))\n",
        "        if batchnorm:\n",
        "            layers.append(nn.BatchNorm1d(groups * dim_out))\n",
        "        if i < len(pairs) - 1:\n",
        "            layers.append(nn.ReLU())\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def convs(channel_sizes, kernel_sizes, bias=True, batchnorm=True, residual=False, groups=1):\n",
        "    \"\"\"\n",
        "    Generate a convolutional neural network.\n",
        "    \"\"\"\n",
        "    assert len(channel_sizes) >= 2\n",
        "    assert len(channel_sizes) == len(kernel_sizes) + 1\n",
        "    pairs = [(channel_sizes[i], channel_sizes[i + 1]) for i in range(len(channel_sizes) - 1)]\n",
        "    layers = []\n",
        "\n",
        "    for i, (dim_in, dim_out) in enumerate(pairs):\n",
        "        ks = (kernel_sizes[i], kernel_sizes[i])\n",
        "        in_group = 1 if i == 0 else groups\n",
        "        _dim_in = dim_in * in_group\n",
        "        _dim_out = dim_out * groups\n",
        "        if not residual:\n",
        "            layers.append(nn.Conv2d(_dim_in, _dim_out, ks, padding=[k // 2 for k in ks], bias=bias, groups=in_group))\n",
        "            if batchnorm:\n",
        "                layers.append(nn.BatchNorm2d(_dim_out))\n",
        "            if i < len(pairs) - 1:\n",
        "                layers.append(nn.ReLU())\n",
        "        else:\n",
        "            layers.append(BottleneckResidualConv2d(\n",
        "                _dim_in, _dim_out, ks, bias=bias,\n",
        "                batchnorm=batchnorm, groups=in_group\n",
        "            ))\n",
        "            if i == len(pairs) - 1:\n",
        "                layers.append(nn.Conv2d(_dim_out, _dim_out, (1, 1), bias=bias))\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class GroupedLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True, groups=1):\n",
        "\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.groups = groups\n",
        "        self.bias = bias\n",
        "        assert groups > 1\n",
        "\n",
        "        self.layer = nn.Conv1d(in_features, out_features, bias=bias, kernel_size=1, groups=groups)\n",
        "\n",
        "    def forward(self, input):\n",
        "        assert input.dim() == 2 and input.size(1) == self.in_features\n",
        "        return self.layer(input.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, groups={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.groups, self.bias is not None\n",
        "        )\n",
        "\n",
        "\n",
        "class BottleneckResidualConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, bias=True, batchnorm=True, groups=1):\n",
        "\n",
        "        super().__init__()\n",
        "        hidden_channels = min(input_channels, output_channels)\n",
        "        assert all(k % 2 == 1 for k in kernel_size)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size, padding=[k // 2 for k in kernel_size], bias=bias, groups=groups)\n",
        "        self.conv2 = nn.Conv2d(hidden_channels, output_channels, kernel_size, padding=[k // 2 for k in kernel_size], bias=bias, groups=groups)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        self.batchnorm = batchnorm\n",
        "        if self.batchnorm:\n",
        "            self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
        "            self.bn2 = nn.BatchNorm2d(output_channels)\n",
        "\n",
        "        if input_channels == output_channels:\n",
        "            self.residual = nn.Sequential()\n",
        "        else:\n",
        "            self.residual = nn.Conv2d(input_channels, output_channels, (1, 1), bias=False, groups=groups)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.bn1(x) if self.batchnorm else x\n",
        "        x = self.act(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x) if self.batchnorm else x\n",
        "        x = self.act(x + self.residual(input))\n",
        "        return x\n",
        "\n",
        "\n",
        "class QueryIdentity(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, heads, shuffle_hidden):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.heads = heads\n",
        "        self.shuffle_query = shuffle_hidden\n",
        "        assert shuffle_hidden is False or heads > 1\n",
        "        assert shuffle_hidden is False or self.input_dim % (2 ** self.heads) == 0\n",
        "        if shuffle_hidden:\n",
        "            self.slices = {head_id: get_slices(input_dim, head_id) for head_id in range(heads)}\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Generate queries from hidden states by either\n",
        "        repeating them or creating some shuffled version.\n",
        "        \"\"\"\n",
        "        assert input.shape[-1] == self.input_dim\n",
        "        input = input.contiguous().view(-1, self.input_dim) if input.dim() > 2 else input\n",
        "        bs = len(input)\n",
        "\n",
        "        if self.heads == 1:\n",
        "            query = input\n",
        "\n",
        "        elif not self.shuffle_query:\n",
        "            query = input.unsqueeze(1).repeat(1, self.heads, 1)\n",
        "            query = query.view(bs * self.heads, self.input_dim)\n",
        "\n",
        "        else:\n",
        "            query = torch.cat([\n",
        "                input[:, a:b]\n",
        "                for head_id in range(self.heads)\n",
        "                for a, b in self.slices[head_id]\n",
        "            ], 1).view(bs * self.heads, self.input_dim)\n",
        "\n",
        "        assert query.shape == (bs * self.heads, self.input_dim)\n",
        "        return query\n",
        "\n",
        "\n",
        "class QueryMLP(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, input_dim, heads, k_dim, product_quantization, multi_query_net,\n",
        "        sizes, bias=True, batchnorm=True, grouped_conv=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.heads = heads\n",
        "        self.k_dim = k_dim\n",
        "        self.sizes = sizes\n",
        "        self.grouped_conv = grouped_conv\n",
        "        assert not multi_query_net or product_quantization or heads >= 2\n",
        "        assert sizes[0] == input_dim\n",
        "        assert sizes[-1] == (k_dim // 2) if multi_query_net else (heads * k_dim)\n",
        "        assert self.grouped_conv is False or len(sizes) > 2\n",
        "\n",
        "        # number of required MLPs\n",
        "        self.groups = (2 * heads) if multi_query_net else 1\n",
        "\n",
        "        # MLPs\n",
        "        if self.grouped_conv:\n",
        "            self.query_mlps = mlp(sizes, bias=bias, batchnorm=batchnorm, groups=self.groups)\n",
        "        elif len(self.sizes) == 2:\n",
        "            sizes_ = list(sizes)\n",
        "            sizes_[-1] = sizes_[-1] * self.groups\n",
        "            self.query_mlps = mlp(sizes_, bias=bias, batchnorm=batchnorm, groups=1)\n",
        "        else:\n",
        "            self.query_mlps = nn.ModuleList([\n",
        "                mlp(sizes, bias=bias, batchnorm=batchnorm, groups=1)\n",
        "                for _ in range(self.groups)\n",
        "            ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Compute queries using either grouped 1D convolutions or ModuleList + concat.\n",
        "        \"\"\"\n",
        "        assert input.shape[-1] == self.input_dim\n",
        "        input = input.contiguous().view(-1, self.input_dim) if input.dim() > 2 else input\n",
        "        bs = len(input)\n",
        "\n",
        "        if self.grouped_conv or len(self.sizes) == 2:\n",
        "            query = self.query_mlps(input)\n",
        "        else:\n",
        "            outputs = [m(input) for m in self.query_mlps]\n",
        "            query = torch.cat(outputs, 1) if len(outputs) > 1 else outputs[0]\n",
        "\n",
        "        assert query.shape == (bs, self.heads * self.k_dim)\n",
        "        return query.view(bs * self.heads, self.k_dim)\n",
        "\n",
        "\n",
        "class QueryConv(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self, input_dim, heads, k_dim, product_quantization, multi_query_net,\n",
        "        sizes, kernel_sizes, bias=True, batchnorm=True,\n",
        "        residual=False, grouped_conv=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.heads = heads\n",
        "        self.k_dim = k_dim\n",
        "        self.sizes = sizes\n",
        "        self.grouped_conv = grouped_conv\n",
        "        assert not multi_query_net or product_quantization or heads >= 2\n",
        "        assert sizes[0] == input_dim\n",
        "        assert sizes[-1] == (k_dim // 2) if multi_query_net else (heads * k_dim)\n",
        "        assert self.grouped_conv is False or len(sizes) > 2\n",
        "        assert len(sizes) == len(kernel_sizes) + 1 >= 2 and all(ks % 2 == 1 for ks in kernel_sizes)\n",
        "\n",
        "        # number of required CNNs\n",
        "        self.groups = (2 * heads) if multi_query_net else 1\n",
        "\n",
        "        # CNNs\n",
        "        if self.grouped_conv:\n",
        "            self.query_convs = convs(sizes, kernel_sizes, bias=bias, batchnorm=batchnorm, residual=residual, groups=self.groups)\n",
        "        elif len(self.sizes) == 2:\n",
        "            sizes_ = list(sizes)\n",
        "            sizes_[-1] = sizes_[-1] * self.groups\n",
        "            self.query_convs = convs(sizes_, kernel_sizes, bias=bias, batchnorm=batchnorm, residual=residual, groups=1)\n",
        "        else:\n",
        "            self.query_convs = nn.ModuleList([\n",
        "                convs(sizes, kernel_sizes, bias=bias, batchnorm=batchnorm, residual=residual, groups=1)\n",
        "                for _ in range(self.groups)\n",
        "            ])\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        bs, nf, h, w = input.shape\n",
        "        assert nf == self.input_dim\n",
        "\n",
        "        if self.grouped_conv or len(self.sizes) == 2:\n",
        "            query = self.query_convs(input)\n",
        "        else:\n",
        "            outputs = [m(input) for m in self.query_convs]\n",
        "            query = torch.cat(outputs, 1) if len(outputs) > 1 else outputs[0]\n",
        "\n",
        "        assert query.shape == (bs, self.heads * self.k_dim, h, w)\n",
        "        query = query.transpose(1, 3).contiguous().view(bs * w * h * self.heads, self.k_dim)\n",
        "        return query"
      ],
      "metadata": {
        "id": "KbUeKHz-8gAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "# load FAISS GPU library if available (dramatically accelerates the nearest neighbor search)\n",
        "try:\n",
        "    import faiss\n",
        "    FAISS_AVAILABLE = hasattr(faiss, 'StandardGpuResources')\n",
        "except ImportError:\n",
        "    FAISS_AVAILABLE = False\n",
        "    sys.stderr.write(\"Impossible to import FAISS library!!\\n\")\n",
        "\n",
        "\n",
        "def get_gaussian_keys(n_keys, dim, normalized, seed):\n",
        "    \"\"\"\n",
        "    Generate random Gaussian keys.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    X = rng.randn(n_keys, dim)\n",
        "    if normalized:\n",
        "        X /= np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "\n",
        "def get_uniform_keys(n_keys, dim, normalized, seed):\n",
        "    \"\"\"\n",
        "    Generate random uniform keys (same initialization as nn.Linear).\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(seed)\n",
        "    bound = 1 / math.sqrt(dim)\n",
        "    X = rng.uniform(-bound, bound, (n_keys, dim))\n",
        "    if normalized:\n",
        "        X /= np.linalg.norm(X, axis=1, keepdims=True)\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "\n",
        "def get_slices(dim, head_id):\n",
        "    \"\"\"\n",
        "    Generate slices of hidden dimensions.\n",
        "    Used when there are multiple heads and/or different set of keys,\n",
        "    and that there is no query network.\n",
        "    \"\"\"\n",
        "    if head_id == 0:\n",
        "        return [(0, dim)]\n",
        "    offset = dim // (2 ** (head_id + 1))\n",
        "    starts = np.arange(0, dim, offset)\n",
        "    slices1 = [(x, x + offset) for i, x in enumerate(starts) if i % 2 == 0]\n",
        "    slices2 = [(x, x + offset) for i, x in enumerate(starts) if i % 2 == 1]\n",
        "    return slices1 + slices2\n",
        "\n",
        "\n",
        "def cartesian_product(a, b):\n",
        "    \"\"\"\n",
        "    Compute the batched cartesian product between two matrices.\n",
        "    Input:\n",
        "        a: Tensor(n, d1)\n",
        "        b: Tensor(n, d2)\n",
        "    Output:\n",
        "        output: Tensor(n, d1 * d2, 2)\n",
        "    \"\"\"\n",
        "    n1, d1 = a.shape\n",
        "    n2, d2 = b.shape\n",
        "    assert n1 == n2\n",
        "    return torch.cat([\n",
        "        a.unsqueeze(-1).repeat(1, 1, d2).unsqueeze(-1),\n",
        "        b.repeat(1, d1).view(n2, d1, d2).unsqueeze(-1)\n",
        "    ], 3).view(n1, d1 * d2, 2)\n",
        "\n",
        "\n",
        "def swig_ptr_from_FloatTensor(x):\n",
        "    assert x.is_contiguous()\n",
        "    assert x.dtype == torch.float32\n",
        "    return faiss.cast_integer_to_float_ptr(x.storage().data_ptr() + x.storage_offset() * 4)\n",
        "\n",
        "\n",
        "def swig_ptr_from_LongTensor(x):\n",
        "    assert x.is_contiguous()\n",
        "    assert x.dtype == torch.int64, 'dtype=%s' % x.dtype\n",
        "    return faiss.cast_integer_to_long_ptr(x.storage().data_ptr() + x.storage_offset() * 8)\n",
        "\n",
        "\n",
        "def get_knn_pytorch(a, b, k, distance='dot_product'):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        - matrix of size (m, d) (keys)\n",
        "        - matrix of size (n, d) (queries)\n",
        "        - number of nearest neighbors\n",
        "        - distance metric\n",
        "    Output:\n",
        "        - `scores`  matrix of size (n, k) with nearest neighors scores\n",
        "        - `indices` matrix of size (n, k) with nearest neighors indices\n",
        "    \"\"\"\n",
        "    m, d = a.size()\n",
        "    n, _ = b.size()\n",
        "    assert b.size(1) == d\n",
        "    assert k > 0\n",
        "    assert distance in ['dot_product', 'cosine', 'l2']\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        if distance == 'dot_product':\n",
        "            scores = a.mm(b.t())                                 # (m, n)\n",
        "\n",
        "        elif distance == 'cosine':\n",
        "            scores = a.mm(b.t())                                 # (m, n)\n",
        "            scores /= (a.norm(2, 1)[:, None] + 1e-9)             # (m, n)\n",
        "            scores /= (b.norm(2, 1)[None, :] + 1e-9)             # (m, n)\n",
        "\n",
        "        elif distance == 'l2':\n",
        "            scores = a.mm(b.t())                                 # (m, n)\n",
        "            scores *= 2                                          # (m, n)\n",
        "            scores -= (a ** 2).sum(1)[:, None]                   # (m, n)\n",
        "            scores -= (b ** 2).sum(1)[None, :]                   # (m, n)\n",
        "\n",
        "        scores, indices = scores.topk(k=k, dim=0, largest=True)  # (k, n)\n",
        "        scores = scores.t()                                      # (n, k)\n",
        "        indices = indices.t()                                    # (n, k)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "\n",
        "def get_knn_faiss(xb, xq, k, distance='dot_product'):\n",
        "    \"\"\"\n",
        "    `metric` can be faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2\n",
        "    https://github.com/facebookresearch/faiss/blob/master/gpu/test/test_pytorch_faiss.py\n",
        "    \"\"\"\n",
        "    assert xb.device == xq.device\n",
        "    assert distance in ['dot_product', 'l2']\n",
        "    metric = faiss.METRIC_INNER_PRODUCT if distance == 'dot_product' else faiss.METRIC_L2\n",
        "\n",
        "    xq_ptr = swig_ptr_from_FloatTensor(xq)\n",
        "    xb_ptr = swig_ptr_from_FloatTensor(xb)\n",
        "\n",
        "    nq, d1 = xq.size()\n",
        "    nb, d2 = xb.size()\n",
        "    assert d1 == d2\n",
        "\n",
        "    D = torch.empty(nq, k, device=xb.device, dtype=torch.float32)\n",
        "    I = torch.empty(nq, k, device=xb.device, dtype=torch.int64)\n",
        "\n",
        "    D_ptr = swig_ptr_from_FloatTensor(D)\n",
        "    I_ptr = swig_ptr_from_LongTensor(I)\n",
        "\n",
        "    faiss.bruteForceKnn(\n",
        "        FAISS_RES, metric,\n",
        "        xb_ptr, nb,\n",
        "        xq_ptr, nq,\n",
        "        d1, k, D_ptr, I_ptr\n",
        "    )\n",
        "\n",
        "    return D, I\n",
        "\n",
        "\n",
        "if FAISS_AVAILABLE:\n",
        "    FAISS_RES = faiss.StandardGpuResources()\n",
        "    FAISS_RES.setDefaultNullStreamAllDevices()\n",
        "    FAISS_RES.setTempMemory(1200 * 1024 * 1024)\n",
        "    get_knn = get_knn_faiss\n",
        "else:\n",
        "    sys.stderr.write(\"Switching to standard nearest neighbors search implementation, this will be significantly slower.\\n\")\n",
        "    get_knn = get_knn_pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uajUrcFv8pfG",
        "outputId": "4877fde2-4b83-4e33-dc1e-5d5d8e52d423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Impossible to import FAISS library!!\n",
            "Switching to standard nearest neighbors search implementation, this will be significantly slower.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Maluuba/nlg-eval.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XtdymZZxCYS6",
        "outputId": "ebf5bfcb-f6e6-4212-ecc4-5784b0d6a481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/Maluuba/nlg-eval.git@master\n",
            "  Cloning https://github.com/Maluuba/nlg-eval.git (to revision master) to /tmp/pip-req-build-243osu4o\n",
            "  Running command git clone -q https://github.com/Maluuba/nlg-eval.git /tmp/pip-req-build-243osu4o\n",
            "Collecting nltk>=3.4.5\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from nlg-eval==2.3) (1.21.6)\n",
            "Collecting psutil>=5.6.2\n",
            "  Downloading psutil-5.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/dist-packages (from nlg-eval==2.3) (2.23.0)\n",
            "Requirement already satisfied: six>=1.11 in /usr/local/lib/python3.7/dist-packages (from nlg-eval==2.3) (1.15.0)\n",
            "Requirement already satisfied: Cython>=0.28.5 in /usr/local/lib/python3.7/dist-packages (from nlg-eval==2.3) (0.29.28)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from nlg-eval==2.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.7/dist-packages (from nlg-eval==2.3) (1.0.2)\n",
            "Collecting gensim~=3.8.3\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting Theano>=0.8.1\n",
            "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.24 in /usr/local/lib/python3.7/dist-packages (from nlg-eval==2.3) (4.64.0)\n",
            "Collecting xdg\n",
            "  Downloading xdg-5.1.1-py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim~=3.8.3->nlg-eval==2.3) (5.2.1)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.5->nlg-eval==2.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4.5->nlg-eval==2.3) (1.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19->nlg-eval==2.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19->nlg-eval==2.3) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19->nlg-eval==2.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19->nlg-eval==2.3) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17->nlg-eval==2.3) (3.1.0)\n",
            "Building wheels for collected packages: nlg-eval, Theano\n",
            "  Building wheel for nlg-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlg-eval: filename=nlg_eval-2.3-py3-none-any.whl size=68175165 sha256=180c6a460628ddf2b1bac79f2870c4f185591bf091006b55847ee088a1364c1c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qrywb12n/wheels/08/20/df/33ced66932f198c4323042d18ff1c2db9b9716369f0de4afb4\n",
            "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Theano: filename=Theano-1.0.5-py3-none-any.whl size=2668111 sha256=0359f69f9349cb91ce102a37cc656d8dcc1df1126a831fad59ad7510b87d8751\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/68/6f/745330367ce7822fe0cd863712858151f5723a0a5e322cc144\n",
            "Successfully built nlg-eval Theano\n",
            "Installing collected packages: regex, xdg, Theano, psutil, nltk, gensim, nlg-eval\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed Theano-1.0.5 gensim-3.8.3 nlg-eval-2.3 nltk-3.7 psutil-5.9.0 regex-2022.4.24 xdg-5.1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nlgeval\n",
        "import nlgeval.utils"
      ],
      "metadata": {
        "id": "lk35FJWMCLjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import getLogger\n",
        "import os\n",
        "import copy\n",
        "import time\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import OrderedDict, defaultdict\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "# import rouge\n",
        "from nlgeval import NLGEval\n",
        "\n",
        "\n",
        "\n",
        "XPersona_LANGS = [\"en\", \"zh\"]\n",
        "# XPersona_LANGS = [\"en\", \"fr\"]\n",
        "# XPersona_LANGS = [\"en\", \"it\"]\n",
        "# XPersona_LANGS = [\"en\", \"id\"]\n",
        "# XPersona_LANGS = [\"en\", \"jp\"]\n",
        "# XPersona_LANGS = [\"en\", \"ko\"]\n",
        "\n",
        "logger = getLogger()\n",
        "# evaluator = rouge.Rouge(\n",
        "#   metrics=['rouge-n', 'rouge-l'],\n",
        "#   max_n=2,\n",
        "#   limit_length=True,\n",
        "#   length_limit=100,\n",
        "#   length_limit_type='words',\n",
        "#   alpha=0.5, # Default F1_score\n",
        "#   weight_factor=1.2,\n",
        "#   stemming=False)\n",
        "nlgeval = NLGEval(\n",
        "  no_skipthoughts=True,no_glove=True,metrics_to_omit=['CIDEr'])\n",
        "\n",
        "\n",
        "def get_parameters(model, train_layers_str):\n",
        "  ret = []\n",
        "\n",
        "  fr, to = map(int, train_layers_str.split(\",\"))\n",
        "  assert fr >= 0\n",
        "  if fr == 0:\n",
        "    # add embeddings\n",
        "    ret += model.embeddings.parameters()\n",
        "    logger.info(\"Adding embedding parameters\")\n",
        "    ret += model.position_embeddings.parameters()\n",
        "    logger.info(\"Adding positional embedding parameters\")\n",
        "    ret += model.lang_embeddings.parameters()\n",
        "    logger.info(\"Adding language embedding parameters\")\n",
        "    fr = 1\n",
        "  assert fr <= to\n",
        "  # add attention layers\n",
        "  # NOTE cross attention is not added\n",
        "  for i in range(fr, to + 1):\n",
        "    ret += model.attentions[i-1].parameters()\n",
        "    ret += model.layer_norm1[i-1].parameters()\n",
        "    ret += model.ffns[i-1].parameters()\n",
        "    ret += model.layer_norm2[i-1].parameters()\n",
        "    logger.info(\"Adding layer-%s parameters to optimizer\" % i)\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "def tokens2words(toks):\n",
        "  words = []\n",
        "  for tok in toks:\n",
        "    if len(words) > 0 and words[-1].endswith(\"@@\"):\n",
        "      words[-1] = words[-1][:-2] + tok\n",
        "    else:\n",
        "      words.append(tok)\n",
        "  return words\n",
        "\n",
        "\n",
        "class XPersona(object):\n",
        "\n",
        "  def __init__(self, encoder, decoder, scores, dico, params):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.params = params\n",
        "    self.scores = scores\n",
        "    self.dico = dico\n",
        "\n",
        "    self.iter_cache = {}\n",
        "  \n",
        "  def setup_vocab_mask(self, dico):\n",
        "    n_words = len(dico)\n",
        "    params = self.params\n",
        "\n",
        "    self.vocab_mask = {}\n",
        "\n",
        "    decode_vocab_sizes = [int(s) for s in params.decode_vocab_sizes.split(\",\")]\n",
        "    assert len(decode_vocab_sizes) == len(XPersona_LANGS)\n",
        "\n",
        "    for lang, sz in zip(XPersona_LANGS, decode_vocab_sizes):\n",
        "      \n",
        "      fn = os.path.join(params.vocab_path, lang + \".vocab\")\n",
        "      assert os.path.isfile(fn), fn\n",
        "\n",
        "      mask = torch.ByteTensor(n_words)\n",
        "      mask.fill_(0)\n",
        "      assert mask.sum() == 0\n",
        "      mask[dico.eos_index] = 1\n",
        "      # TODO generate unk?\n",
        "      mask[dico.unk_index] = 1\n",
        "      count = 0\n",
        "      with open(fn) as fp:\n",
        "        for line, _ in zip(fp, range(sz)):\n",
        "          tok = line.strip().split(\"\\t\")[0].split(\" \")[0]\n",
        "          if tok not in dico.word2id:\n",
        "            # logger.warn(\"Token %s not in dico\" % tok)\n",
        "            count += 1\n",
        "          else: mask[dico.word2id[tok]] = 1\n",
        "      \n",
        "      # mask[dico.word2id[\"<@@\"]] = 0\n",
        "      logger.warn(\"%d tokens not in dico\" % count)\n",
        "      self.vocab_mask[lang] = mask\n",
        "  \n",
        "  def gen_references_v2(self, dico, eval_directions):\n",
        "    self.references = {}\n",
        "    for split in [\"valid\", \"test\"]:\n",
        "      self.references[split] = {}\n",
        "      for direction in eval_directions:\n",
        "        x_lang, y_lang = direction\n",
        "        if y_lang in self.references: continue\n",
        "        refs = []\n",
        "        for batch in self.get_iterator(split, x_lang, y_lang):\n",
        "          _, (sent_y, len_y), _ = batch\n",
        "          for j in range(len(len_y)):\n",
        "            ref_sent = sent_y[1:len_y[j]-1,j]\n",
        "            ref_toks = [dico[ref_sent[k].item()] for k in range(len(ref_sent))]\n",
        "            ref_words = tokens2words(ref_toks)\n",
        "\n",
        "            #zh or en2zh\n",
        "            if y_lang.endswith(\"zh\"): refs.append(\" \".join(\"\".join(ref_words)))\n",
        "            else: refs.append(\" \".join(ref_words))\n",
        "\n",
        "        self.references[split][y_lang] = refs\n",
        "  \n",
        "  def _parse_lang(self, lang):\n",
        "    if type(lang) == tuple:\n",
        "      assert len(lang) == 2\n",
        "      lang1, lang2 = lang\n",
        "      assert lang1 in XPersona_LANGS\n",
        "      assert lang2 in XPersona_LANGS\n",
        "      return (lang1, lang2)\n",
        "    if type(lang) == str:\n",
        "      if lang in XPersona_LANGS:\n",
        "        return (lang, lang)\n",
        "      else:\n",
        "        lang1, lang2 = lang.split(\"2\")\n",
        "        assert lang1 in XPersona_LANGS\n",
        "        assert lang2 in XPersona_LANGS\n",
        "        return (lang1, lang2)\n",
        "\n",
        "  def get_iterator(self, splt, x_lang, y_lang):\n",
        "    x_lang = self._parse_lang(x_lang)\n",
        "    y_lang = self._parse_lang(y_lang)\n",
        "    logger.info(\"Getting iterator -- x_lang: (%s, %s), y_lang: (%s, %s) split:%s\" % (\n",
        "      x_lang[0], x_lang[1], y_lang[0], y_lang[1], splt))\n",
        "    return self.get_or_load_data(x_lang, y_lang, splt).get_iterator(\n",
        "      shuffle=(splt == 'train'),\n",
        "      group_by_size=self.params.group_by_size,\n",
        "      return_indices=True)\n",
        "  \n",
        "  def next_batch(self, splt, x_lang, y_lang):\n",
        "    \n",
        "    key = (splt, x_lang, y_lang)\n",
        "    if key not in self.iter_cache:\n",
        "      self.iter_cache[key] = self.get_iterator(splt, x_lang, y_lang)\n",
        "    try:\n",
        "      ret = next(self.iter_cache[key])\n",
        "    except StopIteration:\n",
        "      self.iter_cache[key] = self.get_iterator(splt, x_lang, y_lang)\n",
        "      ret = next(self.iter_cache[key])\n",
        "    return ret\n",
        "  \n",
        "  def lang2str(self, lang):\n",
        "    lang1, lang2 = lang\n",
        "    if lang1 == lang2: return lang1\n",
        "    return \"%s-%s\" % (lang1, lang2)\n",
        "  \n",
        "  def get_or_load_data(self, x_lang, y_lang, splt):\n",
        "    params = self.params\n",
        "    data = self.data\n",
        "\n",
        "    lang = (x_lang, y_lang)\n",
        "    if lang in self.data:\n",
        "      if splt in self.data[lang]: return self.data[lang][splt]\n",
        "    else:\n",
        "      self.data[lang] = {}\n",
        "    \n",
        "    dpath = os.path.join(params.data_path, \"eval\", params.ds_name)\n",
        "\n",
        "    x = load_binarized(os.path.join(dpath, \"%s.x.%s.pth\" % (\n",
        "      splt, self.lang2str(x_lang))), params)\n",
        "    y = load_binarized(os.path.join(dpath, \"%s.y.%s.pth\" % (\n",
        "      splt, self.lang2str(y_lang))), params)\n",
        "    data[\"dico\"] = data.get(\"dico\", x[\"dico\"])\n",
        "    set_dico_parameters(params, data, x[\"dico\"])\n",
        "    set_dico_parameters(params, data, y[\"dico\"])\n",
        "\n",
        "    data[lang][splt] = ParallelDataset(\n",
        "      x[\"sentences\"], x[\"positions\"],\n",
        "      y[\"sentences\"], y[\"positions\"],\n",
        "      params)\n",
        "    data[lang][splt].remove_empty_sentences()\n",
        "    data[lang][splt].cut_long_sentences(params.max_len, params.max_len)\n",
        "\n",
        "    if params.cut_dataset > 0 and splt == \"train\":\n",
        "      data[lang][splt].select_data(0, params.cut_dataset + 1)\n",
        "\n",
        "    return self.data[lang][splt]\n",
        "\n",
        "  def run(self):\n",
        "    params = self.params\n",
        "\n",
        "    train_directions = [d.split(\"-\") for d in params.train_directions.split(\",\")]\n",
        "    eval_directions = [d.split(\"-\") for d in params.eval_directions.split(\",\")]\n",
        "\n",
        "    self.data = {}\n",
        "  \n",
        "    # self.encoder.cuda()\n",
        "    # self.decoder.cuda()\n",
        "    self.encoder.to(params.device1)\n",
        "    self.decoder.to(params.device2)\n",
        "\n",
        "    parameters = []\n",
        "    if params.train_layers == \"all\":\n",
        "      parameters.extend([_ for _ in self.encoder.parameters()])\n",
        "      parameters.extend([_ for _ in self.decoder.parameters()])\n",
        "    elif params.train_layers == \"decoder\":\n",
        "      parameters = self.decoder.parameters()\n",
        "    elif params.train_layers == \"encoder\":\n",
        "      parameters = self.encoder.parameters()\n",
        "    else:\n",
        "      parameters = get_parameters(self.encoder, params.train_layers)\n",
        "    self.optimizer = get_optimizer(parameters, params.optimizer)\n",
        "\n",
        "    self.gen_references_v2(self.dico, eval_directions)\n",
        "    if self.params.decode_with_vocab: self.setup_vocab_mask(self.dico)\n",
        "\n",
        "    # self.best_scores = defaultdict(float)\n",
        "    self.best_ppl = 1000000\n",
        "    \n",
        "    for epoch in range(params.n_epochs):\n",
        "      self.epoch = epoch\n",
        "      logger.info(\"XPersona - Training epoch %d ...\" % epoch)\n",
        "      self.train(train_directions)\n",
        "      logger.info(\"XPersona - Evaluating epoch %d ...\" % epoch)\n",
        "      self.eval(eval_directions, \"valid\", True)\n",
        "      self.eval(eval_directions, \"test\", False)\n",
        "  \n",
        "  def gen_resp_(self):\n",
        "    params = self.params\n",
        "\n",
        "    train_directions = [d.split(\"-\") for d in params.train_directions.split(\",\")]\n",
        "    eval_directions = [d.split(\"-\") for d in params.eval_directions.split(\",\")]\n",
        "\n",
        "    self.data = {}\n",
        "  \n",
        "    self.encoder.cuda()\n",
        "    self.decoder.cuda()\n",
        "    parameters = []\n",
        "    if params.train_layers == \"all\":\n",
        "      parameters.extend([_ for _ in self.encoder.parameters()])\n",
        "      parameters.extend([_ for _ in self.decoder.parameters()])\n",
        "    elif params.train_layers == \"decoder\":\n",
        "      parameters = self.decoder.parameters()\n",
        "    elif params.train_layers == \"encoder\":\n",
        "      parameters = self.encoder.parameters()\n",
        "    else:\n",
        "      parameters = get_parameters(self.encoder, params.train_layers)\n",
        "    self.optimizer = get_optimizer(parameters, params.optimizer)\n",
        "\n",
        "    self.gen_references_v2(self.dico, eval_directions)\n",
        "    if self.params.decode_with_vocab: self.setup_vocab_mask(self.dico)\n",
        "\n",
        "    # self.best_scores = defaultdict(float)\n",
        "    self.best_ppl = 0\n",
        "    self.generate_response(eval_directions, \"test\")\n",
        "  \n",
        "  def test(self):\n",
        "    params = self.params\n",
        "\n",
        "    # train_directions = [d.split(\"-\") for d in params.train_directions.split(\",\")]\n",
        "    eval_directions = [d.split(\"-\") for d in params.eval_directions.split(\",\")]\n",
        "\n",
        "    self.data = {}\n",
        "  \n",
        "    # self.encoder.cuda()\n",
        "    # self.decoder.cuda()\n",
        "    self.encoder.to(params.device1)\n",
        "    self.decoder.to(params.device2)\n",
        "    parameters = []\n",
        "    if params.train_layers == \"all\":\n",
        "      parameters.extend([_ for _ in self.encoder.parameters()])\n",
        "      parameters.extend([_ for _ in self.decoder.parameters()])\n",
        "    elif params.train_layers == \"decoder\":\n",
        "      parameters = self.decoder.parameters()\n",
        "    elif params.train_layers == \"encoder\":\n",
        "      parameters = self.encoder.parameters()\n",
        "    else:\n",
        "      parameters = get_parameters(self.encoder, params.train_layers)\n",
        "    self.optimizer = get_optimizer(parameters, params.optimizer)\n",
        "\n",
        "    self.gen_references_v2(self.dico, eval_directions)\n",
        "    if self.params.decode_with_vocab: self.setup_vocab_mask(self.dico)\n",
        "\n",
        "    # self.best_scores = defaultdict(float)\n",
        "    self.best_ppl = 0\n",
        "    self.epoch = 100\n",
        "    self.eval(eval_directions, \"test\", False)\n",
        "  \n",
        "  def generate_response(self, eval_directions, split=\"test\"):\n",
        "    params = self.params\n",
        "    encoder = self.encoder\n",
        "    decoder = self.decoder\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    dico = self.dico\n",
        "    # best_scores = self.best_scores\n",
        "\n",
        "    for direction in eval_directions:\n",
        "      x_lang, y_lang = direction\n",
        "      logger.info(\"Evaluating %s-%s-xpersona on %s set\" % (x_lang, y_lang, split))\n",
        "\n",
        "      X, Y = [], []\n",
        "      x_lang_id = params.lang2id[x_lang[-2:]]\n",
        "      y_lang_id = params.lang2id[y_lang[-2:]]\n",
        "      vocab_mask=self.vocab_mask[y_lang[-2:]] if params.decode_with_vocab else None\n",
        "\n",
        "      perplexity_list = []\n",
        "      for batch in self.get_iterator(split, x_lang, y_lang):\n",
        "        (sent_x, len_x), (sent_y, len_y), _ = batch\n",
        "        lang_x = sent_x.clone().fill_(x_lang_id)\n",
        "        lang_y = sent_y.clone().fill_(y_lang_id)\n",
        "\n",
        "        sent_x, len_x, lang_x, sent_y, len_y, lang_y = to_cuda(sent_x, len_x, lang_x, sent_y, len_y, lang_y)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          encoded = encoder(\n",
        "            \"fwd\", x=sent_x, lengths=len_x, langs=lang_x, causal=False)\n",
        "          encoded = encoded.transpose(0, 1)\n",
        "          \n",
        "          # calculate perplexity\n",
        "          alen = torch.arange(len_y.max(), dtype=torch.long, device=len_y.device)\n",
        "          pred_mask = alen[:, None] < len_y[None] - 1\n",
        "          y = sent_y[1:].masked_select(pred_mask[:-1])\n",
        "\n",
        "          if params.beam_size == 1:\n",
        "            decoded, _ = decoder.generate(\n",
        "              encoded, len_x, y_lang_id, max_len=params.max_dec_len,\n",
        "              vocab_mask=vocab_mask)\n",
        "          else:\n",
        "            decoded, _ = decoder.generate_beam(\n",
        "              encoded, len_x, y_lang_id, beam_size=params.beam_size,\n",
        "              length_penalty=0.9, early_stopping=False,\n",
        "              max_len=params.max_dec_len, vocab_mask=vocab_mask)\n",
        "      \n",
        "        for j in range(decoded.size(1)):\n",
        "          sent = decoded[:, j]\n",
        "          delimiters = (sent == params.eos_index).nonzero().view(-1)\n",
        "          assert len(delimiters) >= 1 and delimiters[0].item() == 0\n",
        "          sent = sent[1:] if len(delimiters) == 1  else sent[1: delimiters[1]]\n",
        "\n",
        "          trg_tokens = [dico[sent[k].item()] for k in range(len(sent))]\n",
        "          trg_words = tokens2words(trg_tokens)\n",
        "          if y_lang.endswith(\"zh\"): Y.append(\" \".join(\"\".join(trg_words)))\n",
        "          else: Y.append(\" \".join(trg_words))\n",
        "\n",
        "          # if len(X) < 5:\n",
        "          x_sent = sent_x[1:len_x[j], j]\n",
        "          x_toks = [dico[x_sent[k].item()] for k in range(len(x_sent))]\n",
        "          x_words = tokens2words(x_toks)\n",
        "          X.append(x_words)\n",
        "    \n",
        "      logger.info(\"%d res %d ref\" % (len(Y), len(self.references[split][y_lang])))\n",
        "      for i in range(len(X)):\n",
        "        logger.info(\"%d X: %s\\nGenerated: %s\\nReference: %s\\n\" % (\n",
        "            i, \" \".join(X[i]), Y[i], self.references[split][y_lang][i]))\n",
        "      \n",
        "  def train(self, train_directions):\n",
        "    params = self.params\n",
        "    encoder = self.encoder\n",
        "    decoder = self.decoder\n",
        "\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    # training variables\n",
        "    losses = []\n",
        "    ns = 0  # number of sentences\n",
        "    nw = 0  # number of words\n",
        "    t = time.time()\n",
        "\n",
        "    # x_lang, y_lang = train_direction\n",
        "    # x_lang_id = params.lang2id[x_lang[-2:]]\n",
        "    # y_lang_id = params.lang2id[y_lang[-2:]]\n",
        "    n_train_drt = len(train_directions)\n",
        "\n",
        "    for step_idx in range(params.epoch_size):\n",
        "\n",
        "      x_lang, y_lang = train_directions[step_idx % n_train_drt]\n",
        "      x_lang_id = params.lang2id[x_lang[-2:]]\n",
        "      y_lang_id = params.lang2id[y_lang[-2:]]\n",
        "\n",
        "      batch = self.next_batch(\"train\", x_lang, y_lang)\n",
        "      (sent_x, len_x), (sent_y, len_y), _ = batch\n",
        "      lang_x = sent_x.clone().fill_(x_lang_id)\n",
        "      lang_y = sent_y.clone().fill_(y_lang_id)\n",
        "      alen = torch.arange(len_y.max(), dtype=torch.long, device=len_y.device)\n",
        "      pred_mask = alen[:, None] < len_y[None] - 1\n",
        "      y = sent_y[1:].masked_select(pred_mask[:-1])\n",
        "      assert len(y) == (len_y-1).sum().item()\n",
        "\n",
        "      # sent_x, len_x, lang_x, sent_y, len_y, lang_y, y = to_cuda(\n",
        "        # sent_x, len_x, lang_x, sent_y, len_y, lang_y, y)\n",
        "      \n",
        "      sent_x, len_x, lang_x = sent_x.to(params.device1), len_x.to(params.device1), lang_x.to(params.device1)\n",
        "      sent_y, len_y, lang_y, y = sent_y.to(params.device2), len_y.to(params.device2), lang_y.to(params.device2), y.to(params.device2)\n",
        "\n",
        "      enc_x = self.encoder(\"fwd\", x=sent_x, lengths=len_x, langs=lang_x, causal=False)\n",
        "      enc_x = enc_x.transpose(0, 1)\n",
        "\n",
        "      enc_x, len_x = enc_x.to(params.device2), len_x.to(params.device2)\n",
        "\n",
        "      dec_y = self.decoder('fwd', x=sent_y, lengths=len_y, langs=lang_y,\n",
        "        causal=True, src_enc=enc_x, src_len=len_x)\n",
        "      \n",
        "      _, loss = self.decoder(\"predict\", tensor=dec_y, pred_mask=pred_mask, y=y,\n",
        "        get_scores=False)\n",
        "      \n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      bs = len(len_y)\n",
        "      ns += bs\n",
        "      nw += len_y.sum().item()\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      # log\n",
        "      if ns % (100 * bs) < bs:\n",
        "        logger.info(\n",
        "          \"XPersona - Epoch %i - Train iter %7i - %.1f words/s - Loss: %.4f\" % (\n",
        "            self.epoch, ns, nw / (time.time() - t), sum(losses) / len(losses)))\n",
        "        nw, t = 0, time.time()\n",
        "        losses = []\n",
        "  \n",
        "  def eval(self, eval_directions, split=\"test\", save=True):\n",
        "    params = self.params\n",
        "    encoder = self.encoder\n",
        "    decoder = self.decoder\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    dico = self.dico\n",
        "    # best_scores = self.best_scores\n",
        "\n",
        "    for direction in eval_directions:\n",
        "      x_lang, y_lang = direction\n",
        "      logger.info(\"Evaluating %s-%s-xpersona on %s set\" % (x_lang, y_lang, split))\n",
        "\n",
        "      X, Y = [], []\n",
        "      x_lang_id = params.lang2id[x_lang[-2:]]\n",
        "      y_lang_id = params.lang2id[y_lang[-2:]]\n",
        "      vocab_mask=self.vocab_mask[y_lang[-2:]] if params.decode_with_vocab else None\n",
        "\n",
        "      # perplexity_list = []\n",
        "      loss_list = []\n",
        "      for batch in self.get_iterator(split, x_lang, y_lang):\n",
        "        (sent_x, len_x), (sent_y, len_y), _ = batch\n",
        "        lang_x = sent_x.clone().fill_(x_lang_id)\n",
        "        lang_y = sent_y.clone().fill_(y_lang_id)\n",
        "\n",
        "        # sent_x, len_x, lang_x, sent_y, len_y, lang_y = to_cuda(sent_x, len_x, lang_x, sent_y, len_y, lang_y)\n",
        "        \n",
        "        sent_x, len_x, lang_x = sent_x.to(params.device1), len_x.to(params.device1), lang_x.to(params.device1)\n",
        "        sent_y, len_y, lang_y = sent_y.to(params.device2), len_y.to(params.device2), lang_y.to(params.device2)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          encoded = encoder(\n",
        "            \"fwd\", x=sent_x, lengths=len_x, langs=lang_x, causal=False)\n",
        "          encoded = encoded.transpose(0, 1)\n",
        "          \n",
        "          encoded, len_x = encoded.to(params.device2), len_x.to(params.device2)\n",
        "\n",
        "          # calculate perplexity\n",
        "          alen = torch.arange(len_y.max(), dtype=torch.long, device=len_y.device)\n",
        "          pred_mask = alen[:, None] < len_y[None] - 1\n",
        "          y = sent_y[1:].masked_select(pred_mask[:-1])\n",
        "\n",
        "          dec_y = self.decoder('fwd', x=sent_y, lengths=len_y, langs=lang_y, causal=True, src_enc=encoded, src_len=len_x)\n",
        "          _, loss = self.decoder(\"predict\", tensor=dec_y, pred_mask=pred_mask, y=y, get_scores=False)\n",
        "\n",
        "          loss_list.append(loss.item())\n",
        "          # perplexity = math.exp(loss.item())\n",
        "          # perplexity_list.append(perplexity)\n",
        "\n",
        "          if params.beam_size == 1:\n",
        "            decoded, _ = decoder.generate(\n",
        "              encoded, len_x, y_lang_id, max_len=params.max_dec_len,\n",
        "              vocab_mask=vocab_mask)\n",
        "          else:\n",
        "            decoded, _ = decoder.generate_beam(\n",
        "              encoded, len_x, y_lang_id, beam_size=params.beam_size,\n",
        "              length_penalty=0.9, early_stopping=False,\n",
        "              max_len=params.max_dec_len, vocab_mask=vocab_mask)\n",
        "      \n",
        "        for j in range(decoded.size(1)):\n",
        "          sent = decoded[:, j]\n",
        "          delimiters = (sent == params.eos_index).nonzero().view(-1)\n",
        "          assert len(delimiters) >= 1 and delimiters[0].item() == 0\n",
        "          sent = sent[1:] if len(delimiters) == 1  else sent[1: delimiters[1]]\n",
        "\n",
        "          trg_tokens = [dico[sent[k].item()] for k in range(len(sent))]\n",
        "          trg_words = tokens2words(trg_tokens)\n",
        "          if y_lang.endswith(\"zh\"): Y.append(\" \".join(\"\".join(trg_words)))\n",
        "          else: Y.append(\" \".join(trg_words))\n",
        "\n",
        "          # if len(X) < 5:\n",
        "          x_sent = sent_x[1:len_x[j], j]\n",
        "          x_toks = [dico[x_sent[k].item()] for k in range(len(x_sent))]\n",
        "          x_words = tokens2words(x_toks)\n",
        "          X.append(x_words)\n",
        "    \n",
        "      logger.info(\"%d res %d ref\" % (len(Y), len(self.references[split][y_lang])))\n",
        "      for i in range(5):\n",
        "        logger.info(\"%d X: %s\\nGenerated: %s\\nReference: %s\\n\" % (\n",
        "            i, \" \".join(X[i]), Y[i], self.references[split][y_lang][i]))\n",
        "      eval_res = nlgeval.compute_metrics([self.references[split][y_lang][:len(Y)]], Y)\n",
        "      # eval_res = evaluator.get_scores(Y, self.references[y_lang][:len(Y)])\n",
        "\n",
        "      direction_str = \"-\".join(direction)\n",
        "\n",
        "      # if save:\n",
        "      #   if eval_res[\"Bleu_4\"] > best_scores[direction_str]:\n",
        "      #     logger.info(\"New best Bleu_4 score: %.5f! Saving model...\" % eval_res[\"Bleu_4\"])\n",
        "      #     best_scores[direction_str] = eval_res[\"Bleu_4\"]\n",
        "      #     self.save(\"best_%s_Bleu_4\" % direction_str)\n",
        "      \n",
        "      # use perplexity to stop train\n",
        "      # calculate perplexity\n",
        "      avg_loss = np.mean(loss_list)\n",
        "      perplexity = math.exp(avg_loss)\n",
        "      if save:\n",
        "        if perplexity < self.best_ppl:\n",
        "          logger.info(\"New best Perplexity: %.5f! Saving model...\" % perplexity)\n",
        "          self.best_ppl = perplexity\n",
        "          self.save(\"best_%s_Perplexity\" % direction_str)\n",
        "      \n",
        "      if split == \"test\":\n",
        "        print(\"writing down output and refenerences ....\")\n",
        "        assert len(X) == len(self.references[split][y_lang][:len(Y)]) == len(Y)\n",
        "\n",
        "        with open(os.path.join(self.params.dump_path, \"output_\"+str(self.epoch)+\"_\"+str(y_lang)+\".txt\"), \"w\") as f:\n",
        "          for sent in Y:\n",
        "            f.write(sent + \"\\n\")\n",
        "        with open(os.path.join(self.params.dump_path, \"ref_\"+str(self.epoch)+\"_\"+str(y_lang)+\".txt\"), \"w\") as f:\n",
        "          for sent in self.references[split][y_lang][:len(Y)]:\n",
        "            f.write(sent + \"\\n\")\n",
        "        with open(os.path.join(self.params.dump_path, \"persona_chat_\"+str(self.epoch)+\"_\"+str(y_lang)+\".txt\"), \"w\") as f:\n",
        "          for persona_and_history, output, reference in zip(X, Y, self.references[split][y_lang][:len(Y)]):\n",
        "            f.write(\"=====================================================\\n\")\n",
        "            f.write(\"History:\\n\")\n",
        "            f.write(\" \".join(persona_and_history))\n",
        "            f.write('\\n')\n",
        "            f.write(\"Response: \")\n",
        "            f.write(output)\n",
        "            f.write('\\n')\n",
        "            f.write(\"Ref: \")\n",
        "            f.write(reference)\n",
        "            f.write('\\n')\n",
        "      # logger.info(\"XPersona - %s - Epoch %d - Best BLEU-4: %.5f - scores: %s\" % (\n",
        "      #   direction_str, self.epoch, best_scores[direction_str], eval_res))\n",
        "      \n",
        "      logger.info(\"XPersona - %s - Epoch %d - Current Perplexity %.5f - Best Perplexity: %.5f - Metrics Scores: %s\" % (\n",
        "        direction_str, self.epoch, perplexity, self.best_ppl, eval_res))\n",
        "      \n",
        "      # eval_res_print = {metric:results[\"f\"] for metric, results in sorted(eval_res.items(), key=lambda x: x[0])}\n",
        "\n",
        "      # logger.info(\"XPersona - %s - Epoch %d - Best rouge-l: %.5f - scores: %s\" % (\n",
        "      #   direction_str, self.epoch, best_scores[direction_str], eval_res_print))\n",
        "      \n",
        "      # if eval_res[\"rouge-l\"]['f'] > best_scores[direction_str]:\n",
        "      #   logger.info(\"New best rouge-l score! Saving model...\")\n",
        "      #   best_scores[direction_str] = eval_res[\"rouge-l\"]['f']\n",
        "      #   self.save(\"best_%s_rouge-l\" % direction_str)\n",
        "\n",
        "  def save(self, name):\n",
        "    path = os.path.join(self.params.dump_path, \"%s.pth\" % name)\n",
        "    logger.info(\"Saving %s to %s ...\" % (name, path))\n",
        "    data = {\n",
        "      \"epoch\": getattr(self, \"epoch\", 0),\n",
        "      \"encoder\": self.encoder.state_dict(),\n",
        "      \"decoder\": self.decoder.state_dict(),\n",
        "      \"enc_params\": {\n",
        "        k: v for k, v in self.params.encoder_model_params.__dict__.items()},\n",
        "      \"dec_params\": {\n",
        "        k: v for k, v in self.params.decoder_model_params.__dict__.items()},\n",
        "      \"dico_id2word\": self.dico.id2word,\n",
        "      \"dico_word2id\": self.dico.word2id,\n",
        "      \"dico_counts\": self.dico.counts,\n",
        "      \"params\": {k: v for k, v in self.params.__dict__.items()}\n",
        "    }\n",
        "    torch.save(data, path)"
      ],
      "metadata": {
        "id": "qsjJdJ4JzPLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import getLogger\n",
        "import math\n",
        "import itertools\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "N_MAX_POSITIONS = 512  # maximum input sequence length\n",
        "\n",
        "DECODER_ONLY_PARAMS = [\n",
        "    'layer_norm15.%i.weight', 'layer_norm15.%i.bias',\n",
        "    'encoder_attn.%i.q_lin.weight', 'encoder_attn.%i.q_lin.bias',\n",
        "    'encoder_attn.%i.k_lin.weight', 'encoder_attn.%i.k_lin.bias',\n",
        "    'encoder_attn.%i.v_lin.weight', 'encoder_attn.%i.v_lin.bias',\n",
        "    'encoder_attn.%i.out_lin.weight', 'encoder_attn.%i.out_lin.bias'\n",
        "]\n",
        "\n",
        "TRANSFORMER_LAYER_PARAMS = [\n",
        "    'attentions.%i.q_lin.weight', 'attentions.%i.q_lin.bias',\n",
        "    'attentions.%i.k_lin.weight', 'attentions.%i.k_lin.bias',\n",
        "    'attentions.%i.v_lin.weight', 'attentions.%i.v_lin.bias',\n",
        "    'attentions.%i.out_lin.weight', 'attentions.%i.out_lin.bias',\n",
        "    'layer_norm1.%i.weight', 'layer_norm1.%i.bias',\n",
        "    'ffns.%i.lin1.weight', 'ffns.%i.lin1.bias',\n",
        "    'ffns.%i.lin2.weight', 'ffns.%i.lin2.bias',\n",
        "    'layer_norm2.%i.weight', 'layer_norm2.%i.bias'\n",
        "]\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def Embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
        "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
        "    if padding_idx is not None:\n",
        "        nn.init.constant_(m.weight[padding_idx], 0)\n",
        "    return m\n",
        "\n",
        "\n",
        "def Linear(in_features, out_features, bias=True):\n",
        "    m = nn.Linear(in_features, out_features, bias)\n",
        "    # nn.init.normal_(m.weight, mean=0, std=1)\n",
        "    # nn.init.xavier_uniform_(m.weight)\n",
        "    # nn.init.constant_(m.bias, 0.)\n",
        "    return m\n",
        "\n",
        "\n",
        "def create_sinusoidal_embeddings(n_pos, dim, out):\n",
        "    position_enc = np.array([\n",
        "        [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
        "        for pos in range(n_pos)\n",
        "    ])\n",
        "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "    out.detach_()\n",
        "    out.requires_grad = False\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "    GELU activation\n",
        "    https://arxiv.org/abs/1606.08415\n",
        "    https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/model_pytorch.py#L14\n",
        "    https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/modeling.py\n",
        "    \"\"\"\n",
        "    # return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def get_masks(slen, lengths, causal):\n",
        "    \"\"\"\n",
        "    Generate hidden states mask, and optionally an attention mask.\n",
        "    \"\"\"\n",
        "    assert lengths.max().item() <= slen\n",
        "    bs = lengths.size(0)\n",
        "    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
        "    mask = alen < lengths[:, None]\n",
        "\n",
        "    # attention mask is the same as mask, or triangular inferior attention (causal)\n",
        "    if causal:\n",
        "        attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]\n",
        "    else:\n",
        "        attn_mask = mask\n",
        "\n",
        "    # sanity check\n",
        "    assert mask.size() == (bs, slen)\n",
        "    assert causal is False or attn_mask.size() == (bs, slen, slen)\n",
        "\n",
        "    return mask, attn_mask\n",
        "\n",
        "\n",
        "class PredLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Prediction layer (cross_entropy or adaptive_softmax).\n",
        "    \"\"\"\n",
        "    def __init__(self, params):\n",
        "        super().__init__()\n",
        "        self.asm = params.asm\n",
        "        self.n_words = params.n_words\n",
        "        self.pad_index = params.pad_index\n",
        "        dim = params.emb_dim\n",
        "\n",
        "        if params.asm is False:\n",
        "            self.proj = Linear(dim, params.n_words, bias=True)\n",
        "        else:\n",
        "            self.proj = nn.AdaptiveLogSoftmaxWithLoss(\n",
        "                in_features=dim,\n",
        "                n_classes=params.n_words,\n",
        "                cutoffs=params.asm_cutoffs,\n",
        "                div_value=params.asm_div_value,\n",
        "                head_bias=True,  # default is False\n",
        "            )\n",
        "\n",
        "    def forward(self, x, y, get_scores=False):\n",
        "        \"\"\"\n",
        "        Compute the loss, and optionally the scores.\n",
        "        \"\"\"\n",
        "        assert (y == self.pad_index).sum().item() == 0\n",
        "\n",
        "        if self.asm is False:\n",
        "            scores = self.proj(x).view(-1, self.n_words)\n",
        "            loss = F.cross_entropy(scores, y, reduction='mean')\n",
        "        else:\n",
        "            _, loss = self.proj(x, y)\n",
        "            scores = self.proj.log_prob(x) if get_scores else None\n",
        "\n",
        "        return scores, loss\n",
        "\n",
        "    def get_scores(self, x):\n",
        "        \"\"\"\n",
        "        Compute scores.\n",
        "        \"\"\"\n",
        "        assert x.dim() == 2\n",
        "        return self.proj.log_prob(x) if self.asm else self.proj(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    NEW_ID = itertools.count()\n",
        "\n",
        "    def __init__(self, n_heads, dim, dropout, tf_cls):\n",
        "        super().__init__()\n",
        "        self.layer_id = next(MultiHeadAttention.NEW_ID)\n",
        "        self.dim = dim\n",
        "        self.n_heads = n_heads\n",
        "        self.ms2s = (tf_cls != TransformerModel)\n",
        "        self.dropout = dropout\n",
        "        assert self.dim % self.n_heads == 0\n",
        "\n",
        "        self.q_lin = Linear(dim, dim)\n",
        "        self.k_lin = Linear(dim, dim)\n",
        "        self.v_lin = Linear(dim, dim)\n",
        "        self.out_lin = Linear(dim, dim)\n",
        "\n",
        "    def forward(self, input, mask, kv=None, cache=None, ms2s=False):\n",
        "        \"\"\"\n",
        "        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n",
        "        \"\"\"\n",
        "        # Input is (bs, qlen, dim)\n",
        "        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n",
        "        bs, qlen, dim = input.size()\n",
        "        if kv is None:\n",
        "            klen = qlen if cache is None else cache['slen'] + qlen\n",
        "        else:\n",
        "            klen = kv.size(1)\n",
        "        assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)\n",
        "        n_heads = self.n_heads\n",
        "        dim_per_head = dim // n_heads\n",
        "        mask_reshape = (bs, 1, qlen, klen) if mask.dim() == 3 else (bs, 1, 1, klen)\n",
        "\n",
        "        def shape(x):\n",
        "            \"\"\"  projection \"\"\"\n",
        "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
        "\n",
        "        def unshape(x):\n",
        "            \"\"\"  compute context \"\"\"\n",
        "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
        "\n",
        "        q = shape(self.q_lin(input))                                          # (bs, n_heads, qlen, dim_per_head)\n",
        "        if kv is None:\n",
        "            k = shape(self.k_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n",
        "            v = shape(self.v_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n",
        "        elif cache is None or self.layer_id not in cache:\n",
        "            k = v = kv\n",
        "            k = shape(self.k_lin(k))                                          # (bs, n_heads, qlen, dim_per_head)\n",
        "            v = shape(self.v_lin(v))                                          # (bs, n_heads, qlen, dim_per_head)\n",
        "\n",
        "        if cache is not None:\n",
        "            if self.ms2s:\n",
        "                if self.layer_id in cache:\n",
        "                    if kv is None:\n",
        "                        k_, v_ = cache[self.layer_id]\n",
        "                        k = torch.cat([k_, k], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
        "                        # print(v_.size(2), v.size(2))\n",
        "                        v = torch.cat([v_, v], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
        "                        cache[self.layer_id] = (k[:,:,:-1,:], v[:,:,:-1,:])\n",
        "                    else:\n",
        "                        k, v = cache[self.layer_id]\n",
        "                else: cache[self.layer_id] = (k[:,:,:-1,:], v[:,:,:-1,:])\n",
        "                torch.cuda.empty_cache()\n",
        "            else:\n",
        "                if self.layer_id in cache:\n",
        "                    if kv is None:\n",
        "                        k_, v_ = cache[self.layer_id]\n",
        "                        k = torch.cat([k_, k], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
        "                        v = torch.cat([v_, v], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
        "                    else:\n",
        "                        k, v = cache[self.layer_id]\n",
        "                cache[self.layer_id] = (k, v)\n",
        "\n",
        "        q = q / math.sqrt(dim_per_head)                                       # (bs, n_heads, qlen, dim_per_head)\n",
        "        scores = torch.matmul(q, k.transpose(2, 3))                           # (bs, n_heads, qlen, klen)\n",
        "        mask = (mask == 0).view(mask_reshape).expand_as(scores)               # (bs, n_heads, qlen, klen)\n",
        "        scores.masked_fill_(mask, -float('inf'))                              # (bs, n_heads, qlen, klen)\n",
        "\n",
        "        weights = F.softmax(scores.float(), dim=-1).type_as(scores)           # (bs, n_heads, qlen, klen)\n",
        "        weights = F.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n",
        "        context = torch.matmul(weights, v)                                    # (bs, n_heads, qlen, dim_per_head)\n",
        "        context = unshape(context)                                            # (bs, qlen, dim)\n",
        "\n",
        "        return self.out_lin(context)\n",
        "\n",
        "\n",
        "class TransformerFFN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, dim_hidden, out_dim, dropout, gelu_activation):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.lin1 = Linear(in_dim, dim_hidden)\n",
        "        self.lin2 = Linear(dim_hidden, out_dim)\n",
        "        self.act = gelu if gelu_activation else F.relu\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.lin1(input)\n",
        "        x = self.act(x)\n",
        "        x = self.lin2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    ATTRIBUTES = ['encoder', 'with_output', 'eos_index', 'pad_index', 'n_langs', 'n_words', 'dim', 'n_layers', 'n_heads', 'hidden_dim', 'dropout', 'attention_dropout', 'asm', 'asm_cutoffs', 'asm_div_value']\n",
        "\n",
        "    def __init__(self, params, dico, is_encoder, with_output):\n",
        "        \"\"\"\n",
        "        Transformer model (encoder or decoder).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # encoder / decoder, output layer\n",
        "        self.is_encoder = is_encoder\n",
        "        self.is_decoder = not is_encoder\n",
        "        self.with_output = with_output\n",
        "\n",
        "        # dictionary / languages\n",
        "        self.n_langs = params.n_langs\n",
        "        self.n_words = params.n_words\n",
        "        self.eos_index = params.eos_index\n",
        "        self.pad_index = params.pad_index\n",
        "        self.dico = dico\n",
        "        self.id2lang = params.id2lang\n",
        "        self.lang2id = params.lang2id\n",
        "        self.use_lang_emb = getattr(params, 'use_lang_emb', True)\n",
        "        assert len(self.dico) == self.n_words\n",
        "        assert len(self.id2lang) == len(self.lang2id) == self.n_langs\n",
        "\n",
        "        # model parameters\n",
        "        self.dim = params.emb_dim       # 512 by default\n",
        "        self.hidden_dim = self.dim * 4  # 2048 by default\n",
        "        self.n_heads = params.n_heads   # 8 by default\n",
        "        if is_encoder: \n",
        "            # self.n_layers = params.n_enc_layers\n",
        "            self.n_layers = getattr(params, 'n_enc_layers', params.n_layers)\n",
        "        else: self.n_layers = params.n_layers\n",
        "        self.dropout = params.dropout\n",
        "        self.attention_dropout = params.attention_dropout\n",
        "        assert self.dim % self.n_heads == 0, 'transformer dim must be a multiple of n_heads'\n",
        "\n",
        "        # embeddings\n",
        "        self.position_embeddings = Embedding(N_MAX_POSITIONS, self.dim)\n",
        "        if params.sinusoidal_embeddings:\n",
        "            create_sinusoidal_embeddings(N_MAX_POSITIONS, self.dim, out=self.position_embeddings.weight)\n",
        "        if params.n_langs > 1 and self.use_lang_emb:\n",
        "            self.lang_embeddings = Embedding(self.n_langs, self.dim)\n",
        "        self.embeddings = Embedding(self.n_words, self.dim, padding_idx=self.pad_index)\n",
        "        self.layer_norm_emb = nn.LayerNorm(self.dim, eps=1e-12)\n",
        "\n",
        "        # transformer layers\n",
        "        self.attentions = nn.ModuleList()\n",
        "        self.layer_norm1 = nn.ModuleList()\n",
        "        self.ffns = nn.ModuleList()\n",
        "        self.layer_norm2 = nn.ModuleList()\n",
        "        if self.is_decoder:\n",
        "            self.layer_norm15 = nn.ModuleList()\n",
        "            self.encoder_attn = nn.ModuleList()\n",
        "\n",
        "        # memories\n",
        "        self.memories = nn.ModuleDict()\n",
        "        if getattr(params, 'use_memory', False):\n",
        "            mem_positions = params.mem_enc_positions if is_encoder else params.mem_dec_positions\n",
        "            for layer_id, pos in mem_positions:\n",
        "                assert 0 <= layer_id <= params.n_layers - 1\n",
        "                assert pos in ['in', 'after']\n",
        "                self.memories['%i_%s' % (layer_id, pos)] = HashingMemory.build(self.dim, self.dim, params)\n",
        "\n",
        "        for layer_id in range(self.n_layers):\n",
        "            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout, tf_cls=self.__class__))\n",
        "            self.layer_norm1.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "            if self.is_decoder:\n",
        "                self.layer_norm15.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "                self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout, tf_cls=self.__class__))\n",
        "            if ('%i_in' % layer_id) in self.memories:\n",
        "                self.ffns.append(None)\n",
        "            else:\n",
        "                self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, dropout=self.dropout, gelu_activation=params.gelu_activation))\n",
        "            self.layer_norm2.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
        "\n",
        "        # output layer\n",
        "        if self.with_output:\n",
        "            self.pred_layer = PredLayer(params)\n",
        "            if params.share_inout_emb:\n",
        "                self.pred_layer.proj.weight = self.embeddings.weight\n",
        "\n",
        "    def forward(self, mode, **kwargs):\n",
        "        \"\"\"\n",
        "        Forward function with different forward modes.\n",
        "        ### Small hack to handle PyTorch distributed.\n",
        "        \"\"\"\n",
        "        if mode == 'fwd':\n",
        "            return self.fwd(**kwargs)\n",
        "        elif mode == 'predict':\n",
        "            return self.predict(**kwargs)\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode: %s\" % mode)\n",
        "\n",
        "    def fwd(self, x, lengths, causal, src_enc=None, src_len=None, positions=None, langs=None, cache=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            `x` LongTensor(slen, bs), containing word indices\n",
        "            `lengths` LongTensor(bs), containing the length of each sentence\n",
        "            `causal` Boolean, if True, the attention is only done over previous hidden states\n",
        "            `positions` LongTensor(slen, bs), containing word positions\n",
        "            `langs` LongTensor(slen, bs), containing language IDs\n",
        "        \"\"\"\n",
        "        # lengths = (x != self.pad_index).float().sum(dim=1)\n",
        "        # mask = x != self.pad_index\n",
        "\n",
        "        # check inputs\n",
        "        slen, bs = x.size()\n",
        "        assert lengths.size(0) == bs\n",
        "        assert lengths.max().item() <= slen\n",
        "        x = x.transpose(0, 1)  # batch size as dimension 0\n",
        "        assert (src_enc is None) == (src_len is None)\n",
        "        if src_enc is not None:\n",
        "            assert self.is_decoder\n",
        "            assert src_enc.size(0) == bs\n",
        "\n",
        "        # generate masks\n",
        "        mask, attn_mask = get_masks(slen, lengths, causal)\n",
        "        if self.is_decoder and src_enc is not None:\n",
        "            src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]\n",
        "\n",
        "        # positions\n",
        "        if positions is None:\n",
        "            positions = x.new(slen).long()\n",
        "            positions = torch.arange(slen, out=positions).unsqueeze(0)\n",
        "        else:\n",
        "            assert positions.size() == (slen, bs)\n",
        "            positions = positions.transpose(0, 1)\n",
        "\n",
        "        # langs\n",
        "        if langs is not None:\n",
        "            assert langs.size() == (slen, bs)\n",
        "            langs = langs.transpose(0, 1)\n",
        "\n",
        "        # do not recompute cached elements\n",
        "        if cache is not None:\n",
        "            _slen = slen - cache['slen']\n",
        "            x = x[:, -_slen:]\n",
        "            positions = positions[:, -_slen:]\n",
        "            if langs is not None:\n",
        "                langs = langs[:, -_slen:]\n",
        "            mask = mask[:, -_slen:]\n",
        "            attn_mask = attn_mask[:, -_slen:]\n",
        "\n",
        "        # embeddings\n",
        "        tensor = self.embeddings(x)\n",
        "        tensor = tensor + self.position_embeddings(positions).expand_as(tensor)\n",
        "        if langs is not None and self.use_lang_emb:\n",
        "            tensor = tensor + self.lang_embeddings(langs)\n",
        "        tensor = self.layer_norm_emb(tensor)\n",
        "        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n",
        "        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # transformer layers\n",
        "        for i in range(self.n_layers):\n",
        "\n",
        "            # self attention\n",
        "            attn = self.attentions[i](tensor, attn_mask, cache=cache)\n",
        "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "            tensor = tensor + attn\n",
        "            tensor = self.layer_norm1[i](tensor)\n",
        "\n",
        "            # encoder attention (for decoder only)\n",
        "            if self.is_decoder and src_enc is not None:\n",
        "                attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)\n",
        "                attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
        "                tensor = tensor + attn\n",
        "                tensor = self.layer_norm15[i](tensor)\n",
        "\n",
        "            # FFN\n",
        "            if ('%i_in' % i) in self.memories:\n",
        "                tensor = tensor + self.memories['%i_in' % i](tensor)\n",
        "            else:\n",
        "                tensor = tensor + self.ffns[i](tensor)\n",
        "            tensor = self.layer_norm2[i](tensor)\n",
        "\n",
        "            # memory\n",
        "            if ('%i_after' % i) in self.memories:\n",
        "                tensor = tensor + self.memories['%i_after' % i](tensor)\n",
        "            # TODO: add extra layer norm here?\n",
        "\n",
        "            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
        "\n",
        "        # update cache length\n",
        "        if cache is not None:\n",
        "            cache['slen'] += tensor.size(1)\n",
        "\n",
        "        # move back sequence length to dimension 0\n",
        "        tensor = tensor.transpose(0, 1)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def predict(self, tensor, pred_mask, y, get_scores):\n",
        "        \"\"\"\n",
        "        Given the last hidden state, compute word scores and/or the loss.\n",
        "            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n",
        "                we need to predict a word\n",
        "            `y` is a LongTensor of shape (pred_mask.sum(),)\n",
        "            `get_scores` is a boolean specifying whether we need to return scores\n",
        "        \"\"\"\n",
        "        masked_tensor = tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim)\n",
        "        scores, loss = self.pred_layer(masked_tensor, y, get_scores)\n",
        "        return scores, loss\n",
        "\n",
        "    def generate(self, src_enc, src_len, tgt_lang_id, max_len=200, sample_temperature=None, vocab_mask=None):\n",
        "        \"\"\"\n",
        "        Decode a sentence given initial start.\n",
        "        `x`:\n",
        "            - LongTensor(bs, slen)\n",
        "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
        "                <EOS> W1 W2 W3   W4  <EOS>\n",
        "        `lengths`:\n",
        "            - LongTensor(bs) [5, 6]\n",
        "        `positions`:\n",
        "            - False, for regular \"arange\" positions (LM)\n",
        "            - True, to reset positions from the new generation (MT)\n",
        "        `langs`:\n",
        "            - must be None if the model only supports one language\n",
        "            - lang_id if only one language is involved (LM)\n",
        "            - (lang_id1, lang_id2) if two languages are involved (MT)\n",
        "        \"\"\"\n",
        "\n",
        "        if vocab_mask is not None:\n",
        "            assert len(vocab_mask.size()) == 1\n",
        "            vocab_mask = 1 - vocab_mask\n",
        "            vocab_mask = vocab_mask.unsqueeze(0)\n",
        "\n",
        "        # input batch\n",
        "        bs = len(src_len)\n",
        "        assert src_enc.size(0) == bs\n",
        "\n",
        "        # generated sentences\n",
        "        generated = src_len.new(max_len, bs)  # upcoming output\n",
        "        generated.fill_(self.pad_index)       # fill upcoming ouput with <PAD>\n",
        "        generated[0].fill_(self.eos_index)    # we use <EOS> for <BOS> everywhere\n",
        "\n",
        "        # positions\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand(max_len, bs)\n",
        "\n",
        "        # language IDs\n",
        "        langs = src_len.new(max_len).long().fill_(tgt_lang_id)\n",
        "        langs = langs.unsqueeze(1).expand(max_len, bs)\n",
        "\n",
        "        # current position / max lengths / length of generated sentences / unfinished sentences\n",
        "        cur_len = 1\n",
        "        gen_len = src_len.clone().fill_(1)\n",
        "        unfinished_sents = src_len.clone().fill_(1)\n",
        "\n",
        "        # cache compute states\n",
        "        cache = {'slen': 0}\n",
        "\n",
        "        while cur_len < max_len:\n",
        "\n",
        "            # compute word scores\n",
        "            tensor = self.forward(\n",
        "                'fwd',\n",
        "                x=generated[:cur_len],\n",
        "                lengths=gen_len,\n",
        "                positions=positions[:cur_len],\n",
        "                langs=langs[:cur_len],\n",
        "                causal=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                cache=cache\n",
        "            )\n",
        "            assert tensor.size() == (1, bs, self.dim), (cur_len, max_len, src_enc.size(), tensor.size(), (1, bs, self.dim))\n",
        "            tensor = tensor.data[-1, :, :].type_as(src_enc)  # (bs, dim)\n",
        "            scores = self.pred_layer.get_scores(tensor)      # (bs, n_words)\n",
        "            \n",
        "            if vocab_mask is not None: \n",
        "                scores[vocab_mask.expand_as(scores)] = -float('inf')\n",
        "            # select next words: sample or greedy\n",
        "            if sample_temperature is None:\n",
        "                next_words = torch.topk(scores, 1)[1].squeeze(1)\n",
        "            else:\n",
        "                next_words = torch.multinomial(F.softmax(scores / sample_temperature, dim=1), 1).squeeze(1)\n",
        "            assert next_words.size() == (bs,)\n",
        "\n",
        "            # update generations / lengths / finished sentences / current length\n",
        "            generated[cur_len] = next_words * unfinished_sents + self.pad_index * (1 - unfinished_sents)\n",
        "            gen_len.add_(unfinished_sents)\n",
        "            unfinished_sents.mul_(next_words.ne(self.eos_index).long())\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
        "            if unfinished_sents.max() == 0:\n",
        "                break\n",
        "\n",
        "        # add <EOS> to unfinished sentences\n",
        "        if cur_len == max_len:\n",
        "            generated[-1].masked_fill_(unfinished_sents.byte(), self.eos_index)\n",
        "\n",
        "        # sanity check\n",
        "        assert (generated == self.eos_index).sum() == 2 * bs\n",
        "\n",
        "        return generated[:cur_len], gen_len\n",
        "\n",
        "    def generate_beam(self, src_enc, src_len, tgt_lang_id, beam_size, length_penalty, early_stopping, max_len=200, vocab_mask=None):\n",
        "        \"\"\"\n",
        "        Decode a sentence given initial start.\n",
        "        `x`:\n",
        "            - LongTensor(bs, slen)\n",
        "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
        "                <EOS> W1 W2 W3   W4  <EOS>\n",
        "        `lengths`:\n",
        "            - LongTensor(bs) [5, 6]\n",
        "        `positions`:\n",
        "            - False, for regular \"arange\" positions (LM)\n",
        "            - True, to reset positions from the new generation (MT)\n",
        "        `langs`:\n",
        "            - must be None if the model only supports one language\n",
        "            - lang_id if only one language is involved (LM)\n",
        "            - (lang_id1, lang_id2) if two languages are involved (MT)\n",
        "        \"\"\"\n",
        "\n",
        "        if vocab_mask is not None:\n",
        "            assert len(vocab_mask.size()) == 1\n",
        "            vocab_mask = 1 - vocab_mask\n",
        "            vocab_mask = vocab_mask.unsqueeze(0)\n",
        "\n",
        "        # check inputs\n",
        "        assert src_enc.size(0) == src_len.size(0)\n",
        "        assert beam_size >= 1\n",
        "\n",
        "        # batch size / number of words\n",
        "        bs = len(src_len)\n",
        "        n_words = self.n_words\n",
        "\n",
        "        # expand to beam size the source latent representations / source lengths\n",
        "        src_enc = src_enc.unsqueeze(1).expand((bs, beam_size) + src_enc.shape[1:]).contiguous().view((bs * beam_size,) + src_enc.shape[1:])\n",
        "        src_len = src_len.unsqueeze(1).expand(bs, beam_size).contiguous().view(-1)\n",
        "\n",
        "        # generated sentences (batch with beam current hypotheses)\n",
        "        generated = src_len.new(max_len, bs * beam_size)  # upcoming output\n",
        "        generated.fill_(self.pad_index)                   # fill upcoming ouput with <PAD>\n",
        "        generated[0].fill_(self.eos_index)                # we use <EOS> for <BOS> everywhere\n",
        "\n",
        "        # generated hypotheses\n",
        "        generated_hyps = [BeamHypotheses(beam_size, max_len, length_penalty, early_stopping) for _ in range(bs)]\n",
        "\n",
        "        # positions\n",
        "        positions = src_len.new(max_len).long()\n",
        "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand_as(generated)\n",
        "\n",
        "        # language IDs\n",
        "        langs = positions.clone().fill_(tgt_lang_id)\n",
        "\n",
        "        # scores for each sentence in the beam\n",
        "        beam_scores = src_enc.new(bs, beam_size).fill_(0)\n",
        "        beam_scores[:, 1:] = -1e9\n",
        "        beam_scores = beam_scores.view(-1)\n",
        "\n",
        "        # current position\n",
        "        cur_len = 1\n",
        "\n",
        "        # cache compute states\n",
        "        cache = {'slen': 0}\n",
        "\n",
        "        # done sentences\n",
        "        done = [False for _ in range(bs)]\n",
        "\n",
        "        while cur_len < max_len:\n",
        "\n",
        "            # compute word scores\n",
        "            tensor = self.forward(\n",
        "                'fwd',\n",
        "                x=generated[:cur_len],\n",
        "                lengths=src_len.new(bs * beam_size).fill_(cur_len),\n",
        "                positions=positions[:cur_len],\n",
        "                langs=langs[:cur_len],\n",
        "                causal=True,\n",
        "                src_enc=src_enc,\n",
        "                src_len=src_len,\n",
        "                cache=cache\n",
        "            )\n",
        "            assert tensor.size() == (1, bs * beam_size, self.dim)\n",
        "            tensor = tensor.data[-1, :, :]               # (bs * beam_size, dim)\n",
        "            scores = self.pred_layer.get_scores(tensor)  # (bs * beam_size, n_words)\n",
        "            if vocab_mask is not None: \n",
        "                scores[vocab_mask.expand_as(scores)] = -float('inf')\n",
        "            scores = F.log_softmax(scores, dim=-1)       # (bs * beam_size, n_words)\n",
        "            assert scores.size() == (bs * beam_size, n_words)\n",
        "\n",
        "            # select next words with scores\n",
        "            _scores = scores + beam_scores[:, None].expand_as(scores)  # (bs * beam_size, n_words)\n",
        "            _scores = _scores.view(bs, beam_size * n_words)            # (bs, beam_size * n_words)\n",
        "\n",
        "            next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)\n",
        "            assert next_scores.size() == next_words.size() == (bs, 2 * beam_size)\n",
        "\n",
        "            # next batch beam content\n",
        "            # list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)\n",
        "            next_batch_beam = []\n",
        "\n",
        "            # for each sentence\n",
        "            for sent_id in range(bs):\n",
        "\n",
        "                # if we are done with this sentence\n",
        "                done[sent_id] = done[sent_id] or generated_hyps[sent_id].is_done(next_scores[sent_id].max().item())\n",
        "                if done[sent_id]:\n",
        "                    next_batch_beam.extend([(0, self.pad_index, 0)] * beam_size)  # pad the batch\n",
        "                    continue\n",
        "\n",
        "                # next sentence beam content\n",
        "                next_sent_beam = []\n",
        "\n",
        "                # next words for this sentence\n",
        "                for idx, value in zip(next_words[sent_id], next_scores[sent_id]):\n",
        "\n",
        "                    # get beam and word IDs\n",
        "                    beam_id = idx // n_words\n",
        "                    word_id = idx % n_words\n",
        "\n",
        "                    # end of sentence, or next word\n",
        "                    if word_id == self.eos_index or cur_len + 1 == max_len:\n",
        "                        generated_hyps[sent_id].add(generated[:cur_len, sent_id * beam_size + beam_id].clone(), value.item())\n",
        "                    else:\n",
        "                        next_sent_beam.append((value, word_id, sent_id * beam_size + beam_id))\n",
        "\n",
        "                    # the beam for next step is full\n",
        "                    if len(next_sent_beam) == beam_size:\n",
        "                        break\n",
        "\n",
        "                # update next beam content\n",
        "                assert len(next_sent_beam) == 0 if cur_len + 1 == max_len else beam_size\n",
        "                if len(next_sent_beam) == 0:\n",
        "                    next_sent_beam = [(0, self.pad_index, 0)] * beam_size  # pad the batch\n",
        "                next_batch_beam.extend(next_sent_beam)\n",
        "                assert len(next_batch_beam) == beam_size * (sent_id + 1)\n",
        "\n",
        "            # sanity check / prepare next batch\n",
        "            assert len(next_batch_beam) == bs * beam_size\n",
        "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
        "            beam_words = generated.new([x[1] for x in next_batch_beam])\n",
        "            beam_idx = src_len.new([x[2] for x in next_batch_beam])\n",
        "\n",
        "            # re-order batch and internal states\n",
        "            generated = generated[:, beam_idx]\n",
        "            generated[cur_len] = beam_words\n",
        "            for k in cache.keys():\n",
        "                if k != 'slen':\n",
        "                    cache[k] = (cache[k][0][beam_idx], cache[k][1][beam_idx])\n",
        "\n",
        "            # update current length\n",
        "            cur_len = cur_len + 1\n",
        "\n",
        "            # stop when we are done with each sentence\n",
        "            if all(done):\n",
        "                break\n",
        "\n",
        "        # visualize hypotheses\n",
        "        # print([len(x) for x in generated_hyps], cur_len)\n",
        "        # globals().update( locals() );\n",
        "        # !import code; code.interact(local=vars())\n",
        "        # for ii in range(bs):\n",
        "        #     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):\n",
        "        #         print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))\n",
        "        #     print(\"\")\n",
        "\n",
        "        # select the best hypotheses\n",
        "        tgt_len = src_len.new(bs)\n",
        "        best = []\n",
        "\n",
        "        for i, hypotheses in enumerate(generated_hyps):\n",
        "            best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
        "            tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
        "            best.append(best_hyp)\n",
        "\n",
        "        # generate target batch\n",
        "        decoded = src_len.new(tgt_len.max().item(), bs).fill_(self.pad_index)\n",
        "        for i, hypo in enumerate(best):\n",
        "            decoded[:tgt_len[i] - 1, i] = hypo\n",
        "            decoded[tgt_len[i] - 1, i] = self.eos_index\n",
        "\n",
        "        # sanity check\n",
        "        assert (decoded == self.eos_index).sum() == 2 * bs\n",
        "\n",
        "        return decoded, tgt_len\n",
        "\n",
        "\n",
        "class BeamHypotheses(object):\n",
        "\n",
        "    def __init__(self, n_hyp, max_len, length_penalty, early_stopping):\n",
        "        \"\"\"\n",
        "        Initialize n-best list of hypotheses.\n",
        "        \"\"\"\n",
        "        self.max_len = max_len - 1  # ignoring <BOS>\n",
        "        self.length_penalty = length_penalty\n",
        "        self.early_stopping = early_stopping\n",
        "        self.n_hyp = n_hyp\n",
        "        self.hyp = []\n",
        "        self.worst_score = 1e9\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Number of hypotheses in the list.\n",
        "        \"\"\"\n",
        "        return len(self.hyp)\n",
        "\n",
        "    def add(self, hyp, sum_logprobs):\n",
        "        \"\"\"\n",
        "        Add a new hypothesis to the list.\n",
        "        \"\"\"\n",
        "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
        "        if len(self) < self.n_hyp or score > self.worst_score:\n",
        "            self.hyp.append((score, hyp))\n",
        "            if len(self) > self.n_hyp:\n",
        "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
        "                del self.hyp[sorted_scores[0][1]]\n",
        "                self.worst_score = sorted_scores[1][0]\n",
        "            else:\n",
        "                self.worst_score = min(score, self.worst_score)\n",
        "\n",
        "    def is_done(self, best_sum_logprobs):\n",
        "        \"\"\"\n",
        "        If there are enough hypotheses and that none of the hypotheses being generated\n",
        "        can become better than the worst one in the heap, then we are done with this sentence.\n",
        "        \"\"\"\n",
        "        if len(self) < self.n_hyp:\n",
        "            return False\n",
        "        elif self.early_stopping:\n",
        "            return True\n",
        "        else:\n",
        "            return self.worst_score >= best_sum_logprobs / self.max_len ** self.length_penalty"
      ],
      "metadata": {
        "id": "hFWGta6u9iPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import getLogger\n",
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "def load_fasttext_model(path):\n",
        "    \"\"\"\n",
        "    Load a binarized fastText model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import fastText\n",
        "    except ImportError:\n",
        "        raise Exception(\"Unable to import fastText. Please install fastText for Python: \"\n",
        "                        \"https://github.com/facebookresearch/fastText\")\n",
        "    return fastText.load_model(path)\n",
        "\n",
        "\n",
        "def read_txt_embeddings(path, params):\n",
        "    \"\"\"\n",
        "    Reload pretrained embeddings from a text file.\n",
        "    \"\"\"\n",
        "    word2id = {}\n",
        "    vectors = []\n",
        "\n",
        "    # load pretrained embeddings\n",
        "    _emb_dim_file = params.emb_dim\n",
        "    with io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i == 0:\n",
        "                split = line.split()\n",
        "                assert len(split) == 2\n",
        "                assert _emb_dim_file == int(split[1])\n",
        "                continue\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            if word in word2id:\n",
        "                logger.warning(\"Word \\\"%s\\\" found twice!\" % word)\n",
        "                continue\n",
        "            if not vect.shape == (_emb_dim_file,):\n",
        "                logger.warning(\"Invalid dimension (%i) for word \\\"%s\\\" in line %i.\"\n",
        "                               % (vect.shape[0], word, i))\n",
        "                continue\n",
        "            assert vect.shape == (_emb_dim_file,)\n",
        "            word2id[word] = len(word2id)\n",
        "            vectors.append(vect[None])\n",
        "\n",
        "    assert len(word2id) == len(vectors)\n",
        "    logger.info(\"Loaded %i pretrained word embeddings from %s\" % (len(vectors), path))\n",
        "\n",
        "    # compute new vocabulary / embeddings\n",
        "    embeddings = np.concatenate(vectors, 0)\n",
        "    embeddings = torch.from_numpy(embeddings).float()\n",
        "\n",
        "    assert embeddings.size() == (len(word2id), params.emb_dim)\n",
        "    return word2id, embeddings\n",
        "\n",
        "\n",
        "def load_bin_embeddings(path, params):\n",
        "    \"\"\"\n",
        "    Reload pretrained embeddings from a fastText binary file.\n",
        "    \"\"\"\n",
        "    model = load_fasttext_model(path)\n",
        "    assert model.get_dimension() == params.emb_dim\n",
        "    words = model.get_labels()\n",
        "    logger.info(\"Loaded binary model from %s\" % path)\n",
        "\n",
        "    # compute new vocabulary / embeddings\n",
        "    embeddings = np.concatenate([model.get_word_vector(w)[None] for w in words], 0)\n",
        "    embeddings = torch.from_numpy(embeddings).float()\n",
        "    word2id = {w: i for i, w in enumerate(words)}\n",
        "    logger.info(\"Generated embeddings for %i words.\" % len(words))\n",
        "\n",
        "    assert embeddings.size() == (len(word2id), params.emb_dim)\n",
        "    return word2id, embeddings\n",
        "\n",
        "\n",
        "def load_embeddings(path, params):\n",
        "    \"\"\"\n",
        "    Reload pretrained embeddings.\n",
        "    \"\"\"\n",
        "    if path.endswith('.bin'):\n",
        "        return load_bin_embeddings(path, params)\n",
        "    else:\n",
        "        return read_txt_embeddings(path, params)"
      ],
      "metadata": {
        "id": "kCsq60t-9s1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import getLogger\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "\n",
        "logger = getLogger()\n",
        "\n",
        "\n",
        "class SentenceEmbedder(object):\n",
        "\n",
        "    @staticmethod\n",
        "    def reload(path, params, cls_name=TransformerModel):\n",
        "        \"\"\"\n",
        "        Create a sentence embedder from a pretrained model.\n",
        "        \"\"\"\n",
        "        # reload model\n",
        "        reloaded = torch.load(path)\n",
        "        state_dict = reloaded['model']\n",
        "\n",
        "        # handle models from multi-GPU checkpoints\n",
        "        if 'checkpoint' in path:\n",
        "            state_dict = {(k[7:] if k.startswith('module.') else k): v for k, v in state_dict.items()}\n",
        "\n",
        "        # reload dictionary and model parameters\n",
        "        dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
        "        pretrain_params = AttrDict(reloaded['params'])\n",
        "        pretrain_params.n_words = len(dico)\n",
        "        pretrain_params.bos_index = dico.index(BOS_WORD)\n",
        "        pretrain_params.eos_index = dico.index(EOS_WORD)\n",
        "        pretrain_params.pad_index = dico.index(PAD_WORD)\n",
        "        pretrain_params.unk_index = dico.index(UNK_WORD)\n",
        "        pretrain_params.mask_index = dico.index(MASK_WORD)\n",
        "\n",
        "        # if \"n_nlu_layers\" in params: \n",
        "        #     pretrain_params.n_nlu_layers = params.n_nlu_layers\n",
        "        # if \"n_task_layers\" in params: \n",
        "        #     pretrain_params.n_task_layers = params.n_task_layers\n",
        "        # if \"n_lang_layers\" in params: \n",
        "        #     pretrain_params.n_lang_layers = params.n_lang_layers\n",
        "        \n",
        "        # TODO config n layers to load\n",
        "\n",
        "        # build model and reload weights\n",
        "        model = cls_name(pretrain_params, dico, True, True, params.use_task_emb)\n",
        "        # model = cls_name(params, dico, True, True, params.use_task_emb)\n",
        "        # NOTE task embedding is not included in the Facebook XLM15\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "        model.eval()\n",
        "\n",
        "        # adding missing parameters\n",
        "        params.max_batch_size = 0\n",
        "\n",
        "        return SentenceEmbedder(model, dico, pretrain_params)\n",
        "        # return SentenceEmbedder(model, dico, params)\n",
        "\n",
        "    def __init__(self, model, dico, pretrain_params):\n",
        "        \"\"\"\n",
        "        Wrapper on top of the different sentence embedders.\n",
        "        Returns sequence-wise or single-vector sentence representations.\n",
        "        \"\"\"\n",
        "        self.pretrain_params = {k: v for k, v in pretrain_params.__dict__.items()}\n",
        "        self.model = model\n",
        "        self.dico = dico\n",
        "        self.n_layers = model.n_layers\n",
        "        self.out_dim = model.dim\n",
        "        self.n_words = model.n_words\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n",
        "\n",
        "    def cuda(self):\n",
        "        self.model.cuda()\n",
        "    \n",
        "    def parallel(self, params):\n",
        "        self.model =  nn.parallel.DistributedDataParallel(\n",
        "            self.model, device_ids=[params.local_rank],\n",
        "            output_device=params.local_rank, broadcast_buffers=False)\n",
        "\n",
        "    def get_parameters(self, params):\n",
        "\n",
        "        layer_range = params.finetune_layers\n",
        "\n",
        "        s = layer_range.split(':')\n",
        "        assert len(s) == 2\n",
        "        i, j = int(s[0].replace('_', '-')), int(s[1].replace('_', '-'))\n",
        "\n",
        "        # negative indexing\n",
        "        i = self.n_layers + i + 1 if i < 0 else i\n",
        "        j = self.n_layers + j + 1 if j < 0 else j\n",
        "\n",
        "        # sanity check\n",
        "        assert 0 <= i <= self.n_layers\n",
        "        assert 0 <= j <= self.n_layers\n",
        "\n",
        "        if i > j:\n",
        "            return []\n",
        "\n",
        "        parameters = []\n",
        "\n",
        "        # embeddings\n",
        "        if i == 0:\n",
        "            # embeddings\n",
        "            if not params.fixed_embeddings:\n",
        "                parameters += self.model.embeddings.parameters()\n",
        "                logger.info(\"Adding embedding parameters to optimizer\")\n",
        "            # positional embeddings\n",
        "            if self.pretrain_params['sinusoidal_embeddings'] is False \\\n",
        "                and not params.fixed_position_embeddings:\n",
        "                parameters += self.model.position_embeddings.parameters()\n",
        "                logger.info(\"Adding positional embedding parameters to optimizer\")\n",
        "            # language embeddings\n",
        "            if hasattr(self.model, 'lang_embeddings') and \\\n",
        "                not params.fixed_lang_embeddings:\n",
        "                parameters += self.model.lang_embeddings.parameters()\n",
        "                logger.info(\"Adding language embedding parameters to optimizer\")\n",
        "            # task embeddings\n",
        "            if hasattr(self.model, \"task_embeddings\") and \\\n",
        "                not params.fixed_task_embeddings:\n",
        "                parameters += self.model.task_embeddings.parameters()\n",
        "                logger.info(\"Adding task embedding parameters to optimizer\")\n",
        "            parameters += self.model.layer_norm_emb.parameters()\n",
        "        # layers\n",
        "        for l in range(max(i - 1, 0), j):\n",
        "            parameters += self.model.attentions[l].parameters()\n",
        "            parameters += self.model.layer_norm1[l].parameters()\n",
        "            parameters += self.model.ffns[l].parameters()\n",
        "            parameters += self.model.layer_norm2[l].parameters()\n",
        "            logger.info(\"Adding layer-%s parameters to optimizer\" % (l + 1))\n",
        "\n",
        "        logger.info(\"Optimizing on %i Transformer elements.\" % sum([p.nelement() for p in parameters]))\n",
        "\n",
        "        return parameters\n",
        "\n",
        "    def get_embeddings(self, x, lengths, positions=None, langs=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            `x`        : LongTensor of shape (slen, bs)\n",
        "            `lengths`  : LongTensor of shape (bs,)\n",
        "        Outputs:\n",
        "            `sent_emb` : FloatTensor of shape (bs, out_dim)\n",
        "        With out_dim == emb_dim\n",
        "        \"\"\"\n",
        "        slen, bs = x.size()\n",
        "        assert lengths.size(0) == bs and lengths.max().item() == slen\n",
        "\n",
        "        # get transformer last hidden layer\n",
        "        tensor = self.model('fwd', x=x, lengths=lengths, positions=positions, langs=langs, causal=False)\n",
        "        assert tensor.size() == (slen, bs, self.out_dim)\n",
        "\n",
        "        # single-vector sentence representation (first column of last layer)\n",
        "        return tensor[0]"
      ],
      "metadata": {
        "id": "MI0RcqMMzPIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.argv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f_ZIXCFDGkC",
        "outputId": "5a049e6d-5866-47f0-a42a-a8ea3d9eda00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py',\n",
              " '-f',\n",
              " '/root/.local/share/jupyter/runtime/kernel-44b48dc6-ae97-40e9-b586-c786c1e953bd.json']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile xnlg-ft.py\n",
        "\n",
        "import os\n",
        "import io\n",
        "import argparse\n",
        "import torch\n",
        "import copy\n",
        "import sys\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_params():\n",
        "\n",
        "  # parse parameters\n",
        "  parser = argparse.ArgumentParser(description='Train on XNLG')\n",
        "\n",
        "  # main parameters\n",
        "  parser.add_argument(\"--exp_name\", type=str, default=\"\",\n",
        "                      help=\"Experiment name\")\n",
        "  parser.add_argument(\"--dump_path\", type=str, default=\"\",\n",
        "                      help=\"Experiment dump path\")\n",
        "  parser.add_argument(\"--exp_id\", type=str, default=\"\",\n",
        "                      help=\"Experiment ID\")\n",
        "\n",
        "  parser.add_argument(\"--model_path\", type=str, default=\"\",\n",
        "                      help=\"Model location\")\n",
        "\n",
        "  # data\n",
        "  parser.add_argument(\"--data_path\", type=str, default=\"\",\n",
        "                      help=\"Data path\")\n",
        "  parser.add_argument(\"--ds_name\", type=str, default=\"xpersona\",\n",
        "                      help=\"name of dataset: xsumm or xgiga\")\n",
        "  parser.add_argument(\"--max_vocab\", type=int, default=-1,\n",
        "                      help=\"Maximum vocabulary size (-1 to disable)\")\n",
        "  parser.add_argument(\"--min_count\", type=int, default=0,\n",
        "                      help=\"Minimum vocabulary count\")\n",
        "\n",
        "  # batch parameters\n",
        "  parser.add_argument(\"--max_len\", type=int, default=256,\n",
        "                      help=\"Maximum length of sentences (after BPE)\")\n",
        "  parser.add_argument(\"--max_len_q\", type=int, default=256,\n",
        "                    help=\"Maximum length of sentences (after BPE)\")\n",
        "  parser.add_argument(\"--max_len_a\", type=int, default=256,\n",
        "                    help=\"Maximum length of sentences (after BPE)\")\n",
        "  parser.add_argument(\"--max_len_e\", type=int, default=256,\n",
        "                    help=\"Maximum length of sentences (after BPE)\")\n",
        "  parser.add_argument(\"--group_by_size\", type=bool_flag, default=False,\n",
        "                      help=\"Sort sentences by size during the training\")\n",
        "  parser.add_argument(\"--batch_size\", type=int, default=32,\n",
        "                      help=\"Number of sentences per batch\")\n",
        "  parser.add_argument(\"--max_batch_size\", type=int, default=0,\n",
        "                      help=\"Maximum number of sentences per batch (used in combination with tokens_per_batch, 0 to disable)\")\n",
        "  parser.add_argument(\"--tokens_per_batch\", type=int, default=-1,\n",
        "                      help=\"Number of tokens per batch\")\n",
        "\n",
        "  # model / optimization\n",
        "  parser.add_argument(\"--finetune_layers\", type=str, default='0:_1',\n",
        "                      help=\"Layers to finetune. 0 = embeddings, _1 = last encoder layer\")\n",
        "  parser.add_argument(\"--weighted_training\", type=bool_flag, default=False,\n",
        "                      help=\"Use a weighted loss during training\")\n",
        "  parser.add_argument(\"--dropout\", type=float, default=0,\n",
        "                      help=\"Fine-tuning dropout\")\n",
        "  parser.add_argument(\"--optimizer_e\", type=str, default=\"adam,lr=0.0001\",\n",
        "                      help=\"Embedder (pretrained model) optimizer\")\n",
        "  parser.add_argument(\"--optimizer_p\", type=str, default=\"adam,lr=0.0001\",\n",
        "                      help=\"Projection (classifier) optimizer\")\n",
        "  parser.add_argument(\"--optimizer\", type=str, default=\"adam,lr=0.0001\",\n",
        "                      help=\"Projection (classifier) optimizer\")                    \n",
        "  parser.add_argument(\"--n_epochs\", type=int, default=100,\n",
        "                      help=\"Maximum number of epochs\")\n",
        "  parser.add_argument(\"--epoch_size\", type=int, default=-1,\n",
        "                      help=\"Epoch size (-1 for full pass over the dataset)\")\n",
        "\n",
        "  # debug\n",
        "  parser.add_argument(\"--debug_train\", type=bool_flag, default=False,\n",
        "                      help=\"Use valid sets for train sets (faster loading)\")\n",
        "  parser.add_argument(\"--debug_slurm\", type=bool_flag, default=False,\n",
        "                      help=\"Debug multi-GPU / multi-node within a SLURM job\")\n",
        "  parser.add_argument(\"--sample_alpha\", type=float, default=0,\n",
        "                      help=\"Exponent for transforming word counts to probabilities (~word2vec sampling)\")\n",
        "  parser.add_argument(\"--word_pred\", type=float, default=0.15,\n",
        "                      help=\"Fraction of words for which we need to make a prediction\")\n",
        "\n",
        "  parser.add_argument(\"--max_dec_len\", type=int, default=80,\n",
        "                      help=\"Maximum length of target sentence (after BPE)\")\n",
        "\n",
        "  # decode with vocab\n",
        "\n",
        "  parser.add_argument(\"--decode_with_vocab\", type=bool_flag, default=False,\n",
        "                      help=\"Decode with vocab\")\n",
        "  parser.add_argument(\"--decode_vocab_sizes\", type=str, default=\"26000,20000\",\n",
        "                      help=\"decode_vocab_sizes\")\n",
        "  parser.add_argument(\"--vocab_path\", type=str, default=\"\",\n",
        "                      help=\"vocab_path\")\n",
        "\n",
        "  # multi-gpu\n",
        "  parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                      help=\"Multi-GPU - Local rank\")\n",
        "  parser.add_argument(\"--multi_gpu\", type=bool_flag, default=False,\n",
        "                      help=\"multi-gpu\")\n",
        "\n",
        "  parser.add_argument(\"--train_layers\", type=str, default=\"\",\n",
        "                      help=\"train layers of encoder\") \n",
        "  parser.add_argument(\"--n_enc_layers\", type=int, default=0,\n",
        "                      help=\"\") \n",
        "  parser.add_argument(\"--n_dec_layers\", type=int, default=0,\n",
        "                      help=\"\") \n",
        "  parser.add_argument(\"--fixed_embeddings\", type=bool_flag, default=False,\n",
        "                    help=\"fixed_embeddings\")\n",
        "  parser.add_argument(\"--fixed_position_embeddings\", type=bool_flag, default=False,\n",
        "                      help=\"fixed_position_embeddings\")\n",
        "  parser.add_argument(\"--fixed_lang_embeddings\", type=bool_flag, default=False,\n",
        "                      help=\"fixed_lang_embeddings\")\n",
        "  parser.add_argument(\"--fixed_task_embeddings\", type=bool_flag, default=False,\n",
        "                      help=\"fixed_task_embeddings\")\n",
        "  parser.add_argument(\"--beam_size\", type=int, default=1,\n",
        "                      help=\"\")\n",
        "  parser.add_argument(\"--no_init\", type=str, default=\"None\",\n",
        "                      help=\"dont init with pretrained models\")\n",
        "  \n",
        "  parser.add_argument(\"--train_directions\", type=str, default=\"en-en\",\n",
        "                      help=\"\")\n",
        "  parser.add_argument(\"--eval_directions\", type=str, default=\"\",\n",
        "                      help=\"\")\n",
        "  parser.add_argument(\"--emb_dim\", type=int, default=-1,\n",
        "                      help=\"Number of sentences per batch\")\n",
        "  parser.add_argument(\"--reload_emb\", type=str, default=\"\",\n",
        "                      help=\"path to .vec produced by fasttext\")\n",
        "  parser.add_argument(\"--cut_dataset\", type=int, default=-1,\n",
        "                      help=\"Number of sentences in dataset. -1 for full dataset.\")\n",
        "  \n",
        "  parser.add_argument(\"--device1\", type=int, default=0, help=\"device id for the encoder\")\n",
        "  parser.add_argument(\"--device2\", type=int, default=0, help=\"device id for the decoder\")\n",
        "  \n",
        "  params = parser.parse_args()\n",
        "\n",
        "  return params\n",
        "\n",
        "\n",
        "def read_txt_embeddings(logger, path):\n",
        "  \"\"\"\n",
        "  Reload pretrained embeddings from a text file.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  word2id = {}\n",
        "  vectors = []\n",
        "\n",
        "  # load pretrained embeddings\n",
        "  # _emb_dim_file = params.emb_dim\n",
        "  _emb_dim_file = 0\n",
        "  with io.open(path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
        "    for i, line in enumerate(f):\n",
        "      if i == 0:\n",
        "        split = line.split()\n",
        "        assert len(split) == 2\n",
        "        _emb_dim_file = int(split[1])\n",
        "        continue\n",
        "      word, vect = line.rstrip().split(' ', 1)\n",
        "      vect = np.fromstring(vect, sep=' ')\n",
        "      if word in word2id:\n",
        "        logger.warning(\"Word \\\"%s\\\" found twice!\" % word)\n",
        "        continue\n",
        "      if not vect.shape == (_emb_dim_file,):\n",
        "        logger.warning(\"Invalid dimension (%i) for word \\\"%s\\\" in line %i.\"\n",
        "                        % (vect.shape[0], word, i))\n",
        "        continue\n",
        "      assert vect.shape == (_emb_dim_file,)\n",
        "      word2id[word] = len(word2id)\n",
        "      vectors.append(vect[None])\n",
        "\n",
        "  assert len(word2id) == len(vectors)\n",
        "  logger.info(\"Loaded %i pretrained word embeddings from %s\" % (len(vectors), path))\n",
        "\n",
        "  # compute new vocabulary / embeddings\n",
        "  embeddings = np.concatenate(vectors, 0)\n",
        "  embeddings = torch.from_numpy(embeddings).float()\n",
        "\n",
        "  # assert embeddings.size() == (len(word2id), params.emb_dim)\n",
        "  return word2id, embeddings\n",
        "\n",
        "\n",
        "def load_bin_embeddings(logger, path):\n",
        "  \"\"\"\n",
        "  Reload pretrained embeddings from a fastText binary file.\n",
        "  \"\"\"\n",
        "  import fasttext\n",
        "  import numpy as np\n",
        "  model = fasttext.load_model(path)\n",
        "  words = model.get_labels()\n",
        "  logger.info(\"Loaded binary model from %s\" % path)\n",
        "\n",
        "  # compute new vocabulary / embeddings\n",
        "  embeddings = np.concatenate([model.get_word_vector(w)[None] for w in words], 0)\n",
        "  embeddings = torch.from_numpy(embeddings).float()\n",
        "  word2id = {w: i for i, w in enumerate(words)}\n",
        "  logger.info(\"Generated embeddings for %i words.\" % len(words))\n",
        "\n",
        "  return word2id, embeddings\n",
        "\n",
        "\n",
        "def set_pretrain_emb(logger, model, dico, word2id, embeddings):\n",
        "  \"\"\"\n",
        "  Pretrain word embeddings.\n",
        "  \"\"\"\n",
        "  n_found = 0\n",
        "  with torch.no_grad():\n",
        "    for i in range(len(dico)):\n",
        "      idx = word2id.get(dico[i], None)\n",
        "      if idx is None:\n",
        "        continue\n",
        "      n_found += 1\n",
        "      model.embeddings.weight[i] = embeddings[idx].cuda()\n",
        "      try:\n",
        "        model.pred_layer.proj.weight[i] = embeddings[idx].cuda()\n",
        "      except AttributeError:\n",
        "        pass\n",
        "  logger.info(\"Pretrained %i/%i words (%.3f%%).\"\n",
        "              % (n_found, len(dico), 100. * n_found / len(dico)))\n",
        "\n",
        "\n",
        "def str_to_class(str):\n",
        "  return getattr(sys.modules[__name__], str)\n",
        "\n",
        "\n",
        "def run_xnlg():\n",
        "  params = get_params()\n",
        "\n",
        "  # initialize the experiment / build sentence embedder\n",
        "  logger = initialize_exp(params)\n",
        "\n",
        "  if params.tokens_per_batch > -1:\n",
        "    params.group_by_size = True\n",
        "\n",
        "  # check parameters\n",
        "  assert os.path.isdir(params.data_path)\n",
        "  assert os.path.isfile(params.model_path)\n",
        "\n",
        "  reloaded = torch.load(params.model_path)\n",
        "  model_params = AttrDict(reloaded['params'])\n",
        "  logger.info(\n",
        "    \"Supported languages: %s\" % \", \".join(model_params.lang2id.keys()))\n",
        "  params.n_langs = model_params['n_langs']\n",
        "  params.id2lang = model_params['id2lang']\n",
        "  params.lang2id = model_params['lang2id']\n",
        "\n",
        "  \n",
        "  if \"enc_params\" in reloaded:\n",
        "    encoder_model_params = AttrDict(reloaded[\"enc_params\"])\n",
        "  elif params.n_enc_layers == model_params.n_layers or params.n_enc_layers == 0:\n",
        "    encoder_model_params = model_params\n",
        "  else:\n",
        "    encoder_model_params = AttrDict(reloaded['params'])\n",
        "    encoder_model_params.n_layers = params.n_enc_layers\n",
        "    assert model_params.n_layers is not encoder_model_params.n_layers\n",
        "  \n",
        "  if \"dec_params\" in reloaded:\n",
        "    decoder_model_params = AttrDict(reloaded[\"dec_params\"])\n",
        "  elif params.n_dec_layers == model_params.n_layers or params.n_dec_layers == 0:\n",
        "    decoder_model_params = model_params\n",
        "  else:\n",
        "    decoder_model_params = AttrDict(reloaded['params'])\n",
        "    decoder_model_params.n_layers = params.n_dec_layers\n",
        "    assert model_params.n_layers is not decoder_model_params.n_layers\n",
        "  \n",
        "  params.encoder_model_params = encoder_model_params\n",
        "  params.decoder_model_params = decoder_model_params\n",
        "\n",
        "  if params.emb_dim != -1:\n",
        "    encoder_model_params.emb_dim = params.emb_dim\n",
        "    decoder_model_params.emb_dim = params.emb_dim\n",
        "  \n",
        "  # build dictionary / build encoder / build decoder / reload weights\n",
        "  dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
        "\n",
        "  for p in [params, encoder_model_params, decoder_model_params]:\n",
        "    p.n_words = len(dico)\n",
        "    p.bos_index = dico.index(BOS_WORD)\n",
        "    p.eos_index = dico.index(EOS_WORD)\n",
        "    p.pad_index = dico.index(PAD_WORD)\n",
        "    p.unk_index = dico.index(UNK_WORD)\n",
        "    p.mask_index = dico.index(MASK_WORD)\n",
        "\n",
        "  encoder = TransformerModel(encoder_model_params, dico, is_encoder=True, with_output=False)\n",
        "  decoder = TransformerModel(decoder_model_params, dico, is_encoder=False, with_output=True)\n",
        "\n",
        "  def _process_state_dict(state_dict):\n",
        "    return {(k[7:] if k.startswith('module.') else k): v for k, v in state_dict.items()}\n",
        "\n",
        "  if params.no_init == \"all\":\n",
        "    logger.info(\"All Models will not load state dict.!!!\")\n",
        "  elif params.reload_emb != \"\":\n",
        "    logger.info(\"Reloading embedding from %s ...\" % params.reload_emb)\n",
        "    word2id, embeddings = read_txt_embeddings(logger, params.reload_emb)\n",
        "    set_pretrain_emb(logger, encoder, dico, word2id, embeddings)\n",
        "    set_pretrain_emb(logger, decoder, dico, word2id, embeddings)\n",
        "  else:\n",
        "    if \"model\" in reloaded:\n",
        "      if params.no_init != \"encoder\":\n",
        "        encoder.load_state_dict(_process_state_dict(reloaded['model']), strict=False)\n",
        "      if params.no_init != \"decoder\":\n",
        "        decoder.load_state_dict(_process_state_dict(reloaded['model']), strict=False)\n",
        "    else:\n",
        "      if params.no_init != \"encoder\":\n",
        "        encoder.load_state_dict(_process_state_dict(reloaded['encoder']), strict=False)\n",
        "      if params.no_init != \"decoder\":\n",
        "        decoder.load_state_dict(_process_state_dict(reloaded['decoder']))\n",
        "  \n",
        "  scores = {}\n",
        "\n",
        "  XPersona(encoder, decoder, scores, dico, params).run()\n",
        "if __name__ == \"__main__\":\n",
        "  run_xnlg()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5nkXuCoxdVm",
        "outputId": "40b82f64-5bac-4cf8-e498-8258dc0672d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting xnlg-ft.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "TB3fbwauRi1w",
        "outputId": "85ad5c02-4f48-4b6e-e90e-6e26360c3948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9d509663a8c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_xnlg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'run_xnlg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python xnlg-ft.py --exp_name xpersona --exp_id ftOnZh --optimizer adam,lr=0.00001 --batch_size 8 --n_epochs 200 --epoch_size 3000 --max_len 120 --max_vocab 95000 --train_layers 1,10 --decode_with_vocab False --n_enc_layers 10 --n_dec_layers 6 --ds_name xpersona"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLO9Hnmf-FYs",
        "outputId": "d9237929-d016-4b8f-ea27-bcc65ad7c32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Traceback (most recent call last):\n",
            "  File \"xnlg-ft.py\", line 315, in <module>\n",
            "    run_xnlg()\n",
            "  File \"xnlg-ft.py\", line 229, in run_xnlg\n",
            "    params = get_params()\n",
            "  File \"xnlg-ft.py\", line 50, in get_params\n",
            "    parser.add_argument(\"--group_by_size\", type=bool_flag, default=False,\n",
            "NameError: name 'bool_flag' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python xnlg-ft.py --exp_name xpersona --exp_id ftOnZh --dump_path ./dump --model_path ./data/pretrained_XNLG/en-zh_valid-en-zh.pth --data_path ./data/processed/XNLG --optimizer adam,lr=0.00001 --batch_size 8 --n_epochs 200 --epoch_size 3000 --max_len 120 --max_vocab 95000 --train_layers 1,10 --decode_with_vocab False --n_enc_layers 10 --n_dec_layers 6 --ds_name xpersona --train_directions en-en --eval_directions zh-zh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhDBBgHWSpfo",
        "outputId": "40476496-61c4-4934-a191-fe5a1073a04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Traceback (most recent call last):\n",
            "  File \"xnlg-ft.py\", line 315, in <module>\n",
            "    run_xnlg()\n",
            "  File \"xnlg-ft.py\", line 229, in run_xnlg\n",
            "    params = get_params()\n",
            "  File \"xnlg-ft.py\", line 50, in get_params\n",
            "    parser.add_argument(\"--group_by_size\", type=bool_flag, default=False,\n",
            "NameError: name 'bool_flag' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.py\n",
        "import os\n",
        "import io\n",
        "import argparse\n",
        "import torch\n",
        "import copy\n",
        "import sys\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_params():\n",
        "\n",
        "    # parse parameters\n",
        "    parser = argparse.ArgumentParser(description='Train on XNLG')\n",
        "\n",
        "    # main parameters\n",
        "    parser.add_argument(\"--exp_name\", type=str, default=\"\",\n",
        "                        help=\"Experiment name\")\n",
        "    parser.add_argument(\"--dump_path\", type=str, default=\"\",\n",
        "                        help=\"Experiment dump path\")\n",
        "    parser.add_argument(\"--exp_id\", type=str, default=\"\",\n",
        "                        help=\"Experiment ID\")\n",
        "\n",
        "    parser.add_argument(\"--model_path\", type=str, default=\"\",\n",
        "                        help=\"Model location\")\n",
        "    parser.add_argument(\"--saved_path\", type=str, default=\"\",\n",
        "                        help=\"saved location\")\n",
        "\n",
        "    # data\n",
        "    parser.add_argument(\"--data_path\", type=str, default=\"\",\n",
        "                        help=\"Data path\")\n",
        "    parser.add_argument(\"--ds_name\", type=str, default=\"xpersona\",\n",
        "                        help=\"name of dataset: xsumm or xgiga\")\n",
        "    parser.add_argument(\"--max_vocab\", type=int, default=-1,\n",
        "                        help=\"Maximum vocabulary size (-1 to disable)\")\n",
        "    parser.add_argument(\"--min_count\", type=int, default=0,\n",
        "                        help=\"Minimum vocabulary count\")\n",
        "\n",
        "    # batch parameters\n",
        "    parser.add_argument(\"--max_len\", type=int, default=256,\n",
        "                        help=\"Maximum length of sentences (after BPE)\")\n",
        "    parser.add_argument(\"--max_len_q\", type=int, default=256,\n",
        "                        help=\"Maximum length of sentences (after BPE)\")\n",
        "    parser.add_argument(\"--max_len_a\", type=int, default=256,\n",
        "                        help=\"Maximum length of sentences (after BPE)\")\n",
        "    parser.add_argument(\"--max_len_e\", type=int, default=256,\n",
        "                        help=\"Maximum length of sentences (after BPE)\")\n",
        "    parser.add_argument(\"--group_by_size\", type=bool_flag, default=False,\n",
        "                        help=\"Sort sentences by size during the training\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32,\n",
        "                        help=\"Number of sentences per batch\")\n",
        "    parser.add_argument(\"--max_batch_size\", type=int, default=0,\n",
        "                        help=\"Maximum number of sentences per batch (used in combination with tokens_per_batch, 0 to disable)\")\n",
        "    parser.add_argument(\"--tokens_per_batch\", type=int, default=-1,\n",
        "                        help=\"Number of tokens per batch\")\n",
        "\n",
        "    # model / optimization\n",
        "    parser.add_argument(\"--finetune_layers\", type=str, default='0:_1',\n",
        "                        help=\"Layers to finetune. 0 = embeddings, _1 = last encoder layer\")\n",
        "    parser.add_argument(\"--weighted_training\", type=bool_flag, default=False,\n",
        "                        help=\"Use a weighted loss during training\")\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0,\n",
        "                        help=\"Fine-tuning dropout\")\n",
        "    parser.add_argument(\"--optimizer_e\", type=str, default=\"adam,lr=0.0001\",\n",
        "                        help=\"Embedder (pretrained model) optimizer\")\n",
        "    parser.add_argument(\"--optimizer_p\", type=str, default=\"adam,lr=0.0001\",\n",
        "                        help=\"Projection (classifier) optimizer\")\n",
        "    parser.add_argument(\"--optimizer\", type=str, default=\"adam,lr=0.0001\",\n",
        "                        help=\"Projection (classifier) optimizer\")                    \n",
        "    parser.add_argument(\"--n_epochs\", type=int, default=100,\n",
        "                        help=\"Maximum number of epochs\")\n",
        "    parser.add_argument(\"--epoch_size\", type=int, default=-1,\n",
        "                        help=\"Epoch size (-1 for full pass over the dataset)\")\n",
        "\n",
        "    # debug\n",
        "    parser.add_argument(\"--debug_train\", type=bool_flag, default=False,\n",
        "                        help=\"Use valid sets for train sets (faster loading)\")\n",
        "    parser.add_argument(\"--debug_slurm\", type=bool_flag, default=False,\n",
        "                        help=\"Debug multi-GPU / multi-node within a SLURM job\")\n",
        "    parser.add_argument(\"--sample_alpha\", type=float, default=0,\n",
        "                        help=\"Exponent for transforming word counts to probabilities (~word2vec sampling)\")\n",
        "    parser.add_argument(\"--word_pred\", type=float, default=0.15,\n",
        "                        help=\"Fraction of words for which we need to make a prediction\")\n",
        "\n",
        "    parser.add_argument(\"--max_dec_len\", type=int, default=80,\n",
        "                        help=\"Maximum length of target sentence (after BPE)\")\n",
        "\n",
        "    # decode with vocab\n",
        "\n",
        "    parser.add_argument(\"--decode_with_vocab\", type=bool_flag, default=False,\n",
        "                        help=\"Decode with vocab\")\n",
        "    parser.add_argument(\"--decode_vocab_sizes\", type=str, default=\"26000,20000\",\n",
        "                        help=\"decode_vocab_sizes\")\n",
        "    parser.add_argument(\"--vocab_path\", type=str, default=\"\",\n",
        "                        help=\"vocab_path\")\n",
        "\n",
        "    # multi-gpu\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "                        help=\"Multi-GPU - Local rank\")\n",
        "    parser.add_argument(\"--multi_gpu\", type=bool_flag, default=False,\n",
        "                        help=\"multi-gpu\")\n",
        "\n",
        "    parser.add_argument(\"--train_layers\", type=str, default=\"\",\n",
        "                        help=\"train layers of encoder\") \n",
        "    parser.add_argument(\"--n_enc_layers\", type=int, default=0,\n",
        "                        help=\"\") \n",
        "    parser.add_argument(\"--n_dec_layers\", type=int, default=0,\n",
        "                        help=\"\") \n",
        "    parser.add_argument(\"--fixed_embeddings\", type=bool_flag, default=False,\n",
        "                        help=\"fixed_embeddings\")\n",
        "    parser.add_argument(\"--fixed_position_embeddings\", type=bool_flag, default=False,\n",
        "                        help=\"fixed_position_embeddings\")\n",
        "    parser.add_argument(\"--fixed_lang_embeddings\", type=bool_flag, default=False,\n",
        "                        help=\"fixed_lang_embeddings\")\n",
        "    parser.add_argument(\"--fixed_task_embeddings\", type=bool_flag, default=False,\n",
        "                        help=\"fixed_task_embeddings\")\n",
        "    parser.add_argument(\"--beam_size\", type=int, default=1,\n",
        "                        help=\"\")\n",
        "    parser.add_argument(\"--no_init\", type=str, default=\"None\",\n",
        "                        help=\"dont init with pretrained models\")\n",
        "    \n",
        "    parser.add_argument(\"--train_directions\", type=str, default=\"en-en\",\n",
        "                        help=\"\")\n",
        "    parser.add_argument(\"--eval_directions\", type=str, default=\"\",\n",
        "                        help=\"\")\n",
        "    parser.add_argument(\"--emb_dim\", type=int, default=-1,\n",
        "                        help=\"Number of sentences per batch\")\n",
        "    parser.add_argument(\"--reload_emb\", type=str, default=\"\",\n",
        "                        help=\"path to .vec produced by fasttext\")\n",
        "    parser.add_argument(\"--cut_dataset\", type=int, default=-1,\n",
        "                        help=\"Number of sentences in dataset. -1 for full dataset.\")\n",
        "    \n",
        "    parser.add_argument(\"--device1\", type=int, default=3, help=\"device id for the encoder\")\n",
        "    parser.add_argument(\"--device2\", type=int, default=4, help=\"device id for the decoder\")\n",
        "\n",
        "    params = parser.parse_args()\n",
        "\n",
        "    return params\n",
        "\n",
        "\n",
        "def run_test():\n",
        "    params = get_params()\n",
        "\n",
        "    # initialize the experiment / build sentence embedder\n",
        "    logger = initialize_exp(params)\n",
        "\n",
        "    if params.tokens_per_batch > -1:\n",
        "        params.group_by_size = True\n",
        "    \n",
        "    # check parameters\n",
        "    assert os.path.isdir(params.data_path)\n",
        "    assert os.path.isfile(params.saved_path)\n",
        "    device = torch.device('cpu')\n",
        "    reloaded = torch.load(params.saved_path, map_location=device)\n",
        "    model_params = AttrDict(reloaded['params'])\n",
        "    logger.info(\n",
        "        \"Supported languages: %s\" % \", \".join(model_params.lang2id.keys()))\n",
        "    params.n_langs = model_params['n_langs']\n",
        "    params.id2lang = model_params['id2lang']\n",
        "    params.lang2id = model_params['lang2id']\n",
        "\n",
        "    if \"enc_params\" in reloaded:\n",
        "        encoder_model_params = AttrDict(reloaded[\"enc_params\"])\n",
        "    elif params.n_enc_layers == model_params.n_layers or params.n_enc_layers == 0:\n",
        "        encoder_model_params = model_params\n",
        "    else:\n",
        "        encoder_model_params = AttrDict(reloaded['params'])\n",
        "        encoder_model_params.n_layers = params.n_enc_layers\n",
        "        assert model_params.n_layers is not encoder_model_params.n_layers\n",
        "    \n",
        "    if \"dec_params\" in reloaded:\n",
        "        decoder_model_params = AttrDict(reloaded[\"dec_params\"])\n",
        "    elif params.n_dec_layers == model_params.n_layers or params.n_dec_layers == 0:\n",
        "        decoder_model_params = model_params\n",
        "    else:\n",
        "        decoder_model_params = AttrDict(reloaded['params'])\n",
        "        decoder_model_params.n_layers = params.n_dec_layers\n",
        "        assert model_params.n_layers is not decoder_model_params.n_layers\n",
        "    \n",
        "    params.encoder_model_params = encoder_model_params\n",
        "    params.decoder_model_params = decoder_model_params\n",
        "\n",
        "    # build dictionary / build encoder / build decoder / reload weights\n",
        "    dico = Dictionary(reloaded['dico_id2word'], reloaded['dico_word2id'], reloaded['dico_counts'])\n",
        "\n",
        "    for p in [params, encoder_model_params, decoder_model_params]:\n",
        "        p.n_words = len(dico)\n",
        "        p.bos_index = dico.index(BOS_WORD)\n",
        "        p.eos_index = dico.index(EOS_WORD)\n",
        "        p.pad_index = dico.index(PAD_WORD)\n",
        "        p.unk_index = dico.index(UNK_WORD)\n",
        "        p.mask_index = dico.index(MASK_WORD)\n",
        "\n",
        "    encoder = TransformerModel(encoder_model_params, dico, is_encoder=True, with_output=False)\n",
        "    decoder = TransformerModel(decoder_model_params, dico, is_encoder=False, with_output=True)\n",
        "\n",
        "    encoder.load_state_dict(reloaded[\"encoder\"])\n",
        "    decoder.load_state_dict(reloaded[\"decoder\"])\n",
        "    \n",
        "    scores = {}\n",
        "    XPersona(encoder, decoder, scores, dico, params).test()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mES0kFgFG-Z",
        "outputId": "4740e2ef-38c1-4670-b379-312b30d514c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py --exp_name testonZh --dump_path ./dump --saved_path ./dump/xpersona/ftOnZh/best_zh-zh_Perplexity.pth --data_path ./data/processed/XNLG --optimizer adam,lr=0.00001 --batch_size 8 --n_epochs 200 --epoch_size 3000 --max_len 120 --max_vocab 95000 --train_layers 1,10 --decode_with_vocab False --n_enc_layers 10 --n_dec_layers 6 --ds_name xpersona --train_directions en-en --eval_directions zh-zh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0w5nQAKScyv",
        "outputId": "824a2507-f8e4-48b2-a65f-f0b3fc6daa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Traceback (most recent call last):\n",
            "  File \"test.py\", line 207, in <module>\n",
            "    run_test()\n",
            "  File \"test.py\", line 145, in run_test\n",
            "    params = get_params()\n",
            "  File \"test.py\", line 51, in get_params\n",
            "    parser.add_argument(\"--group_by_size\", type=bool_flag, default=False,\n",
            "NameError: name 'bool_flag' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l51OrCa1Sz-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yZWnWXT5KRG5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}